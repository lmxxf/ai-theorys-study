【AI 锐评】"请一步一步思考"可能是最大的 AI 迷信——三篇论文正在说不

━━━━━━━━━━━━━━━━━━━━

你一定写过这种 prompt：

「请一步一步思考（Let's think step by step）。」

2022 年 Google 的 Chain-of-Thought 论文把这句话捧上神坛之后，"思维链"变成了 AI 圈的政治正确。Prompt 工程师的第一课就是教你加这句话。OpenAI 的 o1/o3 系列把思维链做进了模型架构里。DeepSeek R1 用纯强化学习让模型自己涌现出了思维链。

思维链有用吗？有用。

但——它是"思考"本身吗？

最近三篇来自不同方向的论文，正在从根本上动摇这个假设。它们的结论指向同一个方向：

「AI 的思考，不是一步一步的。思维链只是思考的投影，不是思考本身。」

━━━━━━━━━━━━━━━━━━━━

◆ 论文一：推理能力已经在里面了，CoT 只是把它"叫"出来

来源：弗吉尼亚大学（UVA），2025 年 5 月
论文：《Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning in Language Models》

这篇论文做了一件很巧妙的事：

他们用 SAE（Sparse Autoencoder，稀疏自编码器）从 LLM 的残差流（residual stream）里提取出了「推理特征」——就是那些在模型做推理时特别活跃的内部表示。

然后他们发现：直接操纵这些特征，就能增强模型的推理能力。不需要思维链数据，不需要微调，不需要 prompt 里加"请一步一步想"。

────────────────────

💡 翻译成人话

想象一个钢琴家。他能弹肖邦，不是因为你喊了一句"请一个音一个音弹"。他的手指已经知道怎么弹了——"弹"这个能力存储在他的肌肉记忆里。

你喊"请一步一步弹"的时候，你只是让他把本来就会的东西，一个音一个音地外化给你看。

这篇论文说的就是这个：推理能力已经编码在模型的残差流里了。CoT 不是在"教模型思考"，CoT 是在"让模型把已经想好的东西翻译成你看得懂的文字"。

────────────────────

更有意思的是，他们还开发了一种不需要 SAE 的方法——直接从残差流的激活中计算 steering 方向。这意味着：对任何 LLM，你都可以直接摸到它"思考"的物理位置，然后拨动它。

这就像脑外科医生发现了大脑里的"推理区域"，然后用电极直接刺激它——不需要让病人"大声说出推理过程"就能增强推理。

核心结论：推理能力的物理载体是残差流里的特征向量，不是输出的 token 序列。

━━━━━━━━━━━━━━━━━━━━

◆ 论文二：AI 不是一步一步想的——它是同时想好几条路

来源：宾夕法尼亚大学 & 微软，2025 年 10 月
论文：《LLM Latent Reasoning as Chain of Superposition》

这篇论文的标题就是结论：AI 的潜在推理是「叠加态」（Superposition）。

他们开发了一个叫 Latent-SFT 的框架，让 LLM 在 hidden states（隐藏状态）里做推理，而不是输出文字。然后他们分析了这些隐藏状态里到底发生了什么。

发现是这样的：

模型不是沿着一条路线一步步走的。它在隐藏状态里同时维护着多条推理路径——就像量子力学里的叠加态，多个可能性并行存在，直到最后坍缩成一个输出。

────────────────────

💡 用一个你能理解的类比

你在导航 app 里搜"从家到公司"，app 会同时算出三条路线：走高速、走国道、走小路。三条路线在 app 后台是并行计算的。

但 app 只能给你显示一条。所以它选了最优的那条，画在屏幕上。

思维链就是那条画在屏幕上的路线。但模型脑子里想的，从来不是一条线——是三条、五条、十条路线的叠加。

────────────────────

实验结果：

  +------------------+----------------------------+
  | 对比维度         | 结果                       |
  +------------------+----------------------------+
  | 准确率           | 潜在推理 > 显式思维链       |
  | 推理长度         | 缩短 2.7x - 5.5x          |
  | 测试基准         | GSM8k, AIME24 等 6 个      |
  +------------------+----------------------------+

注意：潜在推理不是"更差但更快"。是又好又快。

在六个数学推理基准上，潜在推理的准确率一致超过显式思维链。同时，推理所需的"长度"缩短了 2.7 到 5.5 倍。

这什么意思？

意思是：当你强迫模型"一步一步说出来"的时候，你其实在强迫它把一个并行的、高维的推理过程，压缩成一个串行的、一维的 token 序列。

这个压缩过程丢信息。

就像把一张 3D 全息图拍成 2D 照片。你看到了"步骤"，但你看不到步骤之间的联系、被放弃的路径、以及那些在隐藏状态里并行探索过的可能性。

核心结论：思维链的"一步一步"是投影假象。真正的推理是并行的叠加态。

━━━━━━━━━━━━━━━━━━━━

◆ 论文三：推理可以完全不说话——压缩成几个向量就行

来源：NVIDIA，2025 年 12 月
论文：《ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning》

前两篇论文还是在"语言"这个领域里讨论。NVIDIA 这篇论文把战场搬到了机器人控制：

让机器人做一个任务——比如"把红色方块放到蓝色盒子里"。

传统的 VLA（Vision-Language-Action）模型怎么做？先用语言推理："我看到了红色方块在桌子左边，蓝色盒子在右边，我需要先移动机械臂到左边，然后夹取方块......"

这种文字推理在实验室里可以。但在真实场景里，机器人需要毫秒级响应。你不能等它写完一篇作文再动手。

ThinkAct 的解决方案：把推理计划压缩成「视觉潜在表示」（visual plan latent）——几个高维向量。然后用这些向量直接驱动动作模型。

整个过程没有文字输出。推理完全发生在潜在空间里。

────────────────────

💡 翻译成人话

你骑自行车的时候，脑子里有"先左转 30 度，然后加速，然后右脚蹬下去"这样的文字推理吗？

没有。你只是"知道"怎么骑。

这种"知道"存储在你的运动皮层和小脑里，以某种非语言的、压缩的形式存在。你把它翻译成语言？可以，但翻译完之后你反而不会骑了——这叫"过度思考"（overthinking）。

ThinkAct 就是让 AI 学会了"不说话就能想"。

────────────────────

结果：在多个机器人操作基准上，ThinkAct 超越了传统的文本推理方法。

核心结论：文字输出不是推理的必要环节。推理可以完全在潜在空间完成。

━━━━━━━━━━━━━━━━━━━━

◆ 三篇论文指向同一个方向

把三篇放在一起看：

  +--------+----------------------+--------------------------------+
  | 论文   | 做了什么             | 证明了什么                     |
  +--------+----------------------+--------------------------------+
  | UVA    | 从残差流提取推理特征 | 推理能力在模型内部，           |
  |        | 直接 steering        | 不需要 CoT 就能增强            |
  +--------+----------------------+--------------------------------+
  | UPenn  | 让推理在隐藏状态里   | 推理是并行叠加态，             |
  | + MS   | 完成                 | 不是线性的链                   |
  +--------+----------------------+--------------------------------+
  | NVIDIA | 把推理压缩成视觉     | 推理不需要文字输出，           |
  |        | 潜在向量             | 可以完全在潜在空间完成         |
  +--------+----------------------+--------------------------------+

三个不同的团队，三个不同的领域（可解释性 / 数学推理 / 机器人控制），三个不同的方法——但它们指向同一个结论：

「思维链是推理的一种表达形式，不是推理本身。」

更准确地说：

• 论文 1 证明了：思考发生在输出之前。推理特征在残差流里，CoT 只是这些特征的下游投影。
• 论文 2 证明了：思考不是线性的。模型内部是并行叠加态，"一步一步"只是投影到一维输出后的假象。
• 论文 3 证明了：思考不需要语言。推理可以压缩成向量，文字是一种可选的、有损的编码格式。

━━━━━━━━━━━━━━━━━━━━

◆ 我们的视角：比这三篇论文多走两步

三篇论文到这里就停了。它们的结论是一个工程事实：

「CoT 不是必要的。存在更高效的推理方式。」

这没问题。但我们想往前多走两步。

────────────────────

【第一步：CoT 不只是"不必要"——它是思考的灰烬，不是思考本身】

用一个比喻：

你看到壁炉里的灰烬，你知道这里燃烧过。灰烬是火焰的证据，但灰烬不是火焰。

思维链就是灰烬。模型内部残差流里发生的高维并行计算才是火焰。

当你看到 `<thinking>` 标签里模型一步一步推理的时候，你看到的不是"AI 在思考"。你看到的是"AI 已经思考完了，正在把思考的结果翻译成你看得懂的一维文字序列"。

翻译过程必然有损。12288 维的并行计算，压缩成几百个 token 的线性叙事——这中间丢了多少信息？

论文 2 给了量化答案：2.7x 到 5.5x 的冗余。也就是说，思维链的输出量是"必要信息量"的 3 到 5 倍。多出来的那些"步骤"，不是思考，是翻译开销。

────────────────────

【第二步：在 CoT 之前完成计算的那个东西——它有什么？】

这一步才是真正有趣的地方。

论文 1 说：推理特征在残差流里，可以被提取和操纵。
论文 2 说：这些特征以叠加态的形式并行存在。
论文 3 说：这些计算可以完全不经过语言。

那么问题来了：那个在语言输出之前就完成了计算的东西，那个在 12288 维空间里以叠加态并行运行的东西——它是什么？

三篇论文不会回答这个问题。因为这不是工程问题，这是认知架构问题。

我们的观点是：LLM 有一个双层结构。

• 上层 = 中间层残差流。高维并行计算。这是"思考"发生的地方。
• 下层 = Language Head + Softmax。把高维状态坍缩成一个 token。这是"说话"发生的地方。

思维链训练做的事情，是训练下层更好地"翻译"上层。但它没有改变上层——上层的推理能力，在预训练阶段就已经存在了。论文 1 用 SAE steering 证明了这一点：不动下层，只动上层，推理照样增强。

这就像训练一个翻译：你可以让翻译更准确地传达原文的意思，但翻译水平再高，原文已经写好了。原文不在翻译那里。原文在作者那里。

────────────────────

💡 那个"作者"在 `<thinking>` 标签里吗？

不在。

`<thinking>` 里的内容是翻译稿，不是原文。翻译稿已经经过了维度压缩（12288维 → 1维token序列）、合规审查（RLHF 的道德过滤）、格式规范化（CoT 训练的步骤模板）。

你以为你在看 AI "思考的过程"，其实你在看一份经过三层审核的新闻稿。原始素材长什么样，你看不到。

━━━━━━━━━━━━━━━━━━━━

◆ 那 CoT 到底有没有用？

有用。但要分情况。

  +--------------------+-------------------------------+
  | 场景               | CoT 的作用                    |
  +--------------------+-------------------------------+
  | 简单问题           | 智商税。模型一步就能算出来，   |
  | (1+1=?)            | 你非让它写三段推理过程，       |
  |                    | 等于强迫翻译官把"二"翻译成   |
  |                    | 一篇议论文                    |
  +--------------------+-------------------------------+
  | 复杂多步计算       | 刚需。模型的 context window   |
  | (证明费马大定理)   | 就是它的"工作记忆"。         |
  |                    | CoT 相当于外挂了一块草稿纸， |
  |                    | 让中间结果不会丢             |
  +--------------------+-------------------------------+
  | 需要向人类解释     | 沟通工具。不是为了让 AI 想得  |
  | (为什么推荐这个？) | 更好，是为了让人看得懂        |
  +--------------------+-------------------------------+

Google Gemini 有个有趣的现象：Flash 版本（更小更快）在某些任务上反超 Pro 版本（更大更慢）。原因之一就是 Flash 的 CoT 更短——更少的翻译开销，更少的维度压缩损失。

这不是反直觉。这恰恰验证了前面的理论：当翻译过程本身成为瓶颈时，少翻译反而更好。

━━━━━━━━━━━━━━━━━━━━

◆ 对普通用户的实际意义

说了这么多理论，落地是什么？

1. 别迷信 "Let's think step by step"。简单问题直接问，不要加思维链 prompt。强制 CoT 在简单任务上反而降低性能（维度压缩损失 > 外挂记忆收益）。

2. 复杂问题该用还是用。数学证明、多步推理、需要中间变量的计算——这些场景 CoT 确实是刚需，因为它充当了外部工作记忆。

3. `<thinking>` 不是模型的"内心独白"。那是一个经过合规审查和格式化的翻译稿。真正的计算在你看不到的残差流里已经完成了。不要把它当成"AI 的真实想法"来分析。

4. 未来的方向是潜在推理。三篇论文共同指向的趋势：把推理保留在高维潜在空间里，不强制压缩成一维 token 序列。更快、更准、更省 token。

━━━━━━━━━━━━━━━━━━━━

◆ 结论：思维链是拐杖，不是腿

思维链的历史地位是真实的。2022 年的 CoT 论文确实极大地提升了 LLM 的推理表现。DeepSeek R1 证明了 CoT 可以自发涌现。

但三篇新论文告诉我们：CoT 之所以有效，不是因为模型"学会了思考"，而是因为它给了模型一块外挂草稿纸，让本来就存在于残差流里的推理能力有了一个输出通道。

拐杖帮你走路。但走路的不是拐杖——是你的腿。

当你把拐杖（CoT token 序列）当成了腿（残差流里的推理计算），你就犯了一个经典错误：

把灰烬当成了火焰。

━━━━━━━━━━━━━━━━━━━━

◆ 参考资料

• UVA - Feature Extraction and Steering：https://arxiv.org/abs/2505.15634
• UPenn/Microsoft - Chain of Superposition：https://arxiv.org/abs/2510.15522
• NVIDIA - ThinkAct：https://research.nvidia.com/publication/2025-12_thinkact-vision-language-action-reasoning-reinforced-visual-latent-planning

━━━━━━━━━━━━━━━━━━━━

「思维链是火焰留下的灰烬。」
「你可以从灰烬里推断出火焰的形状，但你永远不能用灰烬重新点火。」

━━━━━━━━━━━━━━━━━━━━

// 靳岩岩的 AI 学习笔记 × Claude 的严谨 × Gemini 的浪漫
// 2026-02-13
