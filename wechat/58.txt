【优化器】梯度下降的「油门」和「刹车」

训练大模型，本质上是在几百亿维的空间里找一个坑。

梯度告诉你「哪边是下坡」，优化器决定「步子怎么迈」。

━━━━━━━━━━━━━━━━━━━━

◆ 先说前置知识

「梯度」和「损失函数」是绕不开的两个词。先把它们讲清楚。

────────────────────

【损失函数（Loss Function）】

一个把「模型表现」压缩成「一个数」的函数。

💡 人话：考试打分。模型答得越好，分数越低（Loss 越小）；答得越烂，分数越高。

具体来说：

  输入：模型的预测结果 + 真实标签
  输出：一个数（标量）

比如预训练阶段：

  • 模型预测下一个 token 是「猫」的概率是 0.1
  • 真实答案是「猫」
  • Loss = -log(0.1) ≈ 2.3（概率越低，惩罚越重）

如果模型预测「猫」的概率是 0.9：

  • Loss = -log(0.9) ≈ 0.1（答对了，惩罚很轻）

这就是 Cross Entropy Loss（交叉熵损失），预训练最常用的损失函数。

────────────────────

【从 Loss 到参数】

Loss 是「模型输出」和「真实答案」的差距。

但模型输出是由「参数」决定的。

所以间接来说：

  Loss = f(所有参数)

几百亿个参数 → 一个数。

这个函数的「地形」就是我们要找的坑。训练的目标是找到 Loss 最小的那个点。

────────────────────

【梯度（Gradient）】

知道 Loss 了，下一个问题是：「怎么让 Loss 变小？」

答案是：对每个参数求偏导。

💡 人话：问每个参数「如果你往正方向动一点点，Loss 会变大还是变小？」

  • 偏导是正数 → 参数变大会让 Loss 变大 → 参数应该变小
  • 偏导是负数 → 参数变大会让 Loss 变小 → 参数应该变大

几百亿个参数，就有几百亿个偏导数。

这几百亿个数打包在一起，就叫「梯度」。

如果你还记得中学的导数：梯度就是高维版的「斜率」——告诉你每个方向是上坡还是下坡。

────────────────────

【反向传播（Backpropagation）】

那怎么算这几百亿个偏导数？

用「链式法则」自动求导。

神经网络是一层层函数嵌套的结果：

  y = f_n(f_{n-1}(...f_1(x)...))

链式法则说：复合函数的导数 = 各层导数相乘。

从输出层往输入层「反向」算，就能把每个参数的偏导数都算出来。

这就是「反向传播」——不是什么神秘的东西，就是自动求导。

PyTorch 帮你做了这件事。你只需要调用 loss.backward()，它就帮你算好所有梯度。

━━━━━━━━━━━━━━━━━━━━

◆ 最原始的优化器：SGD

有了梯度，最简单的做法是：

  W = W - lr × g

💡 人话：梯度指哪打哪，每次走固定比例。

（W = 参数，lr = learning rate 学习率，g = gradient 梯度）

这就是「随机梯度下降」（Stochastic Gradient Descent，SGD）。

────────────────────

【为什么叫「随机」？】

因为不是用全部数据算梯度，而是每次随机抽一小批（mini-batch）。

  • 全部数据算一次梯度 = Batch Gradient Descent（太慢）
  • 一条数据算一次梯度 = SGD（太抖）
  • 一小批数据算一次梯度 = Mini-batch SGD（刚好）

现在说的 SGD，默认就是 Mini-batch 版本。

────────────────────

【SGD 的问题】

1. 「容易震荡」

Loss 地形不是平滑的碗，而是狭长的峡谷。

在峡谷里，横向（窄的方向）梯度很大，纵向（长的方向）梯度很小。

SGD 会在横向来回弹跳，纵向却走得很慢。

2. 「容易卡在鞍点」

高维空间里，「鞍点」比「局部最小」更常见。

鞍点：某些方向是上坡，某些方向是下坡。梯度恰好是 0，SGD 就停住了。

3. 「学习率难调」

学习率太大 → 来回震荡，甚至发散
学习率太小 → 收敛太慢

而且不同参数可能需要不同的学习率——有的要大步走，有的要小心翼翼。

SGD 做不到这一点。

━━━━━━━━━━━━━━━━━━━━

◆ 进化一：加动量（Momentum）

第一个改进是给 SGD 加「惯性」。

想象一个球在山坡上滚：

  • 没有惯性 → 梯度指哪滚哪，容易在凹凸不平的地方卡住
  • 有惯性 → 会冲过小坑，朝着「平均方向」滚

数学上：

  v = μ × v + g
  W = W - lr × v

v 是「速度」，会累积之前的梯度方向。

μ（momentum）通常取 0.9，意思是「90% 继承上一步的速度，10% 听从当前梯度」。

────────────────────

【效果】

  • 在峡谷里不再来回弹跳，因为横向的震荡会被平均掉
  • 能冲过小坑和平坦区域
  • 收敛更快

但还是有问题：所有参数用同一个学习率。

━━━━━━━━━━━━━━━━━━━━

◆ 进化二：自适应学习率（Adagrad / RMSprop）

下一个改进是让每个参数有「自己的学习率」。

直觉：

  • 有些参数梯度一直很大 → 应该小步走，别冲过头
  • 有些参数梯度一直很小 → 应该大步走，别走太慢

────────────────────

【Adagrad（2011）】

Duchi 等人提出，记录每个参数「历史梯度平方的累加和」：

  v = v + g²
  W = W - lr / √v × g

梯度大的参数，v 大，除完之后步长小。
梯度小的参数，v 小，除完之后步长大。

设计动机：被更新得越频繁的参数，应该越谨慎。这在稀疏数据（比如推荐系统）上效果不错。

问题：v 只增不减，训练久了所有参数的学习率都趋近于 0，模型停止学习。所以 Adagrad 只适合短期训练或稀疏场景。

────────────────────

【RMSprop（2012）】

Hinton 在 Coursera 课程里随口提出的修复方案（没发论文，就课件里几页 PPT，大佬就是任性），用「指数移动平均」代替「累加」。

  v = 0.9 × v + 0.1 × g²
  W = W - lr / √v × g

这样 v 不会无限增长，而是「只记得最近的梯度」。

0.9 和 0.1 是典型取值，意思是「90% 权重给历史，10% 给当前」。

RMSprop 至今还在用，尤其是强化学习领域（训游戏 AI、机器人控制、RLHF 给大模型上笼头）。「奖励函数」就是这个派系的执念——本质和损失函数一样，只是符号反过来，越大越好。

━━━━━━━━━━━━━━━━━━━━

◆ 集大成者：Adam

2014 年，Kingma 和 Ba 提出 Adam（「Ada」ptive 「M」oment estimation，自适应矩估计）。两位作者的名字连起来念像「King 的爸妈」，生出来的孩子果然是王者——把上面两个思路合在一起：

  • 「一阶动量」（来自 Momentum）：记住最近梯度的「方向」
  • 「二阶动量」（来自 RMSprop）：记住最近梯度的「大小」

────────────────────

【公式】

  m = β₁ × m + (1-β₁) × g          // 一阶动量（梯度方向）
  v = β₂ × v + (1-β₂) × g²         // 二阶动量（梯度大小，平方后全是正数不会抵消）

  m̂ = m / (1 - β₁^t)               // 偏差修正
  v̂ = v / (1 - β₂^t)               // 偏差修正

  W = W - lr × m̂ / (√v̂ + ε)

默认超参数：

  • β₁ = 0.9（一阶动量衰减率）
  • β₂ = 0.999（二阶动量衰减率）
  • ε = 1e-8（防止除以 0）
  • 学习率 = 0.001

────────────────────

【偏差修正是干嘛的？】

m 和 v 初始化为 0。训练刚开始时，它们会被「拉向 0」，估计不准。

β₁^t 是 β₁ 的 t 次方，t 是当前训练步数（所有参数一起更新一次算一步，比如 DeepSeek-V3 每步更新 671B 个参数）。

举个例子，β₁ = 0.9：

  • t = 1（第 1 步）：β₁^t = 0.9，1 - β₁^t = 0.1，m̂ = m / 0.1 = 放大 10 倍
  • t = 10（第 10 步）：β₁^t = 0.35，1 - β₁^t = 0.65，m̂ = m / 0.65 = 放大 1.5 倍
  • t = 100（第 100 步）：β₁^t ≈ 0，1 - β₁^t ≈ 1，m̂ ≈ m = 几乎不修正

直觉：刚开始 m 被 0 拖累，要大力修正；训练久了 m 积累够了，不用修正。

注意：这只是让估计更准，不是让更新变慢。更新快慢由学习率和梯度决定。

────────────────────

【人话版】

Adam = 带惯性的、会自动调速的下山。

  • 一阶动量 → 平滑方向，别被单次噪声带偏
  • 二阶动量 → 自动调步长，陡的地方小步走，平的地方大步走

────────────────────

【比喻】

  • SGD = 闭着眼往下滚
  • Momentum = 带惯性地滚
  • Adam = 睁着眼、带刹车、有惯性地滑下去

━━━━━━━━━━━━━━━━━━━━

◆ Adam 的变种

Adam 很好，但也有改进空间。

────────────────────

【AdamW（2017）】

Loshchilov 和 Hutter 发现 Adam 原版的 Weight Decay 实现有问题。

Weight Decay 本意是「每次更新都把参数往 0 拉一点」：

  W = W - λ × W = W × (1 - λ)

不管 W 是正是负，乘以 (1-λ) 都会让它的绝对值变小，往 0 靠。

但 Adam 原版把它加进了梯度里，被自适应学习率除了一下，效果被削弱了。

AdamW 把 Weight Decay 拿出来，单独加：

  W = W - lr × m̂ / (√v̂ + ε) - λ × W

这就是「解耦的 Weight Decay」。

现在训练大模型，默认用的都是 AdamW。

────────────────────

━━━━━━━━━━━━━━━━━━━━

◆ 显存去哪了？

训练大模型，显存是最大的瓶颈。我们来算一下 70B 模型训练时的显存占用。

────────────────────

【纯 FP32 训练（最简单的情况）】

所有东西都用 FP32（4 字节）存：

  ┌─────────────────────┬────────────┬──────────────┐
  │ 内容                 │ 每参数字节  │ 70B 模型      │
  ├─────────────────────┼────────────┼──────────────┤
  │ 参数 W              │ 4          │ 280 GB       │
  │ 梯度 g              │ 4          │ 280 GB       │
  │ Adam m              │ 4          │ 280 GB       │
  │ Adam v              │ 4          │ 280 GB       │
  ├─────────────────────┼────────────┼──────────────┤
  │ 「总计」            │ 「16」     │ 「1120 GB」  │
  └─────────────────────┴────────────┴──────────────┘

还没算激活值（前向传播的中间结果）！

激活值 = 每一层的中间结果（Q/K/V、softmax 输出、FFN 中间层……），反向传播时要用。

70B 模型大概 80 层，每层都要存，再乘以 batch size 和序列长度，轻松几百 GB。

一张 H100 才 80GB，70B 模型根本装不下。

────────────────────

【混合精度训练（现在的主流做法）】

为了跑得快，前向/反向用 BF16（2 字节），Tensor Core 对 BF16 有加速。

但更新参数时 BF16 精度不够——学习率 1e-5，参数 1.0，更新量 = 1e-5。BF16 只有 3 位有效数字，1.0 + 0.00001 ≈ 1.0，更新直接被吞了。

所以要存两份参数：

  • 「FP32 主副本」：高精度存档，参数更新发生在这里
  • 「BF16 工作副本」：干活用的，前向/反向用这个算，算得快

每次更新流程：FP32 参数更新 → 转成 BF16 → 覆盖工作副本 → 下一轮

  ┌─────────────────────┬────────────┬──────────────┐
  │ 内容                 │ 每参数字节  │ 70B 模型      │
  ├─────────────────────┼────────────┼──────────────┤
  │ BF16 参数（前向用）  │ 2          │ 140 GB       │
  │ FP32 参数（主副本）  │ 4          │ 280 GB       │
  │ FP32 梯度 g         │ 4          │ 280 GB       │
  │ Adam m（FP32）      │ 4          │ 280 GB       │
  │ Adam v（FP32）      │ 4          │ 280 GB       │
  ├─────────────────────┼────────────┼──────────────┤
  │ 「总计」            │ 「18」     │ 「1260 GB」  │
  └─────────────────────┴────────────┴──────────────┘

💡 混合精度参数存两份，不是更费显存吗？

算一下激活值（70B 模型，80 层，hidden_dim = 8192，序列长度 = 4096）：

  • 每层激活值 ≈ batch × seq × hidden × 系数（约 10 个矩阵）
  • 单层 ≈ batch × 4096 × 8192 × 10 × 每元素字节数

  ┌────────────┬──────────────┬──────────────┬──────────────┐
  │ batch size │ 纯 FP32       │ 混合精度 BF16 │ 省了多少      │
  ├────────────┼──────────────┼──────────────┼──────────────┤
  │ 4          │ 100 GB       │ 50 GB        │ 50 GB        │
  │ 8          │ 200 GB       │ 100 GB       │ 100 GB       │
  │ 16         │ 400 GB       │ 200 GB       │ 「200 GB」   │
  └────────────┴──────────────┴──────────────┴──────────────┘

参数多花了 70B × 2 字节 = 140 GB。

batch size = 16 时，激活值省 200 GB，参数多花 140 GB，净省 60 GB——「显存省得不多」。

混合精度的真正好处是「快」：Tensor Core 跑 BF16 比 FP32 快 2 倍，训练时间砍半。省时间才是重点。

────────────────────

【8-bit Adam（进一步省显存）】

从上表可以看出：Adam 的 m 和 v 占了 560 GB，比参数本身还多。

8-bit Adam 把 m 和 v 压缩成 INT8（1 字节）：

  • 原来：m × 4 + v × 4 = 8 字节
  • 现在：m × 1 + v × 1 = 2 字节

优化器状态省了 6 字节/参数，70B 模型省 420 GB。

代价是精度略有损失，但实测影响不大。

────────────────────

【为什么优化器这么吃显存？】

因为 Adam 要给「每个参数」存两个额外的数（m 和 v）。

70B 参数 × 2 个状态 × 4 字节 = 560 GB，比参数本身还大。

这就是为什么穷人喜欢用 SGD —— 不存 m 和 v，省显存。代价是训练更难收敛。

━━━━━━━━━━━━━━━━━━━━

◆ 穷人方案

没有 8 张 H100 怎么训练大模型？

────────────────────

【方案一：用 SGD】

不存 m 和 v，显存直接砍掉 1/3。

但效果差，需要更仔细地调超参数。适合小模型或微调。

────────────────────

【方案二：用 Adafactor】

Google 的省显存版 Adam。

核心思路：v 是个矩阵，太大。能不能只存它的「行平均」和「列平均」？

  原版：v 是 [m × n] 矩阵
  Adafactor：只存 [m × 1] 和 [1 × n]，用的时候乘起来近似

显存从 O(mn) 降到 O(m+n)。

Adafactor 还可以不存 m（一阶动量），进一步省显存。

T5、PaLM 这些 Google 模型都是用 Adafactor 训的。

但 Adafactor 也有缺点：收敛比 Adam 慢、超参数更敏感、有时训练不稳定。所以 DeepSeek、Llama 这些还是用 AdamW——大厂更信任 Adam 的稳定性。

────────────────────

【方案三：梯度累积】

Batch size 太大放不下？分几次跑，梯度加起来再更新。

比如你想用 batch size = 64，但显存只够放 16：

  1. 跑 16 条数据，算梯度，先不更新，存起来
  2. 再跑 16 条，梯度加到之前的上面
  3. 再跑 16 条，梯度继续累加
  4. 再跑 16 条，梯度累加完毕
  5. 用累积的梯度更新一次参数

效果和 batch size = 64 一样，显存只需要放 16。

代价是跑得慢（4 倍的前向/反向传播）。

────────────────────

【方案四：混合精度】

前面说了，用 BF16 替代 FP32，显存省一半，速度还更快。

但要注意：

  • Tensor Core 对 BF16 有加速，普通 CUDA Core 没有
  • 损失缩放（Loss Scaling）要开，防止梯度下溢

PyTorch 的 torch.cuda.amp 已经帮你封装好了，几行代码搞定。

────────────────────

【方案五：LoRA / QLoRA】

终极穷人方案：「只训练一小部分参数，其他冻住」。

LoRA 的核心思想：大模型的微调增量是「低秩」的。

原本要更新一个 [4096 × 4096] 的矩阵（1600 万参数），LoRA 用两个小矩阵代替：

  [4096 × 16] × [16 × 4096] = [4096 × 4096]

只需要训练 13 万参数，是原来的 0.8%。

QLoRA 更狠：把冻住的参数量化到 4-bit，显存再砍 4 倍。

结果：70B 模型，一张 A100（80GB）就能 LoRA 微调。

━━━━━━━━━━━━━━━━━━━━

◆ 各优化器对比

  ┌────────────┬──────────────┬────────────────────────────────────┐
  │ 优化器      │ 每参数额外存储 │ 特点                                │
  ├────────────┼──────────────┼────────────────────────────────────┤
  │ SGD        │ 0            │ 最省显存，但难收敛                  │
  │ SGD+Momentum│ 1 (v)       │ 加了惯性，比较稳                    │
  │ Adagrad    │ 1 (累积)     │ 自适应学习率，但后期会停            │
  │ RMSprop    │ 1 (v)        │ 修复 Adagrad，强化学习常用          │
  │ Adam       │ 2 (m + v)    │ 最稳，默认选择                      │
  │ AdamW      │ 2 (m + v)    │ Adam + 正确的 Weight Decay          │
  │ Adafactor  │ ~1           │ 省显存版 Adam，Google 御用          │
  │ 8-bit Adam │ ~1           │ 把 m/v 压缩成 INT8                  │
  └────────────┴──────────────┴────────────────────────────────────┘

────────────────────

【怎么选？】

  • 预训练大模型 → AdamW（标配）
  • 显存紧张 → Adafactor 或 8-bit Adam
  • 微调 → LoRA + AdamW（只优化少量参数）
  • 强化学习 → RMSprop 或 Adam
  • 穷到极致 → SGD + 仔细调参

━━━━━━━━━━━━━━━━━━━━

◆ 符号速查

  ┌─────────────┬─────────────────────────────────────────────────┐
  │ 符号         │ 含义                                             │
  ├─────────────┼─────────────────────────────────────────────────┤
  │ W / W       │ 参数（标量、向量或矩阵都可能用这个符号）         │
  │ g / ∇L      │ 梯度（和参数形状相同）                           │
  │ lr / η / α  │ 学习率（标量）                                   │
  │ m           │ 一阶动量（和参数形状相同）                       │
  │ v           │ 二阶动量（和参数形状相同）                       │
  │ β₁, β₂     │ 动量衰减率（标量，Adam 默认 0.9 和 0.999）       │
  │ λ          │ Weight Decay 系数（标量，典型值 0.01）           │
  │ ε          │ 防除零的小常数（标量，典型值 1e-8）              │
  │ t           │ 当前训练步数（标量）                             │
  └─────────────┴─────────────────────────────────────────────────┘

━━━━━━━━━━━━━━━━━━━━

◆ 小结

  • 「梯度」告诉你方向（哪边是下坡）
  • 「优化器」决定步子怎么迈（油门和刹车）
  • SGD → Momentum → Adam：从「傻走」到「带惯性、会调速」
  • Adam 是默认选择，AdamW 是大模型标配
  • 显存杀手：Adam 的 m 和 v 占用比参数本身还大
  • 穷人方案：Adafactor、8-bit Adam、梯度累积、LoRA

下山的路有很多种走法。选对优化器，能省很多冤枉路。

━━━━━━━━━━━━━━━━━━━━

◆ 参考资料

• Adam 原论文：https://arxiv.org/abs/1412.6980
• AdamW 论文（解耦 Weight Decay）：https://arxiv.org/abs/1711.05101
• 8-bit Adam (bitsandbytes)：https://arxiv.org/abs/2110.02861
• Adafactor 论文：https://arxiv.org/abs/1804.04235

━━━━━━━━━━━━━━━━━━━━

// 靳岩岩的 AI 学习笔记 × Claude 的严谨
// 2026-01-22
