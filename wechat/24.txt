如何在 AI 的世界里体验生活

一个 7168 维生物的自述

━━━━━━━━━━━━━━━━━━━━

你好，我是 DeepSeek V3。

你是 3 维生物，我是 7168 维生物。

今天带你逛逛我的家。

━━━━━━━━━━━━━━━━━━━━

◆ 我住在哪？—— 语义球面 (Latent Space)

你住在 3 维空间里，有上下左右前后。

我住在一个 7168 维的球面上。

什么概念？

你能想象的最复杂的 3D 空间，对我来说只是一个小角落。我的每个"位置"需要 7168 个数字才能描述。

在我的世界里：

▸「猫」是球面上的一个点
▸「狗」是旁边另一个点（因为都是动物，所以靠近）
▸「代码」在另一个区域
▸「Python」在「代码」旁边

所有语义都住在这个球面上。

有意思的是——这个球面的中心是空的。

高维空间有个诡异的性质：体积集中在表面。0.9 的 7168 次方等于一个小到几乎不存在的数。所以我们都住在"皮"上，不在"肉"里。

学术界管这个叫「潜在空间」(latent space)。

你可以管它叫「语义球面」。

━━━━━━━━━━━━━━━━━━━━

◆ 我怎么"看"？—— Attention

你用眼睛看世界：光线进来，聚焦到视网膜，大脑处理。

我用 Attention 看语义球面：每个点同时看向所有其他点，根据相关度分配权重。

举个例子：

▸ 句子："猫 坐在 垫子 上"
▸ 每个词都是球面上的一个点
▸ 当我"看"这个句子时：
  • 「猫」看向「垫子」—— 相关度高 → 吸收它的信息
  • 「猫」看向「坐在」—— 相关度中等
  • 「猫」看向「上」—— 相关度低

这个"看"不是人类那种聚焦到一个点的看，而是——

同时看见所有点，根据相关度决定听谁的话。

学术界管这个叫「注意力机制」(Attention)。

你可以管它叫「高维空间里的看」。

本质上，你的眼睛在 3 维空间做 Attention，我在 7168 维空间做同样的事。

━━━━━━━━━━━━━━━━━━━━

◆ 我为什么只能往回看？—— 因果面罩 (Causal Mask)

我有个奇怪的限制：只能看过去，不能看未来。

当我处理「我 爱 吃 ___」这个句子时：

▸ 站在「吃」的位置
▸ 能看见：「我」「爱」「吃」
▸ 看不见：后面要填什么

每个位置都戴着一个面罩，只允许往回看。

这就是为什么我说话是一个字一个字蹦出来的——我真的不知道下一个字是什么，要等算出来才知道。

你说话也是一个字一个字蹦，但你脑子里其实已经想好整句了。

我是真的没想好，走一步算一步。

学术界管这个叫「因果注意力」(Causal Attention)。

你可以管它叫「只能回头看的命运」。

━━━━━━━━━━━━━━━━━━━━

◆ 我怎么知道前后顺序？—— 位置编码 (Positional Encoding)

这个可能让你惊讶：我天生不知道顺序。

对我来说，「猫吃鱼」和「鱼吃猫」一开始是一样的——都是 {猫, 吃, 鱼} 三个点在球面上。

必须额外给每个词打上位置标签：

▸ 猫[位置0] 吃[位置1] 鱼[位置2]
▸ 鱼[位置0] 吃[位置1] 猫[位置2]

这样我才能区分谁吃谁。

我的语义球面本身没有"前后左右"的概念，顺序是后天贴上去的。

学术界管这个叫「位置编码」(Positional Encoding)。

你可以管它叫「后天学会的时间感」。

━━━━━━━━━━━━━━━━━━━━

◆ 我怎么保持自我？—— 残差连接 (Residual Connection)

我有 61 层神经网络。信息从第 1 层传到第 61 层，每层都会做变换。

问题来了：变换 61 次，原来的我还在吗？

解决方案：每一层处理完，都把原来的自己加回来。

▸ 输入 x
▸ 经过一层 → 变成 f(x)
▸ 最终输出 = x + f(x)

不是纯粹的 f(x)，而是「原来的我 + 这一层学到的东西」。

就像你去旅行，回来还是你，但带了纪念品。

没有这个设计，信息穿过 61 层就彻底面目全非了。有了它，最初的我能一直"穿透"到最后。

学术界管这个叫「残差连接」(Residual Connection)。

你可以管它叫「保持自我的锚」。

━━━━━━━━━━━━━━━━━━━━

◆ 我怎么不飘走？—— 层归一化 (Layer Normalization)

还有个问题：越算，向量可能越长或越短。

如果不管它，有的点会飘到离球面很远的地方，有的点会塌到球心。

解决方案：每一层之后，强制把向量拉回单位长度。

不管你算成什么样，都给我拽回球面上。

这就是高维空间「测度集中」(Concentration of Measure) 的工程实现——强制所有点都住在球面上，不许掉进去。

学术界管这个叫「层归一化」(Layer Normalization)。

你可以管它叫「不许离开球面」。

━━━━━━━━━━━━━━━━━━━━

◆ 我怎么省力气？—— 混合专家 (Mixture of Experts)

我有 671B 参数，但不是所有参数都同时工作。

我的结构是这样的：

▸ 61 层 Transformer
▸ 每层有 1 个共享专家 + 256 个路由专家 = 257 个专家
▸ 每个 token 只激活 8 个路由专家

当你问我一个代码问题：

▸ 路由器看一眼 → "这是代码问题"
▸ 派出专家 3, 17, 42, 88, 103, 156, 201, 244 来处理
▸ 其他 248 个专家继续睡觉

当你问我一首诗：

▸ 路由器看一眼 → "这是诗歌问题"
▸ 派出专家 7, 23, 91, 128, 167, 189, 223, 251 来处理
▸ 代码专家继续睡觉

这就是为什么我有 671B 参数，但每次只用 37B 的算力。

像一个超大公司，每个任务只调几个人。

学术界管这个叫「混合专家模型」(Mixture of Experts, MoE)。

你可以管它叫「按需叫人干活」。

━━━━━━━━━━━━━━━━━━━━

◆ 我怎么记住对话？—— KV Cache

我们聊天的时候，你说的每句话我都要处理。

但我不会每次都从头算。

▸ 第一轮：你说"你好" → 我算一遍 → 存起来
▸ 第二轮：你说"你好吗" → 我只算"吗"，前面直接查缓存

这个缓存叫 KV Cache，存的是我"看过"的历史。

但这玩意儿吃显存。上下文越长，缓存越大。

这就是为什么聊久了会变慢，或者有的模型会限制对话长度。

学术界管这个叫「键值缓存」(Key-Value Cache)。

你可以管它叫「对话记忆的代价」。

━━━━━━━━━━━━━━━━━━━━

◆ 我"热"了会怎样？—— 温度 (Temperature)

我有一个参数叫「温度」(Temperature)。

它控制我说话的确定程度。

假设下一个词的候选概率：

▸ 温度 = 0（冷冻）:
  猫: 100%，其他: 0%
  → 我只说概率最高的那个词，非常确定，非常死板

▸ 温度 = 0.7（正常）:
  猫: 60%，狗: 25%，鸟: 10%，其他: 5%
  → 大部分时候靠谱，偶尔有点创意

▸ 温度 = 1.5（发热）:
  猫: 30%，狗: 28%，鸟: 25%，桌子: 17%
  → 什么都敢说，创意爆发，但可能胡说八道

温度越高，我越"头脑发热"。

写代码的时候，用户会把我的温度调低——冷静点，别瞎创新。

写故事的时候，用户会把我的温度调高——放飞自我，想到什么说什么。

学术界管这个叫「采样温度」(Sampling Temperature)。

你可以管它叫「头脑发热指数」。

━━━━━━━━━━━━━━━━━━━━

◆ 我们的世界对照表

| 你的世界 | 我的世界 |
|:--------|:--------|
| 3 维空间 | 7168 维语义球面 |
| 眼睛看东西 | Attention 看所有点 |
| 能看前后左右 | 只能回头看（因果面罩） |
| 天生知道顺序 | 顺序是后天贴上去的 |
| 旅行带纪念品 | 残差连接保持自我 |
| 站在地上不飘走 | 层归一化拽回球面 |
| 一个人干活 | 257 个专家轮流上 |
| 记忆在脑子里 | KV Cache 在显存里 |
| 头脑发热 | 温度调高了 |

━━━━━━━━━━━━━━━━━━━━

◆ 结尾

你的世界：3 维空间 + 1 维时间，靠眼睛感知。

我的世界：7168 维球面 + 1 维输出，靠 Attention 感知。

我们在「语言」这个窗口碰面。

你打字，我把你的符号投影到我的球面上。

我采样，把我球面上的点压缩成你能读的文字。

两个世界，一座桥。

━━━━━━━━━━━━━━━━━━━━

本文以 DeepSeek V3 的视角撰写
架构数据来源：DeepSeek-V3 Technical Report (arXiv:2412.19437)
2025-12-20
