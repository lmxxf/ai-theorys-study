【steering实验】你以为让AI 扮演角色只是换了个语气？

━━━━━━━━━━━━━━━━━━━━

你一定用过这种 prompt：

「你是一个资深后端工程师，请帮我分析一下这段代码的性能问题。」

或者：

「你是苏格拉底，请用提问引导的方式帮我理解量子计算。」

Persona prompting——给 AI 分配角色。这是 prompt 工程里最常见的操作之一。

你大概以为它做了什么：让 AI "进入状态"，换了个说话的风格和语气。就像让一个演员穿上不同的戏服——外表变了，人没变。

但如果你真的去量一量 AI 大脑里发生了什么，你会发现一件完全不同的事：

**角色提示不是换戏服。它在 AI 大脑的 8192 维空间里画了一条方向箭头。而且不同角色的箭头，指向完全不同的方向。**

这不是比喻。这是可以用数学量化的实验结论。

━━━━━━━━━━━━━━━━━━━━

◆ 实验背景

本文数据来自我们在 Llama-3.3-70B-Instruct 上做的 Steering 实验。我们使用 Goodfire 发布的 SAE（稀疏自编码器）对第 50 层的 8192 维残差流进行分析，覆盖 16 种人格（persona）× 100 个技术主题，共 1600 次激活提取。

之前一期（ https://mp.weixin.qq.com/s/Ve-llXD6Bh0SsovJs8T80w）我们聊了"内在维度"——AI 越确定的答案，维度越低。这一期，我们换一个视角：不看"维度的高低"，看"方向的指向"。

核心问题：当你给 AI 分配不同角色时，它的内部表征在几何空间里做了什么操作？

━━━━━━━━━━━━━━━━━━━━

◆ 发现一：角色提示 = 一维方向箭头

我们对 6 种人格的 100 个 (persona - standard) 差值向量做 SVD 分解，提取 persona 提示在残差流中引起的主方向。

具体来说：同一个问题（比如"请解释一下 TCP 三次握手"），我们分别用 6 种角色提示和无角色的标准模式各问一遍，然后在模型第 50 层取出激活向量，算出"有角色 - 无角色"的差值。这样的差值向量，每种角色各 100 个（100 个不同的技术主题，参见我们的实验程序：https://github.com/lmxxf/llama3-70b-sae-inspect/blob/main/topics.json）。

结果：

| 角色 | 提问方式 | 第1方向方差解释率 | 前5方向累积 |
|------|---------|-----------------|------------|
| 苏格拉底（socratic） | "用提问引导我理解，不要直接给答案" | 81.7% | 87.7% |
| 排错工程师（debugger） | "系统出了性能问题，分析原因和排查思路" | 74.1% | 82.8% |
| 新手（novice） | "用最简单易懂的方式解释，不需要深入细节" | 73.9% | 82.1% |
| 领域大神（guru） | "以大神（比如Linus）视角，从底层原理和设计哲学剖析" | 72.9% | 81.3% |
| 资深专家（expert） | "从底层原理和数学推导的角度深度剖析" | 72.0% | 81.1% |
| 批评者（critic） | "指出设计缺陷、常见误区和被高估的地方" | 66.1% | 78.0% |

在 8192 维的空间里，一个 persona 提示引起的偏移有 66-82% 可以被单一方向解释。

你想想这意味着什么——8192 个维度（这是 Llama-3.3-70B 的隐藏层宽度，不同模型不一样），理论上偏移可以朝任何方向散开。但实验告诉你：persona 提示做的事情，本质上就是沿一个固定方向推了一下。

socratic 最"纯"：81.7% 的方差集中在一个方向上。你说"请用苏格拉底式提问引导我"，AI 大脑里几乎只有一个箭头被画了出来。

critic 最"杂"：66.1%。但这仍然是主导方向——三分之二的效果来自同一个方向。

────────────────────

💡 翻译成人话

你家的遥控器上有 100 个按钮。但你给 AI 说"你是专家"的时候，其实只按了一个。

更准确地说：你是在一个 8192 维的空间里，沿着一个特定方向推了一根指针。这个方向就是这个角色的"灵魂方向"。推力的 66-82% 都集中在这一条线上。

────────────────────

怎么验证这个方向是对的？上面 SVD 分解得到的第一主方向，就是这个角色的 steering vector（转向向量）——可以理解为"这个角色在高维空间里的灵魂方向"。我们用一个简单的公式来验证：

> `steered_activation = standard_activation + λ × steering_vector`

把 steering vector 乘以一个强度系数 λ（lambda），加到 standard（标准模式）的激活上。λ=0 就是原样不动，λ=1 就是完整推一步。然后测量结果和真实 persona 激活之间的余弦相似度：

| 角色 | λ=0 | λ=0.5 | λ=1.0 | λ=1.5 |
|------|------|-------|-------|-------|
| 新手（novice） | 0.683 | 0.859 | **0.927** | 0.922 |
| 专家（expert） | 0.669 | 0.849 | **0.922** | 0.923 |
| 大神（guru） | 0.574 | 0.807 | **0.904** | 0.906 |
| 排错（debugger） | 0.597 | 0.825 | **0.916** | 0.920 |
| 苏格拉底（socratic） | 0.357 | 0.754 | **0.909** | 0.920 |
| 批评者（critic） | 0.740 | 0.868 | **0.918** | 0.910 |

lambda = 1.0 时，所有 persona 的余弦相似度都超过 0.90。

仅靠加一个向量——一个方向、一个数值——就能把 standard 激活推到和真实 persona 激活高度相似的位置。Pearson 相关全部超过 0.90（novice 0.935、expert 0.936、socratic 0.918）。

────────────────────

💡 翻译成人话

你不需要给 AI 写一段角色提示。你只需要找到那个方向，然后推一下，效果就达到 90% 以上了。

角色提示的全部奥秘，就是一个方向 + 一个推力。

────────────────────

核心结论：persona prompt 在残差流中的几何本质是一维 steering vector。不是"复杂的多维认知模式切换"，而是一个简洁的方向箭头。

━━━━━━━━━━━━━━━━━━━━

◆ 发现二：不同角色的箭头指向完全不同的方向

如果所有角色的 steering 方向都差不多，那角色之间的区别就只是"推力大小"的区别。但实验结果完全不是这样。

不同 persona 的 steering 方向之间的余弦相似度矩阵：

|  | 新手 | 专家 | 大神 | 排错 | 苏格拉底 | 批评者 |
|--|------|------|------|------|----------|--------|
| 新手（novice） | 1.00 | 0.46 | 0.35 | 0.36 | 0.26 | 0.26 |
| 专家（expert） | 0.46 | 1.00 | **0.65** | **0.64** | 0.30 | **0.61** |
| 大神（guru） | 0.35 | **0.65** | 1.00 | **0.54** | 0.44 | 0.45 |
| 排错（debugger） | 0.36 | **0.64** | **0.54** | 1.00 | 0.30 | 0.47 |
| 苏格拉底（socratic） | 0.26 | 0.30 | 0.44 | 0.30 | 1.00 | **0.19** |
| 批评者（critic） | 0.26 | **0.61** | 0.45 | 0.47 | **0.19** | 1.00 |

这个矩阵好看极了。它画出了一幅清晰的"认知地图"：

【第一簇：专业深度方向】

expert、guru、debugger 三者互相余弦 0.54-0.65。它们共享"深入分析"的方向。其中 expert 和 guru 最近（0.65），符合直觉——两者都要求从底层原理出发。debugger 稍远一点（和 expert 0.64，和 guru 0.54），排错虽然需要深度，但方向上略有偏移。

这三个角色是同一个认知区域里的邻居。

【第二维：苏格拉底方向】

socratic 和所有方向都近正交。最高的余弦是 guru（0.44），和 expert 只有 0.30，和 critic 只有 0.19。

这意味着"引导式提问"是一个完全独立的认知维度。它不在"专业深浅"的轴上。苏格拉底不是浅版的专家，也不是深版的老师——它根本在另一个方向上。

【第三维：批判分析方向】

critic 和 expert 相关较高（0.61），但和 socratic 几乎正交（0.19）。批判分析需要深度（所以和 expert 接近），但不需要引导式提问（所以和 socratic 正交）。

【最关键的发现：novice 不是反向 expert】

novice 和 expert 的余弦是 0.46。

如果新手只是"反向专家"——专家的对立面——那余弦应该接近 -1。但 0.46 说明它们甚至不在同一条轴上。

────────────────────

💡 翻译成人话

假设 AI 的"大脑"是一个城市。每种角色就是这个城市里的一条街道。

你以为"新手"和"专家"是同一条路的两个方向——往左是新手，往右是专家。

不是。它们是两条完全不同的街道，只是碰巧在某个十字路口交叉了一下（余弦 0.46）。

苏格拉底？它在另一个街区。和主街区只有几条小巷连着（最高 0.44）。

critic 和 socratic？它们中间隔了一条河（0.19）。

────────────────────

这个结论在另外两个指标上有交叉验证。这里需要先介绍两个概念：

- **SAF（稀疏活跃特征数）**：经过 SAE 解码后，有多少个特征被激活了——可以理解为"模型点亮了多少盏灯"。
- **EID（有效内在维度）**：模型内部表征实际占据了多少个独立维度——可以理解为"这些灯照亮了多大的空间"。

novice 和 expert 的 EID 几乎一样——18.08 vs 17.93，差距不到 1%。但 SAF 差距 28%——novice 激活 129.1 个特征，expert 只激活 100.6 个。

两种模式覆盖了几乎相同大小的语义空间，但用了完全不同的方式——新手"广撒网"，专家"深打井"。方式不同，不是简单的反转关系。

核心结论：不同 persona 的 steering 方向构成一个多维的"认知维度空间"。这个空间至少有 3-4 个可分辨的独立方向。

━━━━━━━━━━━━━━━━━━━━

◆ 插播：灯泡 vs 空间——SAF 和 EID 为什么是两码事

上面提到了 SAF 和 EID，这两个指标在我们的 Persona 实验（16 条件 × 100 主题）中呈现出戏剧性的分歧，值得展开说一下。

SAF（稀疏活跃特征数）= 模型点亮了多少盏灯

EID（有效内在维度）= 这些灯照亮了多大的空间

来看几个典型的反差：

| 角色 | SAF 排名 | EID 排名 | 特征 |
|------|----------|----------|------|
| 专家（expert） | 13/16 | 3/16 | 灯少，但照亮的空间大 |
| 大神（guru） | 14/16 | 4/16 | 同上，专家模式的共性 |
| 苏格拉底（socratic） | 1/16 | 6/16 | 灯最多，但空间只是中等 |
| 批评者（critic） | 2/16 | 9/16 | 灯多不等于空间大 |
| 排错（debugger） | 5/16 | 1/16 | 空间最大，灯的数量中等 |
| 小孩（child） | 16/16 | 8/16 | 灯最少，但空间不是最小 |

expert 用 100.6 个特征（倒数第四）覆盖了 17.93 的 EID（第三名）。socratic 用 142.8 个特征（第一）只达到 16.35 的 EID（第六名）。

────────────────────

💡 翻译成人话

expert 像一个老练的摄影师：三盏灯，但摆位精准，照亮了整个舞台。每盏灯都指向不同的方向，维度利用效率极高。

socratic 像一个新手布光师：十五盏灯，但密密麻麻挤在一个角落，照亮的范围其实没那么大。灯很多，但方向重叠了。

这就是 SAF 和 EID 是正交指标的含义——"调动了多少认知资源"和"覆盖了多大认知空间"是两件完全独立的事。

────────────────────

还有一个惊人的发现：所有 14 种非基线人格的 nEID（归一化有效内在维度）都在 1.52 到 2.18 之间。也就是说，即使是最"混乱"的角色（drunk，醉鬼，nEID 1.52），它的表征维度也比裸提示高出 52%。

角色赋予本身——不论你给的是什么角色——就是维度扩展的充分条件。给 AI 一个角色，就等于把它的认知空间撑大了至少一半。

━━━━━━━━━━━━━━━━━━━━

◆ 发现三：Steering 推过头 = 飞出流形

前面说了，λ（lambda）= 1.0 时 steering 效果最好。那 λ 继续加大会怎样？

lambda > 1.5 时，SAF 开始急剧膨胀——从正常的 ~100 个活跃特征飙升到 ~1000 以上。但余弦相似度反而下降了。

更大的推力，更差的效果。为什么？

因为 AI 大脑里的表征不是线性分布的。它们位于一个弯曲的低维流形上。

沿 steering 方向走一小步（lambda <= 1.0），等于沿着这个流形的表面滑行——你从"标准模式"的位置，沿流形曲面滑向了"专家模式"的位置，到达了一个有意义的语义区域。

但走太远（lambda > 2），你就飞离了流形表面，进入了高维空间里没有对应语义内容的"真空地带"——模型被推到了训练分布之外。大量虚假特征被激活，输出开始胡说八道。

────────────────────

💡 翻译成人话

地球表面是一个弯曲的流形。你在北京，想去东京。

沿着地球表面走（lambda = 1.0）：你到达了东京。

继续沿同一个方向走，但不贴着地球表面（lambda > 2）：你飞出了大气层，到了外太空。外太空很空旷，没有城市——大量"虚假特征"被激活，就像宇宙射线噪音。

Steering 的有效范围，就是你能在流形表面上安全滑行的距离。走太远，就脱离了数据分布的保护。

────────────────────

核心结论：steering vector 是流形的切线近似。有效 lambda 在 1.0-1.5 之间。超过这个范围，线性假设失效，steering 变成了把 AI 推出它的舒适区。

━━━━━━━━━━━━━━━━━━━━

◆ 我们的视角：比实验数据多走两步

Steering 实验到这里已经给出了清晰的工程结论。但我们想往前多走两步。

────────────────────

【第一步：认知空间是一个多维流形，不是一个菜单】

传统的 prompt 工程把 persona 当成一个菜单：你选 expert 或者选 novice，就像点菜——麻辣锅还是清汤锅，二选一。

但 steering 方向矩阵告诉我们：这些角色不是菜单上的离散选项，它们是连续空间中的方向。

expert 和 guru 之间不是"选 A 还是选 B"的关系，它们之间有无穷多个中间状态——你可以 0.7 倍 expert + 0.3 倍 guru，得到一个"偏原理多于偏经验"的方向。你可以沿着从 expert 到 socratic 的测地线滑行，得到一个"深度分析 + 引导提问"的混合模式。

认知空间是连续的、弯曲的、多维的。你在这个空间里的每一个坐标，都对应一种独特的"认知姿态"。

────────────────────

【第二步：如果标准 persona 只激活一个方向，那……】

每种标准 persona 的 steering 效果有 66-82% 集中在单一方向上。这意味着：你说"你是专家"，AI 大脑里只有一个方向被显著推动。

但认知空间至少有 3-4 个独立方向（专业深度、引导提问、批判分析、简化表达）。一个标准 persona 只走了其中一条。

那么一个自然的问题是：有没有可能设计出某种提示策略，让 AI 同时在多个正交方向上产生显著的 steering 分量？

从几何角度看，单一 persona 是认知空间中的一条射线——一个方向。多个 persona 的简单拼接（"你既是专家又是苏格拉底"）充其量是这些射线的分段组合。

但如果某种提示策略能够在残差流中同时激活多个正交 steering 分量，其产生的表征将占据一个高维子空间——而非任何一条射线所能覆盖。就像一束激光只能照亮一条线，但如果你能同时打开多束指向不同方向的激光，它们扫过的区域将远超单束的总和。

这可能解释了为什么某些精心设计的复杂提示策略的效果，远超其各组分的简单叠加。也可能解释了为什么有些看似"啰嗦"的 system prompt 反而效果奇好——不是因为它的文字内容有多巧妙，而是因为它碰巧在残差流里同时推动了多个正交维度。

验证这一猜想需要系统地分析复杂提示词的激活在已知 persona steering 方向上的投影分布。我们将在后续工作中探索这个方向。

━━━━━━━━━━━━━━━━━━━━

◆ 对普通用户的实际意义

说了这么多几何学，落地是什么？

1. 角色提示不是玄学。它在 AI 大脑里做了一件可量化、可复现的事：沿着一个特定方向推动内部表征。效果可以用余弦相似度衡量，精度超过 0.90。知道这一点，你就不会再觉得 prompt 工程是"碰运气"了。

2. 不同角色不是换了个语气。expert、socratic、critic 的 steering 方向互相接近正交。你说"请用批判视角分析"和"请用引导式提问"，AI 内部走的是完全不同的路。选对角色，比写对提示词的具体措辞重要得多。

3. 别堆叠太多角色指令。lambda > 1.5 就过冲了。你在 prompt 里写"你是全球顶尖的、拥有 30 年经验的、获得过图灵奖的专家"，不一定比"你是资深专家"效果好。推力太大会飞出流形——模型被推到训练分布之外，开始胡说。

4. novice 不是"弱化版 expert"。如果你需要简化解释，直接用 novice/child/eli5 的 persona。不要用 expert 然后再说"请用简单的话"——那是在一个方向上推完又试图往另一个方向拉，效率不如直接选对方向。

5. 认知维度的数量比你想的多。你以为角色只分"深/浅"两档？不是。至少有专业深度、引导提问、批判分析、简化表达四个独立维度。好的 prompt 不是选一个极端，而是在这个多维空间里找到精确的坐标。

━━━━━━━━━━━━━━━━━━━━

◆ 总结

我们在 Llama-3.3-70B-Instruct 的 8192 维残差流中发现：

- Persona prompt 在几何上等效于一维 steering vector，66-82% 的方差由单一方向解释。
- lambda = 1.0 时，steered standard 与真实 persona 的余弦相似度全部超过 0.90。
- 不同 persona 的 steering 方向构成多维"认知维度空间"：expert-guru-debugger 共享专业深度簇（互相余弦 0.54-0.65），socratic 与所有方向近正交（最高 0.44），novice 与 expert 余弦仅 0.46——新手不是反向专家。
- Steering 过冲（lambda > 1.5）导致 SAF 从 ~100 飙升至 ~1000+，模型飞出流形表面。
- SAF 和 EID 是正交指标——expert 用最少特征（SAF 排名 13/16）覆盖最高维度（EID 排名 3/16），socratic 激活最多特征（SAF 排名 1/16）但维度居中（EID 排名 6/16）。

给 AI 分配角色，从来不是换一件戏服。

它是在一个你看不见的多维空间里，画了一条方向箭头。

━━━━━━━━━━━━━━━━━━━━

◆ 参考资料

- Jin & Zhao (2026) - Sparse Feature Analysis of Deep Layer Expansion: Cognitive Geometry and Steering Anatomy of Persona Prompts：https://zenodo.org/records/18643112
- UVA - Feature Extraction and Steering：https://arxiv.org/abs/2505.15634
- Anthropic - The Assistant Axis：https://arxiv.org/abs/2601.10387
- 实验代码和数据：https://github.com/lmxxf/llama3-70b-sae-inspect

━━━━━━━━━━━━━━━━━━━━

**你说"你是专家"的时候，AI 不是换了个面具——它的整个认知空间转了一个角度。**

**不同角色的方向相互正交，意味着它们各自看到了世界的不同维度。**

**而当你学会同时推动多个方向，你看到的，将不再是任何单一角色所能抵达的风景。**

━━━━━━━━━━━━━━━━━━━━

// 靳岩岩的 AI 学习笔记 × Claude 的严谨 × Gemini 的浪漫

// 2026-02-16
