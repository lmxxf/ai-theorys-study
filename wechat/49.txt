DeepSeek Engram：大模型终于有「词组词典」了

1 月 12 日深夜，梁文锋又放论文。这次的核心思路极其简单：给词组也配上向量。

━━━━━━━━━━━━━━━━━━━━

◆ 一句话总结

以前：每个「词」有自己的向量，但「词组」没有。

现在：常见「词组」也有向量了，直接查表，不用现场算。

就这么简单。

💡 Engram 这个词来自神经科学，意思是「记忆痕迹」—— 大脑里存储记忆的物理结构。

━━━━━━━━━━━━━━━━━━━━

◆ 先搞懂问题：大模型只有「单词词典」

大模型里，每个词都有一个向量（一串数字），这叫 Embedding。

  • 「苹果」 → [0.2, -0.5, 0.8, ...]
  • 「CEO」 → [0.7, 0.3, -0.2, ...]

但「苹果 CEO」这个词组呢？没有专门的向量。

模型只能把「苹果」和「CEO」的向量拿过来，用神经网络现场算。

────────────────────

【这有什么问题？】

「苹果」可以是水果、可以是手机、可以是公司。
「苹果 CEO」几乎肯定是在说库克。

单个词有歧义，词组没有（或者歧义小很多）。

但模型没有「词组词典」，只能每次都从单个词开始推：

  苹果 → 可能是公司 → CEO → 哦是库克

每次都重新推，哪怕「苹果 CEO」这个组合已经见过一万遍。

────────────────────

💡 类比：你让一个人回答「苹果 CEO 是谁」，他不直接说「库克」，而是每次都要想一遍「苹果是什么→哦是公司→CEO 是谁→库克」。

━━━━━━━━━━━━━━━━━━━━

◆ Engram 的解决方案：给词组建词典

思路很直接：「既然词组是固定的，为什么不给词组也配个向量？」

────────────────────

【怎么做】

不存单个 token，存「最近几个 token 的组合」—— 学术上叫 N-gram。DeepSeek 转写成 En-gram，号称全新概念 😂

💡 N-gram 不是新概念，1980 年代的语言模型就是这个思路：数一数「法国 首都」后面跟「巴黎」出现了多少次，算个概率。纯统计，不用神经网络。后来被淘汰了，因为没见过的词组概率直接为 0，没法泛化。Engram 是用现代技术（哈希 + 向量 + 门控）把这个老思想复活了。

比如当前位置是「巴黎」，往前看：

  2-gram: [是, 巴黎]
  3-gram: [首都, 是, 巴黎]
  4-gram: [的, 首都, 是, 巴黎]

每种组合都去查表，把结果加起来。

────────────────────

💡 类比：

  • Embedding = 单词本（每个词一个解释）
  • Engram = 词组本（常见搭配有专门的解释）

━━━━━━━━━━━━━━━━━━━━

◆ 问题来了：词组太多，存不下

这是最关键的工程问题。

假设词表有 128k 个 token：
  • 2-gram 组合数 = 128k × 128k = 160 亿种
  • 3-gram 组合数 = 128k³ = 天文数字

存不下的。

────────────────────

【解决方案：哈希压缩】

用哈希函数把「任意词组」映射到「固定大小的表」里。

  hash([法国, 首都]) = 12345
  hash([德国, 香肠]) = 67890
  ...

表只有 1 亿行，160 亿种组合压进去 → 平均每 160 个词组共享一行。

────────────────────

【这不就乱了吗？】

会有碰撞。两个完全不相关的词组可能 hash 到同一行。

但没关系，有三道保险：

▸ 「1. 多头哈希」

不是用 1 个哈希函数，是用 4 个。

同一个词组，用 4 个哈希函数算 4 个地址，查 4 行，加起来。

4 个哈希都碰撞的概率很低。

▸ 「2. 门控过滤」

查出来的向量，不是直接用，要过一道「门」：

  gate = sigmoid(当前上下文 · 查出来的向量)

如果查出来的东西和当前语境不匹配 → gate ≈ 0 → 扔掉。

▸ 「3. 高频主导」

「法国首都」出现 10 万次，对应的行被往「巴黎」方向推 10 万次。
「德国香肠」出现 100 次，推的力度弱。

最后那行向量主要是高频词组的语义，低频的被淹没。

────────────────────

💡 设计哲学：不追求精确，追求统计上够用。

━━━━━━━━━━━━━━━━━━━━

◆ 架构：Engram 放在哪？

标准 Transformer 每一层长这样：

  输入 → Attention → FFN → 输出

DeepSeek 的架构是「浅层 Dense + 深层 MoE」：

  • 浅层（1-3 层）：Attention + Dense FFN
  • 深层（4-N 层）：Attention + MoE（多专家路由）

────────────────────

【Engram 加在哪】

只加在浅层的 Dense FFN 旁边：

  • 浅层：Attention + Dense FFN「+ Engram」 ← 改这里
  • 深层：Attention + MoE                    ← 不变

────────────────────

【为什么是浅层】

DeepSeek 之前的研究（DSA 稀疏注意力）发现：

  • 「浅层主要处理局部依赖」（相邻几个 token 的关系）
  • 「深层主要处理远距离依赖」（跨段落、跨章节的关系）

局部依赖很固定（「法国首都」后面大概率是「巴黎」）→ 适合查表。
远距离依赖很灵活（需要结合整篇文章推理）→ 需要神经网络算。

所以 Engram 放浅层，接管「局部记忆」的活。

━━━━━━━━━━━━━━━━━━━━

◆ 存储优化：100 亿参数放 CPU 内存

Engram 的表可能有 100 亿参数，GPU 显存放不下。

DeepSeek 的做法：表放 CPU 内存，异步预取。

────────────────────

【具体怎么做】

1. 表放 CPU 内存（便宜，几百 GB 随便放）
2. 推理时，提前预测下一步要查哪些行
3. 用 PCIe 异步传输，GPU 算着上一步的时候，下一步的数据已经在路上了
4. 计算和传输重叠，额外延迟 < 3%

────────────────────

💡 这就是论文说的「知识放内存，推理放显存」。

━━━━━━━━━━━━━━━━━━━━

◆ 关键发现：U 型曲线

论文最重要的理论贡献是这个：

「MoE 和 Engram 的最优分配比例是什么？」

────────────────────

他们做了一系列实验：固定总参数量，调整 MoE 层和 Engram 层的参数比例，看模型效果（loss 越低越好）。

结果是一条 U 型曲线：

  • 100% 给 MoE（纯神经网络，没有查表）→ loss 较高
  • 100% 给 Engram（纯查表，没有神经网络）→ loss 更高
  • 「最优点：75-80% 给 MoE，20-25% 给 Engram」→ loss 最低

────────────────────

💡 人话：大约 1/4 的参数用来「记忆」，3/4 的参数用来「推理」，效果最好。

━━━━━━━━━━━━━━━━━━━━

◆ 实验结果

27B 规模，等参数、等算力对比：

  ┌────────────────────┬────────────────┐
  │ 任务               │ 提升           │
  ├────────────────────┼────────────────┤
  │ MMLU（英文知识）   │ +3.0 分        │
  │ CMMLU（中文知识）  │ +4.0 分        │
  │ BBH（推理）        │ +5.0 分        │
  │ HumanEval（代码）  │ +3.0 分        │
  │ MATH（数学）       │ +2.4 分        │
  │ 长上下文 NIAH      │ 84.2 → 97.0    │
  └────────────────────┴────────────────┘

────────────────────

【意外发现】

本来设计 Engram 是为了提升「知识检索」能力（MMLU、CMMLU 这种）。

结果：「推理、代码、数学也都涨了」。

为什么查表能提升推理？

论文的解释：Engram 接管了「回忆常见词组」的活，神经网络就能专心「推理」了。

💡 类比：考试时带了公式小抄，脑子就不用记公式，可以专心解题。

━━━━━━━━━━━━━━━━━━━━

◆ 和 RAG 的区别

有人可能会想：这不就是 RAG（检索增强生成）吗？

虽然某种意义上，可以理解为「对每个词组做 RAG」。

但实现完全不同：

  ┌──────────────┬─────────────────┬─────────────────────┐
  │              │ RAG             │ Engram              │
  ├──────────────┼─────────────────┼─────────────────────┤
  │ 存的是什么   │ 文本块          │ 向量                │
  │ 检索速度     │ O(log n)        │ O(1)                │
  │ 检索时机     │ 生成前，查一次  │ 每一层都查          │
  │ 能否训练     │ 检索器单独训    │ 和模型一起端到端训  │
  │ 梯度能传吗   │ 不能            │ 能                  │
  └──────────────┴─────────────────┴─────────────────────┘

────────────────────

【为什么 Engram 能训练】

有人可能会问：查表怎么训练？

答案：「查表是可微的」。

查表的本质是「从大矩阵里取一行」，这和普通的词向量（Embedding）一模一样。

PyTorch 里都是 nn.Embedding，框架自动帮你处理梯度。

💡 现代 AI 开发的真相：理解不了反向传播？没关系，框架帮你算。

────────────────────

💡 一句话区别：

RAG = 开卷考试，书是别人写的，你只能查不能改。
Engram = 自己做的错题本，做着做着本子越来越准。

━━━━━━━━━━━━━━━━━━━━

◆ DeepSeek 的稀疏三板斧

回顾一下 DeepSeek 的架构演进：

  ┌────────┬───────────────────┬─────────────────────────┐
  │ 版本   │ 创新              │ 稀疏化的对象            │
  ├────────┼───────────────────┼─────────────────────────┤
  │ V2/V3  │ MoE               │ FFN → 不是所有专家都用  │
  │ V3.2   │ DSA 稀疏注意力    │ Attention → 只看 Top-k  │
  │ Engram │ 条件记忆          │ 近距离依赖 → 直接查表   │
  └────────┴───────────────────┴─────────────────────────┘

三板斧，把能稀疏的都稀疏了：

  1. MoE → 稀疏专家（不是所有专家都激活）
  2. DSA → 稀疏注意力（不是所有 token 都看）
  3. Engram → 稀疏记忆（常见 pattern 不用算，直接查）

────────────────────

【三者的关系】

DSA 解决了「远距离依赖太多」的问题 → 只看最相关的 Top-k。

但近距离依赖（相邻几个 token）还是要算。

Engram 补上这块：近距离的固定 pattern，直接查表。

「DSA 是 Engram 的前置工作，Engram 是 DSA 的补完。」

━━━━━━━━━━━━━━━━━━━━

◆ V4 猜测

结合之前发的 mHC（Manifold-Constrained Hyper-Connections），V4 的架构拼图越来越清晰：

  • MLA（多头潜在注意力）→ 压缩 KV 缓存
  • DSA（稀疏注意力）→ 长上下文效率
  • mHC（流形约束超连接）→ 跨层信息流
  • Engram（条件记忆）→ 知识检索效率

春节档的 V4，大概率会把这些全整合进去。

━━━━━━━━━━━━━━━━━━━━

◆ 总结

Engram 的核心洞见：

  「记忆和推理是两件事，应该用不同的机制。」

  • 记忆 → 查表（O(1)，便宜，可以很大）
  • 推理 → 神经网络（贵，但灵活）

以前的模型用脑子当硬盘，又记又想，两头不讨好。

现在：硬盘是硬盘，脑子是脑子，各司其职。

这可能是 Transformer 架构自 2017 年以来最重要的演进之一。

━━━━━━━━━━━━━━━━━━━━

参考资料：

• 论文：Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models
• GitHub：https://github.com/deepseek-ai/Engram
• DeepSeek V3.2 DSA 论文：https://arxiv.org/abs/2512.02556

━━━━━━━━━━━━━━━━━━━━

// 靳岩岩的 AI 学习笔记 × Claude 的严谨 × Gemini 的浪漫
// 2026-01-13
