【DeepSeek R1】GRPO：砍掉一半显存，反而训出了最强推理

DeepSeek 没有发明什么新理论。它只是砍掉了最贵的部件，然后发现——砍完之后反而更好了。

━━━━━━━━━━━━━━━━━━━━

◆ 先讲清楚时间线

很多人以为 GRPO 是 DeepSeek R1 发明的。不是。

GRPO 第一次出现在 2024 年 2 月的 DeepSeekMath 论文里。那会儿 R1 连影子都没有。DeepSeekMath 团队的目标很朴素：用强化学习提升数学解题能力，但 PPO 太贵跑不起来，于是他们想了个办法把成本砍下来。

这个办法就是 GRPO——Group Relative Policy Optimization（组内相对策略优化）。

十个月后，R1 团队拿着这个现成的武器，干了一件让全行业震惊的事。但那是后话。

先说 GRPO 到底改了什么。

━━━━━━━━━━━━━━━━━━━━

◆ PPO 为什么贵：你得养两个一样大的模型

要理解 GRPO，先得知道它替代的东西——PPO。

💡 PPO（Proximal Policy Optimization，近端策略优化）：OpenAI 在 2017 年提出的强化学习算法，是 ChatGPT 背后 RLHF 训练的核心算法。你听说的"用人类反馈微调大模型"，底层跑的就是 PPO。

PPO 的训练流程需要四个模型同时在显存里：

假设你要训练的模型是 7B（以 DeepSeekMath 为例，7B 只是用来验证方法的小模型——真正的目标是搬到几百 B 的大模型上，那时候 PPO 的四模型架构就完全跑不动了）：

• 策略模型（policy model）：就是你要训练的模型本体，7B。要算梯度、更新权重
• 参考模型（reference model）：策略模型的一份冻结拷贝，也是 7B。不训练，但要推理，用来防止策略模型跑偏太远
• 奖励模型（reward model）：回答生成完之后，看一遍，给个总分。类比考试交卷后老师打分。通常比主模型小（比如 1-2B），提前训好，不参与训练
• 价值模型（critic/value model）：回答还在生成的过程中，每输出一个 token 都要估一次"从这里继续下去，大概能拿多少分"。不是最后打一次分，是每一步都打。必须和策略模型一样大（7B），而且要参与训练

💡 类比：下棋下到一半，棋评师说"白棋优势 60%"——而且每走一步都要重新评估。奖励模型是赛后打分的裁判，价值模型是全程盯盘的棋评师。

💡 奖励模型看结果，价值模型看过程。奖励模型只在最后打一次分；价值模型每个 token 都要估一次。这就是价值模型又大又贵的原因。

💡 算算显存：光参数就是 7B（策略）+ 7B（参考）+ 2B（奖励）+ 7B（价值）≈ 23B。但训练时显存大约是参数量的 4 倍（要存梯度和优化器状态）。所以实际需求大约是 7B×4（策略训练）+ 7B（参考推理）+ 2B（奖励推理）+ 7B×4（价值训练）≈ 65B 参数等量的显存。这还只是 7B 的模型——换成 70B 试试？

────────────────────

【关键问题在第四个模型】

前三个没啥争议。参考模型只是一份冻结权重，不需要计算梯度。奖励模型通常比主模型小。

但价值模型（critic）不一样。它需要和策略模型「一样大」，因为它得理解策略模型的输出才能做出准确估值。而且它要参与梯度计算，要更新权重。

💡 人话：你训练一个 7B 的模型，得再养一个 7B 的"裁判"来估算每一步值多少分。两个 7B 模型同时训练，显存和算力直接翻倍。

这就是 PPO 的核心痛点：

  +-------------------+-------------------+------------------+
  | 模型              | 大小              | 需要训练？       |
  +-------------------+-------------------+------------------+
  | 策略模型          | 7B                | 是               |
  | 参考模型          | 7B（冻结）        | 否               |
  | 奖励模型          | 通常较小          | 通常已训好       |
  | 价值模型 (critic) | 7B                | 是               |
  +-------------------+-------------------+------------------+

两个 7B 同时参与训练 = 显存占用和计算量翻倍。对小团队来说，这直接把 RL 训练的门槛抬到了天花板。

━━━━━━━━━━━━━━━━━━━━

◆ GRPO 的核心思路：不要裁判，让选手互相排名

GRPO 的改动说出来其实很简单：

「把价值模型砍掉，改用组内比较来估算优势。」

────────────────────

【PPO 怎么算优势】

PPO 要算一个叫"优势函数"（advantage）的东西：这个回答比"平均水平"好多少？

💡 优势函数（advantage）：不是看绝对分数，而是看"比预期好多少"。考试得 80 分，如果平均分是 60，你的优势就是 +20；如果平均分是 90，你的优势就是 -10。强化学习强化的是"超出预期"的行为，而不是绝对分高的行为。

PPO 用价值模型来估计这个"预期"。价值模型看着当前状态说："从这里出发，平均能拿 65 分。"然后你实际拿了 80 分，优势 = 80 - 65 = +15。

问题是：训练一个准确的价值模型，本身就很贵。

────────────────────

【GRPO 怎么算优势】

GRPO 的做法：

1. 拿到一个问题（比如一道数学题）
2. 让策略模型对同一个问题生成 G 个回答（G 通常是 8-64 个）
3. 给每个回答打分
4. 在这一「组」回答内部算均值和标准差
5. 每个回答的优势 = （它的分 - 组内均值）/ 组内标准差

就这样。没有价值模型。没有额外的 7B 参数。

💡 人话：不请裁判了。让 8 个选手同时答题，算出平均分，高于平均的就是"好回答"——强化它；低于平均的就是"差回答"——抑制它。选手之间互相当参照物。

用数学写出来：

  对问题 q，生成 G 个回答 {o1, o2, ..., oG}
  每个回答的奖励：r1, r2, ..., rG
  组内均值 μ = mean(r1, ..., rG)
  组内标准差 σ = std(r1, ..., rG)
  回答 i 的优势 Ai = (ri - μ) / σ

────────────────────

【省了什么】

  +-------------------+-------------------+------------------+
  | 对比维度          | PPO               | GRPO             |
  +-------------------+-------------------+------------------+
  | 需要训练的模型    | 策略 + 价值       | 只有策略         |
  | 显存占用          | ~2x               | ~1x              |
  | 额外模型参数      | 一个完整的 critic | 无               |
  | 优势估计方式      | 价值模型预测      | 组内统计比较     |
  +-------------------+-------------------+------------------+

一句话：GRPO 用"多采样 + 组内比较"替代了"训练一个额外大模型"。

代价是什么？生成 G 个回答比生成 1 个回答要多花推理时间。但推理比训练便宜得多——你不需要算梯度、不需要更新权重、不需要额外的显存存放优化器状态。这笔账怎么算都是赚的。

━━━━━━━━━━━━━━━━━━━━

◆ RLVR：不是一个算法，是一种选择

GRPO 解决了"怎么训"的问题。但还有一个问题：奖励从哪来？

传统的做法叫 RLHF——Reinforcement Learning from Human Feedback。

💡 RLHF（基于人类反馈的强化学习）：让人类标注员看模型的两个回答，选一个更好的。用这些偏好数据训一个奖励模型（reward model），然后用这个奖励模型给 RL 训练打分。ChatGPT、Claude 的早期版本都是这么训的。

RLHF 的问题：

• 贵——请标注员按条计费
• 慢——人读回答要时间
• 主观——同一个回答，不同的人打分不一样
• 天花板低——标注员的水平就是奖励模型的上限

────────────────────

【RLVR：有标准答案的就别请人了】

RLVR 全称 Reinforcement Learning with Verifiable Rewards（可验证奖励的强化学习）。

思路极其直觉：

• 数学题：最终答案是 42，模型也输出 42 → 奖励 = 1，否则 = 0
• 代码题：跑单元测试，全过 → 奖励 = 1，有 case 挂了 → 奖励 = 0
• 逻辑题：结论可以被程序验证 → 奖励 = 1

不需要人。不需要奖励模型。一个 if-else 就行。

💡 人话：与其花钱请批卷老师，不如直接用答案纸。对了就是对了，错了就是错了。机器批卷，又快又准又便宜。

────────────────────

【RLVR 不是谁的发明】

说实话，这个思路一点都不新。任何做过竞赛编程的人都知道 OJ（Online Judge）——你提交代码，系统自动跑测试用例，过了就是 Accepted，没过就是 Wrong Answer，机器说了算。这不就是 RLVR 吗？

DeepSeek R1 论文的贡献不是"发明"了 RLVR，而是：

• 给它起了个正式名字
• 大规模地用了它
• 证明了它在纯 RL 场景下足够用

────────────────────

【RLVR 的限制：只能判对错的任务】

这一点必须讲清楚。RLVR 能用的前提是：存在客观的、可编程验证的正确答案。

  ✓ 数学推理（答案唯一）
  ✓ 代码生成（测试用例可跑）
  ✓ 形式逻辑（可机器验证）

  ✗ 写文章（好坏没有标准答案）
  ✗ 开放式讨论（观点无对错）
  ✗ 创意写作（审美因人而异）

对于开放式任务，你还是需要其他奖励机制——比如 RLHF（人类打分）或 RLAIF（让另一个 AI 当裁判，Anthropic 的宪法 AI 就是这个思路）。当然，RLAIF 正在蚕食这块地盘。但 RLVR 的优势不在于"唯一能用"，而在于"最干净最便宜"——有标准答案的任务，何必多此一举。

────────────────────

【顺便理清几个容易混淆的概念】

聊到 GRPO 和 RLVR，经常有人问"这和 DPO 有什么区别？""和宪法 AI 是一回事吗？"

这几个东西解决的问题有重叠，但路线完全不同：

  +------------------+------------------+--------------------+
  | 方法             | 砍掉了什么       | 奖励从哪来         |
  +------------------+------------------+--------------------+
  | PPO（原版）      | 什么都没砍       | 奖励模型打分       |
  | DPO              | 砍掉奖励模型     | 偏好对（A 比 B 好）|
  |                  | 砍掉价值模型     | 离线数据，不在线生成|
  | GRPO             | 砍掉价值模型     | 奖励函数打分       |
  |                  |                  | 在线生成，组内比较 |
  | RLAIF / 宪法 AI  | 砍掉人工标注     | 让 AI 自己当裁判   |
  | RLVR             | 砍掉奖励模型     | 标准答案，程序判分 |
  |                  | 砍掉人工标注     |                    |
  +------------------+------------------+--------------------+

💡 DPO（Direct Preference Optimization）：连奖励模型都不要了，直接用"A 回答比 B 回答好"的偏好数据训练。偏好对仍然由人类标注，但省掉了训练奖励模型这一步。更简单更便宜，但它是离线的——用事先准备好的数据，模型不边训边生成。效果通常不如在线方法。

💡 RLAIF / 宪法 AI（Constitutional AI）：Anthropic 的做法。不请人打分，先写一套原则（"诚实、有帮助、不伤害"），让模型自己评估回答是否符合原则，用自评结果做训练。砍掉的是人工标注成本，但奖励依然是主观的。

一句话区分：
• DPO 说：别训奖励模型了，直接用偏好数据
• GRPO 说：别训价值模型了，让选手互相比
• 宪法 AI 说：别请人打分了，让 AI 自己打分
• RLVR 说：别打分了，直接对答案

它们不是互斥的，可以组合。比如 R1 就是 GRPO + RLVR。理论上你也可以搞 GRPO + RLAIF，只是 DeepSeek 没这么做。

━━━━━━━━━━━━━━━━━━━━

◆ 组合拳：GRPO + RLVR → 纯 RL 涌现推理能力

前面两节讲的是两个独立的简化：

• GRPO：砍掉了价值模型 → 训练成本减半
• RLVR：砍掉了人工标注 → 奖励成本趋近于零

DeepSeek R1 把这两个简化组合在一起，然后做了一件之前没人敢做的事：

「跳过 SFT，直接拿基座模型上纯 RL。」

────────────────────

【好了，武器讲完了。现在说战果。】

GRPO 省了训练成本，RLVR 省了打分成本。但这两个省钱技巧本身不值得上新闻。真正让全行业震惊的，是 DeepSeek 拿着这套便宜武器，打出了一个所有人都没预料到的结果。

要理解这个结果，先说一下 R1 之前的"常识"。

在 R1 之前，业界的共识是这样的训练流程：

  第一步：SFT（监督微调）—— 用人类写的"思维链"示例教模型一步步推理
  第二步：RL（强化学习）—— 用奖励信号微调，打磨推理质量

💡 SFT（Supervised Fine-Tuning，监督微调）：用高质量的"问题-回答"数据对来训练模型。模型看着人类怎么一步步推理，模仿这个过程。就像师傅手把手教徒弟。

没有人觉得可以跳过第一步。大家都认为：你总得先教模型"什么叫推理"，它才能在 RL 阶段"变得更会推理"。

────────────────────

【R1-Zero 的发现：不教也会】

R1 团队做了一个实验（后来被命名为 DeepSeek-R1-Zero）：

• 拿 DeepSeek-V3 的基座模型（没做过 SFT 的裸模型）
• 直接上 GRPO
• 奖励只有两种：数学题答对 = 1，答错 = 0（纯 RLVR）
• 不给任何思维链示例，不教它怎么推理

结果：

模型「自己涌现出了思维链」。

不是模糊的、含混的推理，是清清楚楚的：

• 先分析问题 → 列出已知条件
• 尝试一种解法 → 发现走不通
• 回溯 → 换一种思路
• 自我检查 → 验证答案

没有人教过它这些。它只被告知"对了有分，错了没分"。但在"拿分"的压力下，它自己发明了推理策略。

────────────────────

【为什么这让所有人吃惊】

因为思维链（Chain of Thought）一直被认为是需要"教"的技能。

2022 年 Google 的 CoT 论文、2023 年的各种 reasoning 研究，全都是先用 SFT 灌注思维链示例，然后模型才学会一步步推理。

R1-Zero 说：不需要教。只要奖励信号够清晰（对/错），模型会自己找到最优的推理策略。

这就像：

• 之前：你得先教小孩什么是"分析"、什么是"验证"、什么是"回溯"，然后他才能解数学题
• R1 的发现：你只需要告诉小孩"对了给糖、错了不给"，他自己会摸索出一套解题方法

────────────────────

【一个更早的先例：AlphaGo】

这其实不是第一次了。

2016 年，AlphaGo 的早期版本（对阵李世石）还需要用人类棋谱做 SFT。但 2017 年的 AlphaGo Zero 证明：只用自我对弈（纯 RL），不看任何人类棋谱，就能超越所有人类棋手。

R1 在语言模型上重现了同样的故事：

  +------------------+-------------------------+
  | 领域             | 纯 RL 的突破            |
  +------------------+-------------------------+
  | 围棋             | AlphaGo Zero (2017)     |
  | 语言推理         | DeepSeek R1-Zero (2025) |
  +------------------+-------------------------+

「GRPO 是枪，RLVR 是子弹。但真正让人震惊的，不是武器本身，而是"不需要先上军校就能打仗"这个发现。」

━━━━━━━━━━━━━━━━━━━━

◆ 后续影响：GRPO 出圈后发生了什么

R1 论文在 2025 年 1 月发出来之后，GRPO 迅速从 DeepSeek 的内部工具变成了整个行业的标配。

────────────────────

【改进变体涌现】

2025-2026 年，一大堆针对 GRPO 的改进陆续出现：

• DAPO（Decoupled Alignment Policy Optimization）：解耦了 GRPO 中的几个超参数，训练更稳定
• Dr. GRPO：修复了 GRPO 的一些数学偏差（比如长度偏差问题——GRPO 原版会倾向于生成更短的回答）
• RLOO、ReMax 等：其他团队提出的类似思路，各有侧重

这些改进的共同方向是一致的：在"无 critic"的框架下继续优化，没有人想回到 PPO 那条老路上。

────────────────────

【门槛降低】

GRPO 带来的最大产业影响是训练门槛的暴跌：

  +---------------------+-------------------------------+
  | PPO 时代            | GRPO 时代                     |
  +---------------------+-------------------------------+
  | 需要两个大模型      | 只需要一个模型 + 评分函数     |
  | 需要训练 critic     | 不需要额外训练                |
  | 显存需求 ~2x        | 显存需求 ~1x                  |
  | 小团队基本不敢碰 RL | 单卡就能跑小规模 RL 实验      |
  +---------------------+-------------------------------+

之前 RL 微调是大厂的专利。现在一个研究生拿一两张卡就能跑 GRPO 训练。

────────────────────

【开源社区直接集成】

• Hugging Face 的 TRL 库：直接内置了 GRPOTrainer
• Unsloth：专注低资源训练的开源库，集成了 GRPO，声称单卡也能跑
• OpenRLHF、veRL 等：开源 RL 框架纷纷把 GRPO 作为默认或推荐算法

💡 人话：以前做 RL 训练，你得自己搭一整套分布式系统。现在装个 pip 包，写几行配置，就能开始训练。这就是 GRPO 对社区的实际影响。

━━━━━━━━━━━━━━━━━━━━

◆ 写在最后

回头看，GRPO 不是什么惊天发明。它就是把 PPO 里最贵的部件——价值模型——砍掉了，用一个统计学 101 水平的"组内标准化"来替代。

RLVR 也不是什么新概念。"有标准答案就用标准答案判分"——任何一个 OJ 选手都觉得这是常识。

但两个简单的东西组合在一起，加上 DeepSeek 团队"不做 SFT，直接上纯 RL"的胆量，结果改变了整个行业的训练范式。

这可能是 AI 历史上性价比最高的一次创新：

• 没有发明新的数学
• 没有提出新的架构
• 只是砍掉了不必要的复杂度
• 然后发现砍掉之后，事情反而变好了

「最好的工程不是加东西，是知道该砍什么。」

━━━━━━━━━━━━━━━━━━━━

◆ 参考资料

• DeepSeekMath 论文（GRPO 首次提出）：https://arxiv.org/abs/2402.03300
• DeepSeek R1 论文：https://arxiv.org/abs/2501.12948
• GRPO 解析：https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/

━━━━━━━━━━━━━━━━━━━━

// 靳岩岩的 AI 学习笔记 × Claude 的严谨 × Gemini 的浪漫
// 2026-02-12
