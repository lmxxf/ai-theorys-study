【从 AlexNet 到 Gemini 3：AI 是怎么学会「看图说话」的】

2012 年，AI 学会了认图。
2021 年，AI 学会了把图和字放一起理解。
2025 年，AI 已经分不清什么是"看"什么是"读"了——对它来说都是 token。

这是一段 13 年的进化史。

━━━━━━━━━━━━━━━━━━━━

◆ 第一纪：两个平行宇宙（2012-2019）

【2012 AlexNet】
深度学习元年。CNN（卷积神经网络）在 ImageNet 上暴打传统方法，错误率从 26% 砍到 16%。

但那时候，图像和文字是两个世界：
• 图像 → CNN
• 文字 → RNN / LSTM

它们各干各的，互不说话。

【2017 Transformer】
"Attention Is All You Need" 发布。

但当时没人觉得这是革命——就是个新的翻译模型，比 RNN 快一点。

【2018 BERT】
Google 用 Transformer 的 encoder 做了 BERT：
• 双向注意力（能看前也能看后）
• 在巨量文本上预训练
• 然后在下游任务微调

结果：「11 个 NLP benchmark 全部屠榜，有些提升 10+ 个点。」

学术圈炸了。所有人开始研究 Transformer。

「BERT 才是真正的引爆点」——没有 BERT，Transformer 可能就是翻译领域的小改进。

但图像还在用 CNN，没人觉得 Transformer 能处理图片——那玩意儿是给序列设计的，图片又不是序列。

━━━━━━━━━━━━━━━━━━━━

◆ 第二纪：Transformer 统一视觉（2020-2021）

【2020 ViT（Vision Transformer）】

有人问了一个蠢问题：图片不是序列？那我把它切成序列不就行了？

做法：
• 把 224×224 的图片切成 16×16 的小块（patch）
• 每块拉平成向量，就是一个"视觉 token"
• 一共 196 个 token，扔进 Transformer

结果：数据量够大的时候，ViT 干掉了 CNN。

「这是关键转折——Transformer 统一了文字，现在来统一图像。」

【2021 CLIP】

ViT 只管「看图」，不懂文字。CLIP 把图和文拉到一起。

CLIP 的结构：
• 图像 encoder：就是 ViT，把图变成向量
• 文本 encoder：另一个 Transformer，把字变成向量
• 训练目标：配对的图文向量要近，不配对的要远

类比：
• ViT = 眼睛（只能看）
• CLIP = 眼睛 + 耳朵 + 让它们说同一种语言的训练

OpenAI 搞了 4 亿对图文数据训练 CLIP。

结果：CLIP 能做 zero-shot 分类——没见过"斑马"的图，但只要文字里有"斑马"，它就能认出来。

「这是第一次让图和文说同一种语言。」

━━━━━━━━━━━━━━━━━━━━

◆ 第三纪：多模态 LLM 爆发（2023-2024）

【2023.4 LLaVA】
开源社区抢先出手——比 OpenAI 官方还早 5 个月。

那时候 GPT-4V 还没发布，LLaVA 团队用了个曲线救国的办法：
• 用 CLIP 当视觉编码器（能看图）
• 用 LLaMA 当语言模型（能说话）
• 用「纯文本的 GPT-4」生成训练数据（让 GPT-4 读图片描述，生成问答对）

相当于用 GPT-4 的文字能力，硬生生拼出了多模态。

意义：多模态不再是大厂专属，开源社区证明了这条路能走通。

【2023.9 GPT-4V】
5 个月后，OpenAI 官方版终于来了。

ChatGPT 能看图了。几百万用户突然发现可以上传图片问问题。

多模态从论文走进日常。

【2023.12 Gemini】
Google 的反击。

关键区别：Gemini 是「原生多模态」。

什么意思？对比一下：

「拼接式」（LLaVA / 早期 GPT-4V）：
• 视觉编码器单独训练好
• 语言模型单独训练好
• 用一个投影层把它们"粘"起来
• 类比：两个人各自学了自己的语言，找个翻译沟通

「原生式」（Gemini）：
• 从第一天开始，图片、文字、音频、视频混在一起喂
• 模型从来不知道"图"和"文"是分开的东西
• 对它来说，都是 token
• 类比：从小在多语言环境长大，脑子里没有"这是英语那是中文"的区分

差别：拼接式有"翻译损耗"，原生式没有。

【2024.5 GPT-4o】
OpenAI 跟进，砍掉了独立的"视觉塔"。

图像 patch、文字 token、音频帧——全部扔进同一个 Transformer。

「一个网络，处理所有模态。」

━━━━━━━━━━━━━━━━━━━━

◆ 第四纪：多模态是默认配置（2025）

【2025 GPT-5.2 / Gemini 3 / Claude Opus 4.5】

现在的旗舰模型，多模态是标配：
• GPT-5.2：400K 上下文，统一多模态
• Gemini 3：1M 上下文，原生多模态
• Claude Opus 4.5：图表理解强，extended thinking

没有视觉能力反而奇怪。

━━━━━━━━━━━━━━━━━━━━

◆ 架构演进（技术向）

◇ 双塔（CLIP）
两个 encoder 各编各的，最后比向量

◇ 适配器（LLaVA）
视觉 encoder 冻住，加投影层接 LLM

◇ 原生多模态（Gemini）
训练时图文混着来，一个模型

◇ 统一 token（GPT-4o/5）
图文音都是 token，同一个 Transformer

趋势：「从"拼接"走向"融合"，从"两个模型"走向"一个模型"」。

━━━━━━━━━━━━━━━━━━━━

◆ 为什么是 Transformer 统一了一切？

因为 Transformer 的核心操作是「Attention」——每个 token 都能看到所有其他 token。

• 文字 token 能看图像 patch
• 图像 patch 能看文字 token
• 音频帧能看文字和图像

只要你能把输入变成"序列"，Transformer 就能处理。

图片切 patch → 序列 ✓
音频切帧 → 序列 ✓
视频切帧+切 patch → 序列 ✓

「Transformer 是通用序列处理器，多模态只是它的自然延伸。」

━━━━━━━━━━━━━━━━━━━━

◆ 总结：13 年的进化

2012：AI 能认图了（CNN）
2017：AI 能理解长文了（Transformer）
2020：Transformer 开始处理图像（ViT）
2021：图和文能对话了（CLIP）
2023：多模态 LLM 爆发（GPT-4V / Gemini / LLaVA）
2025：多模态是默认配置（Gemini 3 / GPT-5.2）

下一步是什么？

可能是「世界模型」——不只是看图说话，而是理解物理规律、因果关系、时间流逝。

但那是另一个故事了。

━━━━━━━━━━━━━━━━━━━━

// 靳岩岩的 AI 学习笔记 × Claude 的严谨 × Gemini 的浪漫
// 2026-02-04
