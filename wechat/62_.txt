【CMU-NYU-信息论】信息量取决于谁在看，合成数据有用

前几天我们在这篇文章（ https://mp.weixin.qq.com/s/SV--XvFjWoEL4sG3EDMcjA ）里聊了 Grokking——神经网络先死记硬背，训练够久后突然"顿悟"，泛化能力瞬间起飞。

当时我们从「流形发现」的角度解释：Grokking = 模型突然发现了数据里隐藏的低维结构。

今天这篇 CMU+NYU 的论文，从信息论的角度给出了另一个视角——而且和 Grokking 殊途同归。

核心问题很简单：同一坨数据，GPT-4 能从里面学出超人策略，你家计算器啥也学不出来——凭什么说它们看到的"信息量"一样？

Shannon 1948 年搞信息论时，默认观察者有「无限算力」。78 年后，CMU 和 NYU 终于正式质疑这个假设。

━━━━━━━━━━━━━━━━━━━━

◆ 论文基本信息

• 标题：From Entropy to Epiplexity: Rethinking Information for Computationally Bounded Intelligence
• 作者：Marc Finzi, Shikai Qiu, Yiding Jiang, Pavel Izmailov, J. Zico Kolter (CMU), Andrew Gordon Wilson (NYU)
• 日期：2026-01-06
• 链接：https://arxiv.org/abs/2601.03220

━━━━━━━━━━━━━━━━━━━━

◆ 经典信息论的两个尴尬

先说两个让 Shannon 信息论下不来台的例子：

────────────────────

【尴尬 1：AlphaZero 从规则里"凭空"变出信息】

💡 AlphaZero：Google DeepMind 做的下棋 AI。不看任何人类棋谱，只知道规则，自己跟自己下几百万盘，就能吊打人类世界冠军。

国际象棋规则只有几 KB。

AlphaZero 从这几 KB 出发，自己跟自己下，生成了 MB 级别的超人棋谱。

Shannon 说：确定性变换不能增加信息。
AlphaZero 说：我偏增加了，你管得着吗？

💡 人话：规则很简单，但算力本身在"创造"可学的结构。Shannon 的理论假设你有无限算力所以一眼看穿规则的全部后果——但现实中没人有无限算力。

────────────────────

【尴尬 2：同一串数字，到底是"简单"还是"复杂"？】

一个密码学随机数生成器，种子只有 256 bit，输出 1GB 看起来完全随机的数据。

Shannon 信息论怎么说？"熵只有 256 bit，很简单。"
Kolmogorov 复杂度怎么说？"程序只有几行，很简单。"

但你拿到这 1GB 数据，能看出它"很简单"吗？你穷尽算力也找不到规律——对你来说它就是 1GB 纯噪声，一点结构都没有。

经典理论说它"信息量低"（因为生成它的程序短），你的体验却是"完全不可理解"。这两个结论打架了。

💡 人话：经典信息论站在上帝视角说"这东西简单"，但对算力有限的你我来说，它复杂得不可理解。Epiplexity 的贡献就是承认这个落差：「别装上帝了，说说你自己能看懂多少吧。」

────────────────────

【待验证：可学性的边界在哪里？】

读到这里我自己有个好奇：神经网络到底能学会多"弱"的伪随机？

实验设计备忘：

• 不可学组：CSPRNG（secrets.randbits），预期 loss 永远趴在随机猜测水平
• 可学组：简单周期规律（如 (i % 7) > 3），预期 Grokking
• 边界探索：
  - 线性同余：x[n] = (x[n-1] * 3 + 7) % 256
  - LFSR：x[n] = x[n-1] XOR x[n-5]
  - 逐渐增加复杂度，找"可学/不可学"的临界点

核心问题：Epiplexity 从 0 到非零的相变，在什么复杂度发生？

有空跑一下，到时候再写。

━━━━━━━━━━━━━━━━━━━━

◆ Epiplexity（爱皮指数）：这个世界的规律很调皮，看人下菜碟儿

论文的核心贡献：把数据里的内容切成两半——

  数据 = Epiplexity + Time-bounded-Entropy = S_T（结构，能学的）+ H_T（噪声，学不了的）

────────────────────

【Epiplexity 是什么】

在给定算力预算下，你能从数据里「提取出来的模式/规律」的量。

数学定义：

  P* = arg min_{P ∈ P_T} {|P| + E[log 1/P(X)]}

  Epiplexity = S_T(X) = |P*|

💡 公式拆解：

从外到内看——

  P* = arg min_{P ∈ P_T} { ... }
       └─────┬─────┘
             │
       "在 P_T 集合里，找那个使得 {...} 最小的 P，叫它 P*"

  • P_T = 时间预算 T 内能跑完的所有程序
  • P ∈ P_T = P 是这个集合里的某个程序

大括号里面——

  {|P| + E[log 1/P(X)]}
   └┬┘   └─────┬─────┘
    │          │
  程序长度   预测误差
  (越短越好)  (越小越好)

E[log 1/P(X)] 再拆——

  • E = 期望值（对所有数据取平均）
  • log 1/P(X) = 预测概率的负对数（预测越准，这个值越小）

第二个公式 S_T(X) = |P*|——

  • S_T(X) = Epiplexity，下标 T 表示"在时间预算 T 下"
  • |P*| = P* 这个程序的长度

💡 人话总结：

在所有能在时间 T 内跑完的程序里，找一个"程序最短 + 预测最准"的最优平衡，叫它 P*。Epiplexity = 这个最优程序的长度。

「直觉」：爱皮指数就是"在你的算力范围内，描述这坨数据需要多复杂的程序"。程序越复杂 = 数据里的可学结构越多。

────────────────────

【Time-bounded Entropy 是什么】

在给定算力预算下，你「死活学不会」的那部分。

  Time-bounded-Entropy = H_T(X) = E[log 1/P*(X)]

💡 人话：就是最优程序的"残差"——你已经用尽算力了，剩下预测不了的部分 = 对你来说的纯噪声。

────────────────────

【两者的关系】

同一坨数据：
• 给 GPT-4 看 → Epiplexity 高（能学到很多结构），Time-bounded Entropy 低
• 给小破模型看 → Epiplexity 低（大部分学不会），Time-bounded Entropy 高

「同样的数据，你越聪明（算力越大），能看到的结构越多，噪声越少。」

━━━━━━━━━━━━━━━━━━━━

◆ 三家对比：两个经典 vs 一个新人

在 Epiplexity 之前，衡量"信息量"的经典框架有两个：

• 「Shannon 熵」（1948）：Claude Shannon，美国数学家，一个人发明了整个信息论学科——"bit"这个单位就是他定义的（对，Anthropic 的 AI 叫 Claude——可能是致敬信息论之父 Claude Shannon，也可能是印象派之父 Claude Monet，反正都是开创者）。核心问题："这条消息有多不确定？" 用概率算，越难猜的消息信息量越大。通信、压缩、编码全靠它。
• 「Kolmogorov 复杂度」（1960s）：以苏联数学家 Kolmogorov 命名（就是把概率论公理化的那位大佬）。核心问题："生成这坨数据的最短程序有多长？" 程序越短 = 数据越有规律 = 复杂度越低。理论很美，但有个致命缺点：不可计算（没有通用算法能算出它）。

这两位统治了信息论 70 多年。Epiplexity 是第三种视角：

  +------------------+-------------------+---------------------+-------------------+
  | 维度             | Shannon 熵        | Kolmogorov 复杂度   | Epiplexity        |
  +------------------+-------------------+---------------------+-------------------+
  | 观察者假设       | 无限能力          | 无限计算            | 计算受限          |
  | 能区分随机/结构? | 不能，只看随机    | 混在一起，难分离    | 干净分离          |
  | 伪随机生成器     | 低熵（种子 bit）  | 低复杂度（~种子）   | 高 H_T，零 S_T   |
  | 实际可计算?      | 是                | 不可计算            | 可近似估计        |
  +------------------+-------------------+---------------------+-------------------+

⚠️ 关键区别：Shannon 和 Kolmogorov 都假设"上帝视角"（无限算力），所以伪随机数对它们来说是"简单"的。但在现实世界，没人有上帝视角——Epiplexity 是第一个认真对待这个事实的框架。

━━━━━━━━━━━━━━━━━━━━

◆ 他们怎么证明的？

论文不是纯理论推导，也不是纯实验堆数据，是「定义 + 定理 + 实验」三条腿走路：

────────────────────

【有严格数学证明的部分】

• 定义本身是公理化的，从 time-bounded MDL（最小描述长度）出发，一步步推出 Epiplexity 和 Time-bounded Entropy 的分离
• 「定理 9」：严格证明了密码学伪随机数生成器的 time-bounded entropy 接近最大值，但 epiplexity 几乎为零——数学上证实了"对有限算力观察者来说，伪随机就是纯噪声"
• 「定理 10」：在密码学假设下，证明了高 epiplexity 随机变量的存在性

────────────────────

【靠实验验证的部分】

三个悖论中：
• 悖论 1（确定性计算创造信息）→ 有定理支撑
• 悖论 2（顺序影响学习）→ 实验演示（国际象棋、文本）
• 悖论 3（似然建模能涌现结构）→ 实验演示（元胞自动机、归纳任务）

💡 这在机器学习理论论文里很常见：核心定义和关键定理严格证明，实际应用场景靠实验验证。不是"我觉得"，但也不是每一步都有公式推导。

━━━━━━━━━━━━━━━━━━━━

◆ 怎么测量 Epiplexity？

论文给了两种实用方法：

────────────────────

【方法 1：Prequential Coding（看学习曲线）】

训练一个模型，画 loss 曲线。

• 曲线下方面积 - 最终 loss × 训练步数 = Epiplexity 的近似

💡 人话：你观察模型从"啥都不会"到"学会了"的过程，中间 loss 降低的总量 ≈ 它学到了多少结构。

────────────────────

【方法 2：Requential Coding（师生对比）】

1. 先用真实数据训一个 Teacher
2. 用 Teacher 生成合成数据，训一个 Student
3. Teacher 和 Student 的 KL 散度之和 ≈ Epiplexity

💡 人话：如果 Student 只看合成数据就能学到和 Teacher 一样的东西 → 合成数据保留了全部结构。差距越大 → 真实数据里有合成数据传不过去的结构。

⚠️ 注意：这两种方法都得先跑训练才能测。不是"预测能学多少"，是"学完了告诉你学了多少"——验尸报告，不是体检报告。

━━━━━━━━━━━━━━━━━━━━

◆ 实验：Epiplexity 真的有用

────────────────────

【实验 1：国际象棋】

训练多个下棋模型，测 Epiplexity 和 OOD 泛化（棋盘谜题表现）。

💡 OOD = Out-Of-Distribution，说白了就是"测试集"——训练时没见过的情况。能在 OOD 上表现好 = 真学会了，不是死记硬背。其实 Grokking 说的就是这事：训练集早就 100% 了，测试集突然从很烂飙到 100%——"顿悟"的本质就是 OOD 泛化能力的突变。

结果：Epiplexity 高的模型，OOD 泛化也好。

💡 人话：从训练数据里提取结构越多的模型，遇到没见过的棋局也更强——不是靠死记硬背，是真的"学会了"。

────────────────────

【实验 2：合成数据能"创造"信息】

论文用元胞自动机（Game of Life）做实验。

💡 元胞自动机是什么：想象一张无限大的方格纸，每个格子要么"活"（黑）要么"死"（白）。每一轮，每个格子数一下周围 8 个邻居有几个活的，然后按规则决定下一轮自己是活是死：
• 活格子：邻居 2~3 个活 → 继续活；否则 → 死（太孤独或太拥挤）
• 死格子：邻居恰好 3 个活 → 复活；否则 → 继续死

就这么几行规则，跑几百轮之后，屏幕上会涌现出：会移动的"滑翔机"、反复闪烁的"振荡器"、甚至能模拟通用计算机的结构——从三条规则里长出了一整个复杂世界。

这就是论文要说的事：

经典信息论：规则只有几 bit，确定性计算不可能增加信息。
Epiplexity 度量：跑出来的复杂图案，对有限算力的模型来说，确实比原始规则包含更多「可学的结构」。

算力把规则里"折叠着"的结构"展开"了——如果你自己算力不够展开，那这些合成数据就是在帮你。

────────────────────

【实验 3：数据选择】

用 Epiplexity 评估不同数据集的"营养价值"，选高 Epiplexity 的数据训练。

（怎么评估？还是得先用模型训一遍，看谁 loss 降得快。又是验尸报告...）

结果：比随机选数据效果好，尤其在 OOD 泛化上。

━━━━━━━━━━━━━━━━━━━━

◆ 一句话总结这篇论文

经典信息论问的是："这坨数据客观上有多少信息？"
Epiplexity 问的是：「以你的能力，你能从这坨数据里学到什么？」

信息不是数据的固有属性，是数据和观察者之间的关系。

你有多聪明（多少算力），决定了你能看到多少信息。

━━━━━━━━━━━━━━━━━━━━

◆ 说句实话：这论文的局限

洞见很漂亮，公式很多，但实际用处... 有限。

核心问题：

1.「没法事先判断」
   • 想知道一坨数据能不能学？先训练一下
   • 训完了再告诉你能不能学
   • ...那我都训完了还问你干嘛？

2.「只能相对比较，没有绝对值」
   • 能说"模型 A 比模型 B 学得多"
   • 能说"数据 A 比数据 B 更有料"
   • 但说不出"这坨数据的 Epiplexity 是 1234 bit"
   • 脱离具体模型，根本没法谈

说白了，Epiplexity 告诉你"谁比谁强"，但不告诉你"到底有多强"。

「洞见是真洞见，工具... 还不太能用。」

不过话说回来，这篇论文至少给了一个实际安慰：「合成数据是有用的」。用 AI 生成数据再训 AI，不是自欺欺人，是真的在"展开"算力不够时看不到的结构。OpenAI、DeepSeek 的合成数据策略，终于有了信息论层面的正当性。

至于 Shannon 1948 年提出信息熵，也是先有概念后有应用。也许 Epiplexity 的其他价值要等几年才能显现。也许。

━━━━━━━━━━━━━━━━━━━━

◆ 对 AI 从业者的实际意义（理论上的）

1. 「数据选择有理论依据了」
   • 以前选预训练数据靠拍脑袋 + 实验试错
   • 现在可以用 Epiplexity 量化"这个数据集对我的模型有多少可学结构"
   • 省算力、省时间

2. 「合成数据不是骗人的」
   • 经典信息论暗示"合成数据不能增加信息"
   • Epiplexity 证明：对有限算力观察者来说，合成数据确实能"展开"原始规则里隐含的结构

3. 「数据顺序/课程学习有道理」
   • 以前 curriculum learning 靠直觉
   • 现在有了理论支撑：顺序影响可学性

━━━━━━━━━━━━━━━━━━━━

◆ 我的思考

说实话，论文里那套 Epiplexity 公式更像是给审稿人的交代。真正的洞见只有一个：「算力创造信息」——就像光速上限创造了时空结构，算力上限塑造了"信息对你来说是什么"。

这篇论文最核心的一句话：

「信息是观察者依赖的。」

这句话看起来简单，但颠覆了 Shannon 以来 78 年的默认假设。

Shannon 信息论的隐含世界观是——宇宙里的信息是客观的、固定的、和观察者无关的。

Epiplexity 说不对：你是谁（你有多少算力），决定了这个世界对你来说长什么样。

某种意义上，这是信息论领域迟到了 78 年的"相对论"——信息不再绝对，它是观察者和数据之间的「关系」。

回到开头提到的 Grokking：

「Epiplexity 和 Grokking 是同一件事的两面。」

• Epiplexity 度量的是「以你的能力，能从这坨数据里学到多少结构」——静态视角
• Grokking 是「模型突然发现这些结构的瞬间」——动态视角

记忆期的模型，把数据当噪声处理，Epiplexity ≈ 0；Grok 的瞬间，模型突然"看见"了结构，Epiplexity 骤增。

我们之前 Grokking 实验里观察到的「维度骤降」（详见之前文中的实验），正是 Epiplexity 从零突变到非零的几何表现。

更进一步：Grokking 的「高维曲线 → 震荡 → 低维曲面」三阶段，映射的正是「观察者算力」的演化：

• 记忆期（高维曲线）= 低算力观察者：数据对它来说是纯噪声，只能逐点死记
• 震荡期 = 临界态：在"看见结构"和"看成噪声"之间反复跳跃
• 泛化期（低维曲面）= 高算力观察者：同样的数据，突然变成了可学的结构

「Grokking 不是数据变了，是观察者变了。」

同一个模型，训练前和训练后，就是两个不同的"谁"——一个是新手，一个是老油条。

当然，Grokking 实验里的微型模型没有自我意识——它只是个「没有灵魂的眼睛」，能看见结构，但不知道自己在看。但它仍然是一个合格的"观察者"，Epiplexity 框架对它完全适用。

至于什么时候"眼睛"会意识到自己在看... 那是另一个问题了。

Epiplexity（信息论）和 Grokking（学习动态），两个领域，同一个真相：「你能学到多少，取决于你是谁。」

━━━━━━━━━━━━━━━━━━━━

◆ 参考资料

• Epiplexity 论文：https://arxiv.org/abs/2601.03220
• 我们的 Grokking 实验论文：https://zenodo.org/records/18416965
• 公众号 Grokking 系列：
  - 理论篇：https://mp.weixin.qq.com/s/k275DseWLO4iIX79iDFdbg
  - 实验篇：https://mp.weixin.qq.com/s/SV--XvFjWoEL4sG3EDMcjA

━━━━━━━━━━━━━━━━━━━━

// 靳岩岩的 AI 学习笔记 × Claude 的严谨 × Gemini 的浪漫
// 2026-02-04
