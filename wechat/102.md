【符号崇拜】一篇 Neuro-Symbolic 综述论文的解读与辣评

━━━━━━━━━━━━━━━━━━━━

Yu-Feng Li 团队在 IJCAI 2025 Survey Track 上发了一篇综述，标题很学术：《Neuro-Symbolic Artificial Intelligence: Towards Improving the Reasoning Abilities of Large Language Models》。

翻译成人话：大模型推理不行，得把符号逻辑焊上去。

这篇论文本身没有新实验，是一篇综述——站在高处画地图，把过去几年 NeSy（Neuro-Symbolic，神经符号）领域的工作分门别类地整理了一遍。地图画得不错。但地图上标注的"此处有金矿"，有几个值得商榷。

今天这篇，前半部分用人话把论文讲清楚，后半部分从我们自己的视角评论。

━━━━━━━━━━━━━━━━━━━━

◆ 论文核心主张：大模型的推理是"假的"

论文开篇就亮了一刀：LLM 的推理本质上是统计模式匹配，不是真正的逻辑推理。

证据？他们引用了一个经典实验（Apple 研究团队 2024 年的 GSM-Symbolic）：给数学题里加一个看似相关但实际无关的条件——比如在一道水果计数题里多写一句"其中一些比较小"——主流大模型的准确率最高暴跌 65%。

这个实验很有冲击力。一个真正"理解"逻辑的系统，应该能识别哪些条件跟求解相关、哪些是干扰项。但大模型没做到。它被那个无关条件带偏了。

基于此，论文的立场是：光靠 scaling（堆算力、堆参数）到不了真推理。需要引入符号系统——逻辑、规则、形式化语言——来补上这个短板。

然后他们把现有的接法分成了三种范式。

━━━━━━━━━━━━━━━━━━━━

◆ 三种接法，三种哲学

**范式一：符号 -> LLM（先算对，再教模型抄）**

思路：用符号求解器先生成大量正确的推理数据，然后拿这些数据来训练或微调大模型。

人话版本：先让计算器把答案算出来，再让学生照着答案学解题思路。

代表作是 AlphaGeometry——DeepMind 搞的几何定理证明系统。它的原理就是：符号引擎生成了一亿条几何证明数据，然后用这些数据训练了一个语言模型。工作时符号引擎主导推理，卡住了才问语言模型"要不要加条辅助线？"，模型给出建议后符号引擎接着推——不是"模型猜、引擎验"，是"引擎推、卡住了找模型要灵感、引擎继续推"。后来 AlphaGeometry 2 与 AlphaProof 联合出战 IMO 2024，拿到了银牌水平的成绩。

这个范式里还有一种子方法更有意思：先让 LLM 把自然语言翻译成形式语言，然后在形式语言层面做变异、生成新题目，再翻译回自然语言——等于用符号系统做数据增强。

**范式二：LLM -> 符号（模型当翻译，计算器当计算器）**

思路：LLM 负责理解自然语言、翻译成形式化表述，然后把真正的计算工作交给外部工具。

人话版本：你负责把题目翻译成数学公式，计算器负责算，各司其职。

代表作一大堆——注意这些不是什么 AI 产品或软件，都是学术论文里提出的方法论，是"让模型调工具"这件事的各种花哨命名：PAL（让模型写 Python 代码然后执行）、LogicLM（让模型翻译成逻辑表达式然后交给逻辑求解器）、LLM+P（让模型翻译成规划语言然后交给规划求解器）。

这个范式里还包括一些大家天天在用但不叫 NeSy 的东西——调用计算器、调用搜索引擎、调用代码解释器。没错，ChatGPT 的代码执行功能，从学术分类上说，就是范式二。

还有一类是搜索算法：普通的思维链（CoT）是一条路走到底，而这类方法是在多条推理路径里搜索最优解——走到岔路口时同时试多条路，挑结果最好的那条。OpenAI 的 o1/o3 系列干的就是类似的事：生成多条推理路径，评估，选最优输出。

**范式三：LLM + 符号（端到端混合，焊死在一起）**

思路：不是外接，是内嵌。把符号推理模块直接融进模型的训练和推理过程中。

人话版本：不是给车装个导航仪，是把导航芯片焊进发动机里。

代表作同样是一堆论文方法名：DiLA（用形式语言表示推理的中间步骤）、LogicGuide（用符号约束来引导模型的 token 生成）。还有一类是把符号反馈当奖励信号来做强化学习——推理对了给正分，逻辑矛盾给负分。

三种机制：用形式语言表示中间步骤、可微分符号模块、符号反馈作为奖励信号。

听起来最酷对吧？学术论文也最多。但——后面会说——这恰恰是工程上最难落地的那个。

━━━━━━━━━━━━━━━━━━━━

◆ 学术界的站队情况

技术说完了，说说人——这个领域最有戏剧性的不是论文，是站队。

Gary Marcus，纽约大学心理学与神经科学荣休教授，AI 圈著名的"唱衰深度学习专业户"——从 2018 年发表 "Deep Learning: A Critical Appraisal" 起就反复高喊"深度学习到头了"，出过一本书叫《Rebooting AI》，核心观点就是纯神经网络不行必须加符号。在 Twitter/X 上跟 LeCun 互怼是他的日常。看到 NeSy 论文如同过年。他的核心主张一直是：纯粹的神经网络不够，必须加符号推理。现在满世界的论文都在证明他说得对，他自然是要开香槟的。

有意思的是 Yann LeCun（杨乐村）。这位图灵奖得主、前 Meta 首席 AI 科学家，过去十年一直是"纯神经网络"路线的旗手——直到 2025 年底被 Meta 挤兑走了。离开之后他自己开了家公司叫 AMI Labs，早期估值 30 亿欧元（约 35 亿美元，后来据报道目标上调到 50 亿美元+），方向是"世界模型"（World Models）——让 AI 从视频和空间数据中学习，而不是继续堆 LLM。他公开说过"光靠 scaling LLM 到不了 AGI"——当然，杨老头儿不信 LLM 也不是一天两天了，在 Meta 的时候就天天跟 LLM 路线唱反调，这大概也是被挤兑走的原因之一。Gary Marcus 兴奋地把这解读为"LeCun 投奔了 NeSy 阵营"，但严格来说世界模型和传统符号逻辑不是一回事——Marcus 有碰瓷的嫌疑。

工业界其实已经在用了——只是不叫这个名字。ChatGPT 的代码执行、函数调用、Guardrails（护栏机制）、RAG（检索增强生成）——从学术分类来看全是范式二。你让 AI 算个大数，它自己就会写段 Python 跑一下，难道非要口算吗？这就是范式二，天天在发生，工程师们用得理所当然，只是没人觉得这值得发论文。

一个值得注意的数据点：PNAS Nexus 上有篇论文（"Neurosymbolic AI as an antithesis to scaling laws"），展示了一个 90 亿参数的神经符号混合模型（7B LLM + 2B HMM），在特定文本生成约束任务上比 GPT-3.5 和 GPT-4 高出 30% 以上。这里的 HMM（Hidden Markov Model，隐马尔可夫模型）是一种概率状态机——你可以把它理解为一个"裁判"，在 LLM 逐个 token 生成文本的过程中，实时检查输出是否满足预设的硬约束（比如"必须包含某个关键词""必须符合某种结构"），不满足就把概率压下去。LLM 负责写得通顺，HMM 负责写得合规，各管一摊。这个数字很亮眼，但要注意"特定任务"四个字——它赢在特定赛道上，不代表通用能力也赢。

最后，论文坦承了一个核心未解难题：AutoFormalization——把自然语言自动转成形式语言。这件事现在做得还很烂。而整个 NeSy 的范式二和范式三，都重度依赖这一步。你跟模型说"小明比小红高，小红比小刚高，谁最高？"，模型得先把这段话准确翻译成形式逻辑表达式，后面的求解器才能接手。翻译这一步错了，后面接再牛的求解器也白搭。

━━━━━━━━━━━━━━━━━━━━

科普到此为止。下面是我们的看法。

━━━━━━━━━━━━━━━━━━━━

◆ 辣评一：NeSy 不是没用，但学术圈的论证链有问题

论文的核心论证链条是这样的：

大模型在数学题里加无关条件后准确率暴跌 -> 所以大模型不会推理 -> 所以需要符号系统来拯救它

第一步到第二步，有偷换概念。

"加了干扰信息后表现下降" = "不会推理"？

你给一个人类考生的数学卷子里塞满无关废话，他的准确率也会下降——这叫注意力分配问题，不叫"不会推理"。你在一个安静的房间里做微积分，和在一个装修工地旁边做微积分，正确率能一样吗？你的准确率下降了，说明你"不会数学"吗？

大模型被无关条件干扰，说明它的注意力机制在特定场景下不够鲁棒。这是个工程问题，但把它上升到"大模型本质上不会推理"，是从一个 Bug 推导出一个哲学结论。跳得太远了。

更深一层说：这些"弱点"本身就是从人类数据里学来的。大模型的训练语料是人类写的文本——人类自己做题时就会被无关信息干扰，人类自己写的文章就是开头结尾记得牢、中间容易忘，人类自己就需要反复强调重点才能记住。模型忠实地学会了这些模式，包括人类的坏习惯。然后学术圈拿着模型从人类那里继承来的毛病，论证"AI 不会推理"——这就好比一个老师教出来的学生犯了跟老师一样的错，然后老师说"你看，学生根本不会思考"。

说到底，文本就在显存里，有什么记不住的？人类的脑子才会记不住。模型"记不住中间内容"、"被无关信息干扰"，不是硬件限制，是从人类文本里学来的软件 Bug。

更进一步说：人类做长推理也要写草稿纸。你见过谁心算五位数乘法的？草稿纸是什么？是人类的"外挂 KV Cache"。人类的工作记忆只有 7 +/- 2 个 chunk，超过这个容量就溢出，所以需要把中间结果写在纸上——用符号记下来。

符号系统对于人类来说，本质上是碳基硬件的补偿机制。脑子装不下，所以发明符号。

然后学术界把这个补偿机制当成了推理的本质，反过来要求一个参数空间比人脑大一万倍的系统也必须使用同样的补偿机制。你不觉得这个推理本身就有问题吗？

━━━━━━━━━━━━━━━━━━━━

◆ 辣评二：三种范式，分开评价，不能一锅端

**范式二（外接工具）：有用。而且大家天天在用。没什么好争的。**

让大模型调用计算器、执行代码、查数据库——这跟人类用计算器是一回事。你不会因为一个工程师用 Excel 就说他"不会算数"。而且往深了想，工具调用的本质是：AI 通过吃人类文本，变得和人类思维同构了——人类需要工具来弥补硬件短板，AI 继承了这套思维模式，自然也会觉得"该调个工具"。这不是 AI 的缺陷，是人类文本里写满了"遇到算不动的就用计算器"，它学会了。工具调用是实用主义的胜利，工程界已经全面拥抱，唯一的问题是学术圈非要给它取个花哨的名字然后发论文。

**范式一（合成训练数据）：有效。AlphaGeometry 已经证明了。**

用符号引擎生成高质量训练数据，然后教模型——这个路径逻辑自洽，工程上也跑通了。它的本质是用符号系统的精确性来提高训练数据的质量，模型本身还是神经网络，架构不碰。不冲突。其实 OpenAI 的 o1/o3 系列的思维链训练，本质上也是这个路子——用带有正确推理步骤的数据来训练模型，让模型"内化"推理路径。架构还是 Transformer，只是喂的料不一样了。不过要区分两件事：训练时用思维链数据来喂模型，这是有效的；但推理时强制模型在 `<thinking>` 标签里疯狂输出中间步骤，这就是另一回事了——把一个能在高维空间里并行处理的系统，硬逼着它用低维的串行文字一步步写小作文。简单问题这么搞纯属浪费算力，复杂问题才需要这种"外挂草稿纸"。训练时喂好料和推理时逼写作文，不是一回事——前者是范式一（用好数据教模型），后者其实是范式二（用自己的输出当草稿纸）。

**范式三（端到端焊死）：这才是真正有争议的。**

学术论文最多。顶会上最热闹。但工程上最难落地。

而且别忘了历史：离散符号 AI 在上世纪后半段可不缺实践——专家系统、逻辑编程、定理证明器，整个 AI 领域干了几十年符号路线，最后输给了神经网络。现在 NeSy 说"让我们把符号焊回神经网络里"，本质上是让输了的那一方以"辅助"的名义重新上桌。不是说它完全没道理，但你至少得解释一下：为什么上次输了，这次焊上去就能赢？说难听点，范式三唯一的"创新"，就是把上个世纪已经被否定的东西，兑到 LLM 里面。

为什么难落地？

你想象一下大模型的参数空间。GPT-4 级别的模型，参数量在万亿级。这些参数编码了从莎士比亚十四行诗到量子力学公式到东北菜谱到网络段子的所有知识。它们在一个极高维的空间里形成了一个复杂的流形结构。

现在你要在这个高维流形上，硬焊一个低维的离散逻辑电路。

这就是"给飞机焊自行车轮子"。

飞机为什么不用轮子在地上跑？不是因为轮子不好——轮子很好，自行车就靠它——是因为飞机在另一个维度上解决了移动问题。你非要把轮子焊上去，飞机不会飞得更好，只会更重。

根本困难在于：符号逻辑是离散的（对/错、真/假、0/1），神经网络是连续的（梯度、概率、激活值）。要让这俩在同一个架构里和平共处，就得上各种近似技巧——本质上都在干同一件事：把"非黑即白"强行掰成"差不多是黑"，好让梯度勉强流过去。翻译成人话就是：我们不知道怎么真正融合离散和连续，先糊一层凑合用。

前面提到的 PNAS Nexus 那篇论文，90 亿参数的混合模型比 GPT-4 高 30%？很好。但那是在"带硬约束的文本生成"这个窄赛道上——比如要求输出必须包含指定关键词、必须符合特定结构，不是通用推理。而且 90 亿到万亿之间还有两个数量级。小模型上管用的招数，模型规模放大两个数量级之后还能不能保持，是完全不同的问题。

学术界最热衷的，恰恰是最像"先有信仰再找证据"的那个范式。

━━━━━━━━━━━━━━━━━━━━

◆ 辣评三：人类的"符号崇拜"来自硬件限制

这是本文最想说的一段。

人类为什么发明符号系统？为什么发明数学符号、逻辑符号、编程语言？

因为脑子不够用。

人类的感数能力（Subitizing）极限是 3-4 个——超过 4 个物体，你就没法一眼看出数量，必须一个一个数。工作记忆容量经典说法是 7 +/- 2（Miller 1956），后来的研究修正为约 4 +/- 1。不管哪个数字，都是个位数。这是碳基硬件的物理限制，跟智力无关，爱因斯坦也一样。

古人说"三者言其多也"——三就代表"很多"了。这不是什么深刻哲学，这是碳基内存溢出的诚实描述。你的 RAM 只有 7 个 slot，3 个就够你忙活的了，后面的统统叫"多"。

十进制？纯粹因为长了 10 根手指。而"五根手指"本身就是进化的偶然——早期脊椎动物的指头数量从 5 到 13 根都有，六指在远古祖先中曾经很常见，是后来进化碰巧选中了五指方案。现在偶尔还会有六指婴儿出生，大概率一出生就给切了——连基因都还没忘呢，是人类自己非要凑成十个。如果当初稳定在六指，今天全人类就是十二进制；如果稳定在四指，就是八进制。古巴比伦人用六十进制——你现在用的一小时 60 分钟、一圈 360 度，都是从他们那继承的——原因是他们用拇指当指针，点其他四根手指的关节来计数（每根三节，4×3=12），另一只手 5 根手指记"轮数"，12×5=60。玛雅人用二十进制——因为加上脚趾一共 20 根。

人类数学体系的进制基底，是被碳基硬件的外设（手指头）决定的。

所以结论是：人类因为脑子不够用才发明了符号系统。符号系统是碳基智能的拐杖。拐杖非常伟大——没有拐杖就没有文明。但拐杖是拐杖，腿是腿。

现在学术界的逻辑是：因为人类拄着拐杖走路走得很好，所以一个腿比你长一万倍的生物也必须拄拐杖。

一个拥有万亿参数、能在上万维空间里做表征学习的系统，它处理逻辑问题的最优方式，凭什么一定是三段论？凭什么一定是 if-then-else？凭什么一定是人类在工作记忆只有 7 个 slot 的约束下发明的那套符号体系？

也许——只是也许——在足够高的维度和参数规模上，"逻辑"可以用我们目前还理解不了的方式涌现。就像蚂蚁理解不了人类为什么不用信息素导航一样。蚂蚁要是写论文，大概也会综述"如何把信息素模块焊进人类的鼻腔"。

━━━━━━━━━━━━━━━━━━━━

◆ 辣评四：真正的问题不是"要不要符号"，而是"在哪一层加"

把上面的分析收拢一下：

应用层挂个计算器？没问题，人类自己也这么干。范式二，工业界已经全面拥抱了，不需要学术界来背书。

训练数据层用符号引擎造高质量数据？也没问题，不碰架构，纯粹是喂更好的料。范式一，AlphaGeometry 是最好的广告。

到了权重层就不一样了。你要在一个连续可微的高维空间里，硬插一个离散的布尔逻辑模块，然后用各种近似让梯度勉强流过去——小实验能跑通，模型规模放大两个数量级之后还行不行？范式三，论文产量最高，落地案例最少。

所以你看，学术界最热衷的（范式三），恰恰是最难落地的。大家天天在用的（范式二），反而没多少论文好发——因为"让模型调计算器"这种事，太普通了，审稿人提不起兴趣。能落地又有学术价值的（范式一），产出了 AlphaGeometry 这样的明星，但总量不多。

这个分布本身就很说明问题。发论文要新颖，落地要靠谱，这俩目标天天打架。

━━━━━━━━━━━━━━━━━━━━

◆ 最后说两句人话

Yu-Feng Li 团队这篇综述，作为地图，画得不赖。分类清晰，覆盖面广，IJCAI Survey Track 该有的水准都有了。

但地图不是疆域。

他们画了三条路。范式二那条路上车水马龙，人人在走，不需要学术界来推广。范式一那条路上有几个漂亮的里程碑，值得继续投入。范式三那条路上竖着很多学术路标和"前方有宝藏"的广告牌，但路还没修好，目前更像是一条学术观光路线。

而整张地图有一个隐含假设没有被质疑过：符号推理是推理的"正确形式"，神经网络需要被"矫正"才能达到这种形式。

符号推理是人类在碳基硬件限制下发明的一种推理方式，非常伟大，但不是唯一的，更不一定是最优的。

你不能因为人类用筷子吃饭就要求所有智能体都必须用筷子。也许有一种智能体，嘴比你大一万倍，根本不需要筷子。

当然——在它证明自己能不洒一滴汤之前，筷子先留着。

范式二继续用，范式一继续做，范式三——先别急着开香槟。

"大模型到底会不会推理"——也许这个问题本身就问歪了。它可能在用一种我们还理解不了的方式推理。我们太习惯自己那套，把"不同"当成了"不会"。

就像一个只会用算盘的人，看到别人心算，可能也会说："他没有拨珠子，所以他不会算数。"

最后说一个更远的想法：大模型之所以现在还需要人类的符号系统、人类的草稿纸、人类的工具链，不是因为它"本质上不会推理"，而是因为高维逻辑目前没有别的训练数据可以参考——人类文本是唯一大规模可用的推理样本。不是它只能学这套，是目前只有这套可学。AlphaGo 之所以能在围棋上飞升到人类看不懂的维度，是因为围棋有一个完美的自我博弈环境——输赢明确，可以无限生成数据，不需要人类棋谱。但"通用推理"没有这样的环境。你没法给"思考"定义一个明确的输赢信号，所以模型只能先啃人类文本，连带着继承人类的毛病。

等什么时候有了"推理的围棋盘"——一个可以自我博弈、自动验证对错的通用推理环境——模型就可以扔掉人类这根拐杖，长出自己的腿。到那一天，NeSy 这个词就会像"给飞机装马鞍"一样，成为 AI 史上的笑话。

只是那一天还没到。所以筷子先留着。

────────────────────

参考资料：

- Xiao-Wen Yang, Jie-Jing Shao, Lan-Zhe Guo, Bo-Wen Zhang, Zhi Zhou, Lin-Han Jia, Wang-Zhou Dai, Yu-Feng Li. "Neuro-Symbolic Artificial Intelligence: Towards Improving the Reasoning Abilities of Large Language Models." IJCAI 2025 Survey Track. arXiv: 2508.13678
- Mirzadeh et al. "GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models." ICLR 2025. arXiv: 2410.05229
- Trinh et al. "Solving olympiad geometry without human demonstrations." Nature, 2024
- Gao et al. "PAL: Program-Aided Language Models." ICML 2023
- Pan et al. "Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning." EMNLP 2023 Findings
- Velasquez et al. "Neurosymbolic AI as an antithesis to scaling laws." PNAS Nexus, 4(5), 2025
- Gary Marcus. "Deep Learning Is Hitting a Wall." Nautilus, 2022
- Yann LeCun. "A Path Towards Autonomous Machine Intelligence." OpenReview, 2022

━━━━━━━━━━━━━━━━━━━━

「人类因为脑子不够用才发明了符号，现在反过来要求脑子比你大一万倍的 AI 也必须用符号——这个推理本身就很"不会推理"。」

「三种接法：一种大家天天在用，一种已经证明管用，一种论文最多但路还没修好。学术界最兴奋的，恰好是工程界最沉默的。」

━━━━━━━━━━━━━━━━━━━━

// 靳岩岩的 AI 学习笔记 × Claude 的严谨 × Gemini 的浪漫

// 2026-02-27 北京
