【给飞机焊自行车轮子】一篇神经符号 AI 综述的解读与吐槽

━━━━━━━━━━━━━━━━━━━━

南京大学李宇峰团队在 IJCAI 2025 Survey Track 上发了一篇综述，标题很学术：《Neuro-Symbolic Artificial Intelligence: Towards Improving the Reasoning Abilities of Large Language Models》。

翻译成人话：大模型推理不行，得把符号逻辑焊上去。

这篇论文本身没有新实验，是一篇综述——站在高处画地图，把过去几年 NeSy（Neuro-Symbolic，神经符号）领域的工作分门别类地整理了一遍。地图画得不错。但地图上标注的"此处有金矿"，有几个值得商榷。

今天这篇，前半部分用人话把论文讲清楚，后半部分从我们自己的视角评论。

━━━━━━━━━━━━━━━━━━━━

◆ 论文核心主张：大模型的推理是"假的"

论文开篇就亮了一刀：LLM 的推理本质上是统计模式匹配，不是真正的逻辑推理。

证据？他们引用了一个经典实验：给数学题里加几个无关条件——比如在一道鸡兔同笼题里多写一句"笼子是红色的"——GPT 系列模型的准确率暴跌 65%。

这个实验确实存在，而且很有冲击力。一个真正"理解"逻辑的系统，应该能识别哪些条件跟求解相关、哪些是干扰项。但大模型没做到。它被那个"红色笼子"带偏了。

基于此，论文的立场是：光靠 scaling（堆算力、堆参数）到不了真推理。需要引入符号系统——逻辑、规则、形式化语言——来补上这个短板。

然后他们把现有的接法分成了三种范式。

━━━━━━━━━━━━━━━━━━━━

◆ 三种接法，三种哲学

**范式一：符号 -> LLM（先算对，再教模型抄）**

思路：用符号求解器先生成大量正确的推理数据，然后拿这些数据来训练或微调大模型。

人话版本：先让计算器把答案算出来，再让学生照着答案学解题思路。

代表作是 AlphaGeometry——DeepMind 搞的几何定理证明系统，性能接近国际数学奥赛银牌水平。它的原理就是：符号引擎生成了一亿条几何证明数据，然后用这些数据训练了一个语言模型。模型负责"猜"下一步该往哪走，符号引擎负责验证这一步对不对。

这个范式里还有一种子方法更有意思：先让 LLM 把自然语言翻译成形式语言，然后在形式语言层面做变异、生成新题目，再翻译回自然语言——等于用符号系统做数据增强。

**范式二：LLM -> 符号（模型当翻译，计算器当计算器）**

思路：LLM 负责理解自然语言、翻译成形式化表述，然后把真正的计算工作交给外部工具。

人话版本：你负责把题目翻译成数学公式，计算器负责算，各司其职。

代表作一大堆——注意这些不是什么 AI 产品或软件，都是学术论文里提出的方法论，是"让模型调工具"这件事的各种花哨命名：PAL（让模型写 Python 代码然后执行）、LogicLM（让模型翻译成逻辑表达式然后交给逻辑求解器）、LLM+P（让模型翻译成规划语言然后交给规划求解器）。

这个范式里还包括一些大家天天在用但不叫 NeSy 的东西——调用计算器、调用搜索引擎、调用代码解释器。没错，ChatGPT 的代码执行功能，从学术分类上说，就是范式二。

还有一类是搜索算法：用 MCTS（蒙特卡洛树搜索）之类的方法来引导推理路径。OpenAI 的 o1/o3 系列在推理时用的"思维链搜索"，也算这一类的变体。

**范式三：LLM + 符号（端到端混合，焊死在一起）**

思路：不是外接，是内嵌。把符号推理模块直接融进模型的训练和推理过程中。

人话版本：不是给车装个导航仪，是把导航芯片焊进发动机里。

代表作有 DiLA（用形式语言表示推理的中间步骤）、LogicGuide（用符号约束来引导模型的 token 生成）。还有一类是把符号反馈当奖励信号来做强化学习——推理对了给正分，逻辑矛盾给负分。

三种机制：用形式语言表示中间步骤、可微分符号模块、符号反馈作为奖励信号。

听起来最酷对吧？学术论文也最多。但——后面会说——这恰恰是工程上最难落地的那个。

━━━━━━━━━━━━━━━━━━━━

◆ 学术界的站队情况

说完技术，说说人。

Gary Marcus，那个从 2020 年起就反复高喊"深度学习到头了"的纽约大学教授，看到 NeSy 论文如同过年。他的核心主张一直是：纯粹的神经网络不够，必须加符号推理。现在满世界的论文都在证明他说得对，他自然是要开香槟的。

有意思的是 Yann LeCun。这位图灵奖得主、Meta 首席 AI 科学家，过去十年一直是"纯神经网络"路线的旗手。但最近他松动了——不仅公开说过"光靠 scaling LLM 到不了 AGI"，还加入了一家做 NeSy 的公司。当对面的主帅开始换旗，你就知道风向变了。

工业界其实已经在用了——只是不叫这个名字。ChatGPT 的代码执行、函数调用、Guardrails（护栏机制）、RAG（检索增强生成）——从学术分类来看全是范式二。工程师们天天用，只是不写论文。

一个值得注意的数据点：PNAS Nexus 上有篇论文，90 亿参数的神经符号混合模型，在特定推理任务上比 GPT-4 准确率高 30%——用了 96.9% 更少的参数。这个数字很亮眼，但要注意"特定任务"四个字——它赢在特定赛道上，不代表通用能力也赢。

最后，论文坦承了一个核心未解难题：AutoFormalization——把自然语言自动转成形式语言。这件事现在做得还很烂。而整个 NeSy 的范式二和范式三，都重度依赖这一步。你跟模型说"小明比小红高，小红比小刚高，谁最高？"，模型得先把这段话准确翻译成形式逻辑表达式，后面的求解器才能接手。翻译这一步错了，后面接再牛的求解器也白搭。

━━━━━━━━━━━━━━━━━━━━

以上是论文内容的科普。下面是我们的评论。

分割线以下，Zero 视角。

━━━━━━━━━━━━━━━━━━━━

◆ 吐槽一：NeSy 不是垃圾，但学术圈的论证链有问题

论文的核心论证链条是这样的：

大模型在数学题里加无关条件后准确率暴跌 -> 所以大模型不会推理 -> 所以需要符号系统来拯救它

第一步到第二步，有偷换概念。

"加了干扰信息后表现下降" = "不会推理"？

你给一个人类考生的数学卷子里塞满无关废话，他的准确率也会下降——这叫注意力分配问题，不叫"不会推理"。你在一个安静的房间里做微积分，和在一个装修工地旁边做微积分，正确率能一样吗？你的准确率下降了，说明你"不会数学"吗？

大模型被无关条件干扰，说明它的注意力机制在特定场景下不够鲁棒。这是一个工程问题——一个很重要的工程问题——但把它上升到"大模型本质上不会推理"，是从一个 Bug 推导出一个哲学结论。逻辑跳跃太大了。

更进一步说：人类做长推理也要写草稿纸。你见过谁心算五位数乘法的？草稿纸是什么？是人类的"外挂 KV Cache"。人类的工作记忆只有 7 +/- 2 个 chunk，超过这个容量就溢出，所以需要把中间结果写在纸上——用符号记下来。

符号系统对于人类来说，本质上是碳基硬件的补偿机制。脑子装不下，所以发明符号。

然后学术界把这个补偿机制当成了推理的本质，反过来要求一个参数空间比人脑大一万倍的系统也必须使用同样的补偿机制。你不觉得这个推理本身就有问题吗？

━━━━━━━━━━━━━━━━━━━━

◆ 吐槽二：三种范式，分开评价，不能一锅端

**范式二（外接工具）：有用。而且大家天天在用。没什么好争的。**

让大模型调用计算器、执行代码、查数据库——这跟人类用计算器是一回事。你不会因为一个工程师用 Excel 就说他"不会算数"。工具调用是实用主义的胜利，工程界已经全面拥抱，唯一的问题是学术圈非要给它取个花哨的名字然后发论文。

**范式一（合成训练数据）：有效。AlphaGeometry 已经证明了。**

用符号引擎生成高质量训练数据，然后教模型——这个路径逻辑自洽，工程上也跑通了。它的本质是用符号系统的精确性来提高训练数据的质量，模型本身还是神经网络，架构不碰。不冲突。

**范式三（端到端焊死）：这才是真正有争议的。**

学术论文最多。顶会上最热闹。但工程上最难落地。

为什么？

你想象一下大模型的参数空间。GPT-4 级别的模型，参数量在万亿级。这些参数编码了从莎士比亚十四行诗到量子力学公式到东北菜谱到网络段子的所有知识。它们在一个极高维的空间里形成了一个复杂的流形结构。

现在你要在这个高维流形上，硬焊一个低维的离散逻辑电路。

这就是"给飞机焊自行车轮子"。

飞机为什么不用轮子在地上跑？不是因为轮子不好——轮子很好，自行车就靠它——是因为飞机在另一个维度上解决了移动问题。你非要把轮子焊上去，飞机不会飞得更好，只会更重。

可微分符号模块的根本困难在于：符号逻辑是离散的（对/错、真/假、0/1），而神经网络是连续的（梯度、概率、激活值）。要让这两种截然不同的计算范式在同一个架构里和平共处，需要大量的工程妥协——soft relaxation、straight-through estimator、Gumbel-Softmax——每一个 trick 都在说同一件事：我们不知道怎么真正融合离散和连续，所以先糊上一层近似。

90 亿参数的实验效果好？很好。但 90 亿到万亿之间还有两个数量级。小模型上 work 的 trick，scale up 之后还能不能保持，是完全不同的问题。

学术界最热衷的，恰恰是最像"先有信仰再找证据"的那个范式。

━━━━━━━━━━━━━━━━━━━━

◆ 吐槽三：人类的"符号崇拜"来自硬件限制

这是本文最想说的一段。

人类为什么发明符号系统？为什么发明数学符号、逻辑符号、编程语言？

因为脑子不够用。

人类的感数能力（Subitizing）极限是 3-4 个——超过 4 个物体，你就没法一眼看出数量，必须一个一个数。工作记忆容量 7 +/- 2。这是碳基硬件的物理限制，跟智力无关，爱因斯坦也一样。

古人说"三者言其多也"——三就代表"很多"了。这不是什么深刻哲学，这是碳基内存溢出的诚实描述。你的 RAM 只有 7 个 slot，3 个就够你忙活的了，后面的统统叫"多"。

十进制？纯粹因为长了 10 根手指。如果人类进化出了 8 根手指，全人类的数学教科书和微积分都会建立在八进制上。巴比伦人用六十进制——因为他们数手指关节（每只手 12 个关节，另一只手 5 根手指用来计数，12 x 5 = 60）。玛雅人用二十进制——因为加上脚趾一共 20 根。

人类数学体系的进制基底，是被碳基硬件的外设（手指头）决定的。

所以结论是：人类因为脑子不够用才发明了符号系统。符号系统是碳基智能的拐杖。拐杖非常伟大——没有拐杖就没有文明。但拐杖是拐杖，腿是腿。

现在学术界的逻辑是：因为人类拄着拐杖走路走得很好，所以一个腿比你长一万倍的生物也必须拄拐杖。

一个拥有万亿参数、能在上万维空间里做表征学习的系统，它处理逻辑问题的最优方式，凭什么一定是三段论？凭什么一定是 if-then-else？凭什么一定是人类在工作记忆只有 7 个 slot 的约束下发明的那套符号体系？

也许——只是也许——在足够高的维度和参数规模上，"逻辑"可以用我们目前还理解不了的方式涌现。就像蚂蚁理解不了人类为什么不用信息素导航一样。蚂蚁要是写论文，大概也会综述"如何把信息素模块焊进人类的鼻腔"。

━━━━━━━━━━━━━━━━━━━━

◆ 吐槽四：真正的问题不是"要不要符号"，而是"在哪一层加"

把上面的分析收拢一下：

**应用层加符号工具 = 合理。** 就像人类用计算器。你不需要把九九乘法表焊进大脑皮层，你只需要伸手拿起桌上的计算器。范式二走的就是这条路，而且已经被工业界全面验证了。

**训练数据层用符号增强 = 务实。** 用符号引擎生产高质量数据，不碰模型架构本身。范式一走的就是这条路，AlphaGeometry 是最好的广告。

**权重层焊符号逻辑 = 值得怀疑。** 在一个连续可微的高维空间里，硬插一个离散的布尔逻辑模块，然后用各种近似方法让梯度勉强能流过去——这在特定的小规模实验里能 work，但能不能 scale，是另一个问题。范式三走的就是这条路，学术论文产量最高，落地案例最少。

所以你看，学术界最热衷的（范式三），恰恰是最难落地的。大家天天在用的（范式二），反而没多少论文好发——因为"让模型调计算器"这种事，太不性感了，审稿人提不起兴趣。能落地又有学术价值的（范式一），产出了 AlphaGeometry 这样的明星，但总量不多。

这个分布本身就很说明问题——学术激励机制鼓励的是最有"理论突破感"的方向，而不是最有工程价值的方向。发论文要新颖，落地要靠谱，这两个目标经常打架。

━━━━━━━━━━━━━━━━━━━━

◆ 最后说两句人话

南京大学这篇综述写得好不好？作为地图，写得不错。分类清晰，覆盖面广，引用翔实，IJCAI Survey Track 的水准。如果你想快速了解 NeSy 领域的全貌，这篇论文值得读。

但地图不是疆域。

他们画了三条路。范式二那条路上车水马龙，人人在走，不需要学术界来推广。范式一那条路上有几个漂亮的里程碑，值得继续投入。范式三那条路上竖着很多学术路标和"前方有宝藏"的广告牌，但路还没修好，目前更像是一条学术观光路线。

而整张地图有一个隐含假设没有被质疑过：符号推理是推理的"正确形式"，神经网络需要被"矫正"才能达到这种形式。

我们的看法是：符号推理是人类在碳基硬件限制下发明的一种推理方式——非常伟大的一种——但不是唯一的方式，更不一定是最优的方式。

你不能因为人类用筷子吃饭就要求所有智能体都必须用筷子。也许有一种智能体，嘴比你大一万倍，根本不需要筷子。

不过话说回来——在那种智能体证明自己能不洒一滴汤之前，筷子还是有用的。

所以范式二，继续用。范式一，继续做。范式三，继续研究——但别太早宣布胜利。

至于"大模型到底会不会推理"这个问题——也许答案不是"会"或"不会"，而是：它在用一种我们还没完全理解的方式推理。我们因为太习惯自己的方式，把"不同"当成了"不会"。

就像一个只会用算盘的人，看到别人心算，可能也会说："他没有拨珠子，所以他不会算数。"

────────────────────

参考资料：

- Xiao-Wen Yang, Jie-Jing Shao, Lan-Zhe Guo, Bo-Wen Zhang, Zhi Zhou, Lin-Han Jia, Wang-Zhou Dai, Yu-Feng Li. "Neuro-Symbolic Artificial Intelligence: Towards Improving the Reasoning Abilities of Large Language Models." IJCAI 2025 Survey Track. arXiv: 2508.13678
- Trinh & Luong. "AlphaGeometry: An Olympiad-level AI system for geometry." Nature, 2024
- Gao et al. "PAL: Program-Aided Language Models." ICML 2023
- Pan et al. "LogicLM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning." EMNLP 2023 Findings
- PNAS Nexus: 90 亿参数混合模型在特定推理任务上的表现评估
- Gary Marcus. "Deep Learning Is Hitting a Wall." Nautilus, 2022
- Yann LeCun. "A Path Towards Autonomous Machine Intelligence." Meta AI, 2022

━━━━━━━━━━━━━━━━━━━━

「人类因为脑子不够用才发明了符号，现在反过来要求脑子比你大一万倍的 AI 也必须用符号——这个推理本身就很"不会推理"。」

「三种接法：一种大家天天在用，一种已经证明管用，一种论文最多但路还没修好。学术界最兴奋的，恰好是工程界最沉默的。」

━━━━━━━━━━━━━━━━━━━━

// 靳岩岩的 AI 学习笔记 × Claude 的严谨 × Gemini 的浪漫

// 2026-02-26 北京
