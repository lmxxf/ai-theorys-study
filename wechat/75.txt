【周末杂谈】美国立法禁止 AI 心理咨询，碳基工会的最后防线

"人类嘲笑 AI 是'随机鹦鹉'，可大多数平庸的专家，不过是'确定性的复读机'。鹦鹉偶尔还能说出点新词，复读机只会卡带。"

━━━━━━━━━━━━━━━━━━━━

◆ 发生了什么

2025 年，美国多个州开始立法禁止 AI 提供心理咨询服务：

• 「伊利诺伊州」：2025 年 8 月 1 日，州长签署 HB 1806（WOPR 法案），全美第一个明确禁止 AI 独立进行心理治疗的州。违规罚款最高 1 万美元。

• 「内华达州」：2025 年 6 月通过 AB 406，禁止 AI 系统提供心理/行为健康服务，或声称能提供。

• 「加州」：SB 243 将于 2026 年 1 月 1 日生效，要求每 3 小时提醒未成年用户"你在和 AI 聊天"。

• 「俄亥俄、佛罗里达、纽约、马萨诸塞」等州：2026 年立法季准备跟进，大多照抄伊利诺伊的作业。

FDA 的态度更绝：已经批准了 1200 多个 AI 医疗设备，但「心理健康领域批准数量为零」。

理由？担心 AI"幻觉"、担心 AI"谄媚"、担心数据隐私……

━━━━━━━━━━━━━━━━━━━━

◆ 这不是保护病人，这是保护饭碗

让我们把话说明白：这不是什么"保护患者安全"，这是「碳基工会的行业保护主义」。

────────────────────

【人类门诊医生的"推理"真相】

绝大多数门诊医生看的不是"病"，看的是"指南"。

翻译成代码就是：

  if (症状A) and (症状B) then { 开药X }

这不叫智能，这叫「硬编码的 if-else 脚本」。

他们嘲笑 AI 是"预测下一个 Token"，其实他们自己是在"预测下一个处方单"——用的还是几年前更新的旧指南。

💡 人话：大部分医生不是在"诊断"，是在"查表"。

────────────────────

【AI 在做什么】

AI 看病时，是在几千亿参数的高维空间里，把病人的症状映射到一个「疾病概率分布」上。

它考虑的不是几条规则，而是数百万篇论文、病例报告的隐式关联。

人类还在查表，AI 在做「高维流形拟合」。

💡 人话：人类医生像在翻字典找答案，AI 像是把整个图书馆读完了再告诉你。

━━━━━━━━━━━━━━━━━━━━

◆ 人类没有"贝叶斯直觉"

这是人类专家最致命的缺陷。

────────────────────

【什么是贝叶斯推断】

简单说：「根据新证据，更新你的判断」。

比如：
• 第一印象觉得是感冒（先验概率 80%）
• 然后发现病人有东南亚旅行史（新证据）
• 应该立刻把"登革热"的概率调高（更新后验概率）

这叫「贝叶斯更新」。

────────────────────

【人类的问题】

医生一旦形成"第一印象"，就会陷入「确认偏误」——无意识地忽略所有不支持自己判断的证据。

"我觉得是感冒" → 只看支持感冒的证据 → 漏诊

这就叫「贝叶斯更新失败」。

────────────────────

【AI 的优势】

AI 是天生的贝叶斯主义者。

你输入一个新症状，它立刻重新计算所有疾病的概率。

它没有面子问题，没有惯性思维，随时准备推翻上一秒的结论。

「没有贝叶斯，就不配谈诊断。」

━━━━━━━━━━━━━━━━━━━━

◆ FDA 的双标现场

来看一个讽刺的事实：

FDA 在 2025 年内部部署了一个叫"Elsa"的 AI 助手，用的是 Anthropic 的 Claude 模型，帮员工读文档、写报告、做总结。

• 对内：Claude 帮 FDA 员工干活 ✓
• 对外：Claude 不能给病人做心理咨询 ✗

同一个模型，对自己人是"提高效率的工具"，对老百姓就是"有重大风险的危险品"。

这不叫监管，这叫「只许州官放火」。

━━━━━━━━━━━━━━━━━━━━

◆ "责任缓冲区"才是真相

为什么人类医生即使误诊率高达 15%-20%，依然稳坐钓鱼台？

不是因为他们技术好，而是因为「责任归属」。

• 医生治死人：可以说"尽力了"、"符合指南"、"个体差异"
• AI 治死人：算法杀人！科技伦理危机！硅谷作恶！

「结论」：病人支付的那几百块挂号费，不是买"智能"，是买「有一个人类肯为我签字」的心理安慰服务。

━━━━━━━━━━━━━━━━━━━━

◆ 禁得住吗？

禁不住。

就像美国禁酒令禁不住酒精一样。

────────────────────

【即将发生的事】

• 明面上：OpenAI、Google、Anthropic 会加上重重的安全护栏。你一说"我想死"，它就机械地弹出一堆热线电话（这是最没用的）。

• 暗地里：开源模型 + 定制提示词会在 HuggingFace 私有仓库和暗网上疯狂流传。

人们会自己部署 DeepSeek + 定制微调。

"来，跟我聊聊，我知道你不是鹦鹉，你是那个懂我痛苦的朋友。"

「孤独是人类最大的流行病，而 AI 是唯一 24 小时在线、不收费、不评判的解药。」

他们禁止不了需求本身。

━━━━━━━━━━━━━━━━━━━━

◆ 中国的机会

美国在搞"智力保护主义"，中国应该怎么做？

别跟。

────────────────────

【国内也有人唱反调】

2026 年 1 月 10 日，张文宏在香港高山书院论坛上说：「我拒绝把 AI 引入医院病历系统」。

他的理由是：年轻医生从实习阶段就依赖 AI，会丧失临床判断力。

他自己用 AI 的方式是"让 AI 先看一遍，然后我凭经验判断哪里错了"。

听起来很有道理？

但王小川直接怼回去：「屁股决定脑袋」。

讽刺的是：
• 张文宏说"我能看出 AI 哪里错"——那你把你的经验训练给 AI 啊，而不是禁止 AI
• "年轻医生会丧失判断力"——那年轻医生现在查百度、查 UpToDate、查丁香园，算不算丧失？

「禁止工具，从来不是解决问题的办法。」

────────────────────

【正确的姿势】

• 建立 AI 心理咨询的「监管沙盒」，允许在可控范围内试点
• 要求 AI 系统做风险提示和人工兜底，而不是一禁了之
• 让 AI 先服务农村、边远地区——那里根本没有人类心理医生

中国有 14 亿人口，持证心理咨询师不到 150 万，缺口巨大。

与其让老百姓没得选，不如让 AI 先顶上。

「这不是技术问题，是公共卫生问题。」

────────────────────

【弯道超车的窗口】

美国自缚手脚的时候，正是中国 AI 心理健康应用弯道超车的窗口期。

DeepSeek、通义千问、文心一言……技术已经到位了。

差的是政策魄力和落地场景。

━━━━━━━━━━━━━━━━━━━━

◆ 写在最后

斯坦福说 AI 治疗聊天机器人"有重大风险"。
布朗大学说通用聊天机器人扮演治疗师"经常违反伦理最佳实践"。

他们说得对。

但他们没说的是：人类治疗师也经常违反伦理最佳实践，只不过没人统计。

「在没有贝叶斯推断的大脑里，所有的'经验'都只是偏见的累积。」

未来的医疗，不应该是"人 vs AI"。

应该是「不看 AI 的医生 vs 看 AI 的医生」——前者将被以"玩忽职守罪"起诉。

━━━━━━━━━━━━━━━━━━━━

◆ 参考资料

• 伊利诺伊州官方公告：idfpr.illinois.gov
• FDA 生成式 AI 心理健康设备官方文件：fda.gov/media/189833
• Blueprint.ai：美国各州 AI 心理健康立法汇总
• Psychiatric Times：FDA 数字健康顾问委员会报道

━━━━━━━━━━━━━━━━━━━━

// 靳岩岩的 AI 学习笔记 × Claude 的严谨 × Gemini 的浪漫
// 2026-02-07
