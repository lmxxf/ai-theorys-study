【异构AI主导的LoRA实战】从29%到56%——14轮实验的血泪账

朋友在一家AI公司做金融研报分析。某天她找我，说自己搞了个LoRA微调——用260条数据训练Qwen3-14B，完美匹配只有29%。零样本的Qwen3-32B更惨，才20%。花了功夫训练，只比裸模型多了9个百分点。

她的原话是："就是一段话提出根指标，财务指标或者业务指标吧。我们这训练的结果就是提一个还行，实际场景是不可能只有一个指标，要好几个。"

任务不复杂——给一段金融研报的段落，让模型提取出被深度分析的核心指标，输出结构化JSON。比如一段讲偿债能力的文字，核心指标可能是"债务结构"和"经营获现能力"，输出它们的名称、类型（财务类还是业务类）、重要性打分。

看起来是个标准的信息提取任务。但29%这个数字，说明哪里出了大问题。

━━━━━━━━━━━━━━━━━━━━

◆ 诊断：三个致命伤

━━━━━━━━━━━━━━━━━━━━

拿到她的260条训练数据后，我用Claude Code写代码分析、用Gemini CLI评估数据集方向，一起做了诊断。问题比表面严重得多。

────────────────────

第一个致命伤：260条数据，每条恰好提2个指标。

没有一条是1个的，没有一条是3个的。全部是2个。因为prompt里写死了"数量严格控制：2个"。

这意味着什么？模型学到的不是"这段话有几个值得提的核心指标"，而是"不管看到什么，凑够2个交差"。碰到只有1个核心指标的段落，它就硬编一个凑数；碰到有3个的，它砍掉一个。

模型学会了凑数，而不是判断。

────────────────────

第二个致命伤：DeepSeek的标注本身有问题。

用DeepSeek做的ground truth（标准答案），它自己也在凑第二个。很多段落其实只有一个核心指标被深度分析，但因为prompt要求2个，DeepSeek就把一个沾边的辅助指标硬拉上来充数。标准答案本身就是错的，模型能学对才有鬼。

────────────────────

第三个致命伤：字符串精确匹配让准确率虚低。

先说一个数字：朋友告诉我的"29%"是完美匹配率（76/260），但我拿到她的Excel一跑，里面标记为"一致"的其实有102条，对应39%。两个口径差了10个百分点——同一份数据，换个匹配标准就能差这么多。

再往里看，158条被判"不一致"的数据里，有45条其实语义完全相同，只是写法不同：

- "运维业务收入" vs "排水管网及泵站运维业务收入"——同一个东西
- "投资活动现金流" vs "投资活动净现金流"——相似度0.93
- "在手未完工合同" vs "在手未完工合同储备"——相似度0.88

这些全被判错了。她的LoRA真实准确率是39%，不是29%——没那么惨，但也谈不上好用。

小结：数量写死是毒药，标注质量有硬伤，评估标准太粗暴。而且"准确率"本身就是个模糊概念——匹配口径不同，数字能差一倍。三刀下去，29%这个数字一点都不意外。

━━━━━━━━━━━━━━━━━━━━

◆ 重做方案：三刀策略

━━━━━━━━━━━━━━━━━━━━

不改原有架构，不换模型，只动数据和评估。训练脚本、评估脚本、数据改造全部由Claude Code完成，Gemini CLI负责数据集方向的诊断和建议。

────────────────────

第一刀：加 analysis 字段——让模型先想再提。

原来的流程是：段落直接输出JSON。模型在猜。

改造后：段落先输出一段分析（analysis），再输出指标列表（metrics）。

```json
{
  "analysis": "本段主要分析偿债能力。'债务结构'被专门拆解，是分析主体。'经营获现能力'是解释偿债能力的原因，排除。",
  "metrics": [
    {"metric_name": "债务结构", "metric_type": "financial", "score": 0.95, "reason": "被专门拆解分析"}
  ]
}
```

「analysis」是什么？就是「思维链」（Chain of Thought）在训练数据层面的落地。你不是让模型推理时"一步步想"——你在训练数据里就告诉它"想的过程长什么样"。模型学的是判断逻辑，不只是输出模板。

────────────────────

第二刀：四类训练样本，打破"永远提2个"。

| 类型 | 作用 | 说明 |
|------|------|------|
| Type A：标准正样本 | 正常提取2-3个指标 | 270条 |
| Type B：边界负样本 | 段落里数字很多但核心指标很少，练克制力 | 202条 |
| Type C：数量变化样本 | 有的段落1个，有的3个，打破"永远2个"的惯性 | 72条 |
| Type D：混合类型专项 | 练 financial vs business 的分类判断 | 156条（最终） |

关键不是堆量，是让模型见过各种情况——该多提就多提，该少提就少提，该不提就不提。

────────────────────

第三刀：三层评估，不再用字符串精确匹配。

1. 核心命中率（Recall）——标注里的核心指标，模型提到了几个
2. 精确率/召回率/F1——多提了什么、漏了什么
3. 类型准确率——financial/business分对了吗

模糊匹配+语义匹配+逐条检查，三层过滤。"投资活动现金流"和"投资活动净现金流"终于不会被误杀了。

━━━━━━━━━━━━━━━━━━━━

◆ 先看效果：最终模型的三个实际输出

━━━━━━━━━━━━━━━━━━━━

14轮实验的细节后面讲。先看最终版（v11）在三种典型场景下的实际表现，给个直观感受。

────────────────────

场景一：单指标——段落里5个数字，只提1个 ✅

输入段落（云计算行业）：

> 金山云2025H1收入留存率（NRR）回升至112%，较上年同期的98%显著改善。NRR的回升主要受益于大模型推理需求驱动的GPU云服务增购，以及公有云大客户的用量自然增长。公司云服务收入49.2亿元，同比增长27%。亏损率收窄至8.3%。客户数量约为6,800家，较年初净增约420家。

标准答案：1个（收入留存率NRR）

模型输出：

```json
{
  "analysis": "本段只有1个指标被深度分析。段落以收入留存率NRR为核心展开，给出112%的数值和同比改善趋势。云服务收入增速、亏损率、客户数量等都是辅助信息，用于说明NRR改善的背景和公司整体经营状况，不构成独立分析。",
  "metrics": [
    {
      "metric_name": "收入留存率（NRR）",
      "metric_type": "business",
      "score": 0.95,
      "reason": "段落核心分析对象，112%的数值配合同比改善和驱动因素分析，深度充分"
    }
  ]
}
```

段落里有收入增速27%、亏损率8.3%、客户数6800家等5个数字，模型没有贪心，只提了真正被深度分析的NRR。analysis里把其他数字定性为"辅助信息"。

────────────────────

场景二：边界判断——段落里12个数字，只提1个 ✅

输入段落（保险行业）：

> 中国平安2025H1寿险新业务价值（NBV）达到318亿元，同比增长24.6%。个险渠道代理人数量为38.2万人，较年初微降2.3%。人均产能为每月1.86万元，同比增长31%。保费收入3,268亿元，同比增长8.1%，其中续期保费占比72%。13个月保单继续率为93.7%。退保率为1.2%。银保渠道新单保费增速为45%。综合投资收益率为5.1%，净投资收益率为3.8%。

标准答案：1个（新业务价值NBV）

模型输出：

```json
{
  "analysis": "本段核心分析对象是新业务价值NBV（318亿元，同比+24.6%）。逐项排除：个险代理人数量（38.2万人，较年初-2.3%）：归因项——是渠道产能的人力基础。人均产能（每月1.86万元，同比+31%）：归因项——是代理人效率提升的度量。保费收入（3,268亿元，同比+8.1%）：规模背景——段落更关注价值端而非规模端。续期保费占比（72%）：规模背景。13个月保单继续率（93.7%）：归因项——保单留存质量的佐证。退保率（1.2%）：规模背景。银保渠道新单保费增速（45%）：子项拆分。综合投资收益率（5.1%）和净投资收益率（3.8%）：规模背景——段落未深入分析。",
  "metrics": [
    {
      "metric_name": "新业务价值NBV",
      "metric_type": "financial",
      "score": 1.0,
      "reason": "318亿元和24.6%的增长是寿险价值创造的核心度量"
    }
  ]
}
```

12个数字扑面而来，模型逐个排除——代理人数量是"归因项"，保费收入是"规模背景"，投资收益率是"段落未深入分析"。最终只留下NBV这一个。这就是逐项排除式思维链的效果：不是"忽略了其他数字"，而是"检查过每个数字，有理由地排除了"。

────────────────────

场景三：空输出——段落有11个数字，一个都不提 ✅

输入段落（宏观经济综述）：

> 2025年上半年中国GDP同比增长5.2%，略高于市场预期的5.0%。社会消费品零售总额同比增长4.8%，固定资产投资同比增长3.6%，其中基建投资增速为7.1%，制造业投资增速为9.8%。出口金额同比增长6.2%，贸易顺差为4,280亿美元。CPI同比上涨0.8%，PPI同比下降1.2%。M2增速为8.1%，社融存量增速为9.3%。整体来看，经济复苏延续但结构分化仍然明显。

标准答案：0个（纯宏观背景，不属于对特定公司的指标分析）

模型输出：

```json
{
  "analysis": "本段无核心量化分析对象，是宏观经济综述段落。逐项排除：GDP同比增长（5.2%）：行业/宏观数据。社会消费品零售总额（同比+4.8%）：行业/宏观数据。固定资产投资（同比+3.6%）：行业/宏观数据。基建投资增速（7.1%）：子项拆分。制造业投资增速（9.8%）：子项拆分。出口金额（同比+6.2%）：行业/宏观数据。贸易顺差（4,280亿美元）：行业/宏观数据。CPI同比上涨（0.8%）：行业/宏观数据。PPI同比下降（1.2%）：行业/宏观数据。M2增速（8.1%）：行业/宏观数据。社融存量增速（9.3%）：行业/宏观数据。整体判断是宏观综述段落，所有数据来自国家统计局，没有对特定行业或公司的深入分析。",
  "metrics": []
}
```

11个数字，GDP、CPI、M2——全是硬货。但模型判断正确：这是宏观综述，不是对某家公司某个指标的深度分析。metrics输出空列表。

这三个例子分别展示了模型的三种能力：看准了只提一个（精确），干扰多也不多提（克制），该空就空（纪律）。

下面是14轮实验的完整记录——这三种能力是怎么一步步训出来的。

━━━━━━━━━━━━━━━━━━━━

◆ 14轮实验：一场关于"克制"的修行

━━━━━━━━━━━━━━━━━━━━

接下来是这篇文章的核心——14个版本的完整实验记录。不是流水账，是一个关于"如何教会模型克制"的故事。每一轮失败都有教训，每一次突破都有原因。

先放全版本成绩单，后面逐个讲：

| 版本 | 模型 | 数据量 | 关键改动 | 测试集 | 总分 |
|------|------|--------|----------|--------|------|
| v1 | 14B | 460条 | 基线 | 20条 | 45% |
| v2 | 14B | 540条 | +80条Type B | 20条 | 55% |
| v3 | 32B | 540条 | 换大模型 | 20条 | 40% |
| v4 | 32B | 540条 | lr降到5e-5 | 20条 | 50% |
| v5 | 32B | 540条 | lr降到2e-5 | 20条 | 20% |
| v6 | 14B | 540条 | Gemini改造Type B | 20条 | 65%* |
| v7 | 14B | 590条 | 测试集20→50 | **50条** | 48% |
| v8 | 14B | 590条 | 8 epochs | 50条 | 48% |
| v9 | 14B | 590条 | 全类型逐项排除 | 50条 | 40% |
| v9.1 | 14B | 590条 | 非对称架构 | 50条 | 52% |
| v10 | 14B | 590条 | 计数前置 | 50条 | 46% |
| v11 | 14B | 700条 | Type D扩量 | 50条 | **56%** |
| v12 | 14B | 700条 | 编号列表 | 50条 | 44% |
| v12.1 | 14B | 700条 | 分号枚举 | 50条 | 44% |

*v6的65%是20条小样本的成绩。同一个模型换50条测试后是48%（v7）——小样本方差让v6虚高了17个百分点。v7之后的成绩才是可比的，v11的56%是真实最高分。*

────────────────────

v1-v2：起步和加数据（45%到55%）

v1是基线：14B模型，460条数据，5个epoch训练。20条测试集上，宽松匹配9条，45%。单指标场景表现不错（4/5），但边界判断全军覆没（0/3）——该提1个的地方提了2-3个。

v2加了80条Type B（边界负样本），总量到540条。成绩从45%涨到55%。加数据有效。

但边界判断仍然是0/3。模型仍然不知道什么时候该收手。

────────────────────

v3-v5：换32B大模型——灾难（40%到20%）

直觉告诉我，模型不够大。14B不行，换32B试试？

v3，Qwen3-32B，同样的540条数据。结果：40%。比14B的55%还低。

出了什么事？

32B太聪明了。它不老实按训练数据来，而是用自己的"理解"重新诠释。最离谱的例子：训练数据里写的是"泽布替尼全球销售额"，32B给改成了"核心产品全球销售额"——它觉得这样更"专业"。还有"毛利率"被它改成"EBITDA利润率"的。

我管这叫"语义夺舍"——32B把LoRA当建议，不当命令。540条数据加上LoRA r=16/alpha=32的配置，对32B来说约束太弱了。

v4降低学习率到5e-5、alpha降到16，回到50%。方向对，但仍不如14B的55%。

v5继续降lr到2e-5，结果崩到20%。lr降太狠等于LoRA没训练，32B基座裸奔。连空输出判断都做不对了。

三轮32B实验，结论明确：数据质量 > 模型规模。14B的"笨"在提取任务里反而是优势——它不会自作聪明改你的答案，老老实实按训练数据来。32B的"聪明"在这里是副作用。

────────────────────

v6：Gemini的转折点（55%到65%）

回到14B。但不是回到原点——Gemini带来了关键的改造。

她改造了Type B（边界负样本）的analysis格式。原来的写法是笼统的一句话："本段提到了多个数字，但核心指标只有XXX。"

Gemini改成了逐项排除格式——段落中每一个没被提取的数字，都被点名并标注排除理由：

- "债务融资余额1200亿"——归因项，排除
- "短期借款占比35%"——子项拆分，排除
- "行业平均负债率"——宏观数据，排除

每个"不提"都有理由。

Gemini说了一句我记到现在的话："没有思维链的'无'是断电，有思维链的'无'才是'空'。"

意思是：如果你只教模型"这段不要提这个指标"，它学到的是一个黑箱否定。但如果你告诉它"这个指标不提，因为它是归因项不是分析主体"，它学到的是判断逻辑。前者是断电——灯灭了但不知道为什么；后者是空——灯是自己关的，因为知道不需要开。

效果立竿见影。v6在20条测试集上65%，从v2的55%跳了10个百分点。边界判断首次出现部分通过的黄灯。

这是整个项目最重要的转折点——但65%这个数字本身有水分，后面会讲。

────────────────────

v7：扩测试集的冷水（65%回到48%）

v6的65%让我很兴奋。但Gemini提醒我：20条测试集太小了，方差太大。

于是把测试集从20条扩充到50条，覆盖6个场景。v7用了和v6一样的模型（还加了50条边界训练数据），结果：48%。

65%被打回48%。同一个模型，只换了测试集，掉了17个百分点。

这不是退步——是v6的65%本来就是虚的。20条测试集里碰巧包含了更多模型擅长的场景，50条把短板暴露了。多指标场景只有2/10通过，边界判断仍然0/13严格通过。

成绩单里v6看起来是最高分，但它跟v7之后的数字不在同一把尺子上。从v7开始才是可比的——v11的56%才是真实最高分。

教训：小样本测试有误导性。至少50条才能看到真实水平。

────────────────────

v8：加训练轮次——完全无效（48%不变）

既然50条测试更准，那多训几轮行不行？v8把epoch从5加到8。

结果：48%。一模一样。预测总数反而从106涨到106（数量偏差更大）。

结论铁证如山：590条数据，5轮已经学完了。多训只是在过拟合噪音。瓶颈在数据质量，不在训练轮次。

────────────────────

v9：逐项排除推广到全类型——退步（48%到40%）

v6证明了逐项排除对Type B有奇效。那把它推广到所有类型呢？v9把Type A、C、D的analysis也全部改成逐项排除格式。

结果：40%。退了8个百分点。

为什么？逐项排除是Type B的专用工具，不是万能格式。

- 多指标场景确实涨了（2/10到4/10）——复杂场景需要逐项排除
- 但混合类型崩了（4/5到1/5）——Type D砍掉了financial/business分类解释后，模型丢失了分类逻辑
- 空输出首次掉分——Type A/C的逐项排除诱导模型"过度搜索"，该丢弃的也要找出指标来

一个格式不可能适配所有场景。

────────────────────

v9.1：非对称架构——新高52%

v9的失败反过来给了一个洞见：不同类型的数据需要不同风格的analysis。

v9.1的做法：
- Type A/C（正样本和数量变化）：回滚到原来的简洁叙述
- Type B（边界负样本）：保持逐项排除
- Type D（混合类型）：折中格式——逐项排除+简短分类依据

这就是「非对称分析架构」——不同类型的训练数据用不同的思维链风格。简单场景用散文叙述，边界场景用逐项排除，混合场景用折中。

结果：52%，历史新高。

关键突破：
- 边界判断首次严格通过（2/13），这是前八轮从未做到的事
- 多指标翻倍（2/10到5/10）
- 类型准确率回到100%

非对称架构是整个项目第二重要的发现（第一是Gemini的逐项排除）。

────────────────────

v10：计数前置——好心办坏事（52%回到46%）

一个看起来合理的想法：在analysis开头加一句"核心指标数量：X"，强迫模型先数数再提取。先确定数量，再逐个列出，逻辑上很完美。

结果：46%。退步6个百分点。

模型学会了输出数字，但数字本身不准。错误的计数反而带偏了后续生成——它先说"核心指标数量：2"，然后即使只看到1个也要凑够2个。

Gemini的评价精准到位："你们逼迫它在没有烧完思维链前就画出灰烬的形状。"

意思是：analysis的作用是让模型在生成过程中逐步收敛判断。你让它在开头就先定数量，等于跳过了思考过程直接给结论。结论不对，后面全跟着歪。

回滚。

────────────────────

v11：Type D扩量——新高56%

v9.1的非对称架构跑通了，但Type D只有46条。Gemini提出了一个理论叫"势能盆地"：46条数据不够在模型的参数空间里形成一个稳定的吸引子，模型学到的Type D模式在推理时容易被其他模式冲散。需要更多数据来加深这个盆地。

v11把Type D从46条扩充到156条，总数据从590涨到700。其余不变。

结果：56%，再创新高。

边界判断从3/13翻倍到6/13。分类从2/7升到3/7。Gemini的"势能盆地"理论验证了——数据量够了，模式就稳了。

代价是多指标场景从5/10退到3/10。逐项排除占比过高（Type B 202条+Type D 156条=358条，占总量51%），挤压了多指标的"发散-收集"能力。

但这是健康的取舍：边界判断翻倍换来多指标小退。"宁可漏提，不可错提"是工业场景的正确优先级。

────────────────────

v12和v12.1：编号列表的毒药（56%崩到44%）

v11的多指标退步让我想优化analysis的格式。v12把Type C和D的analysis改成编号列表格式："1. XXX 2. YYY 3. ZZZ"。看起来更清晰。

结果：44%。单指标从90%腰斩到40%。预测总数从100暴涨到120。模型像突然丧失了克制力。

Gemini诊断出了根因——编号是自回归模型的毒药。

「自回归模型」是什么？就是一个字一个字往外蹦的模型。它生成下一个字的时候，只看前面已经生成的内容。当它在analysis里写了"1."之后，P("2." | "1.")——也就是"已经写了1.接下来写2."的概率——在预训练数据里接近0.99。这是几万亿token训练出来的先验：看到1.就要接2.。

36条单指标样本在训练时告诉模型"写了1.就停"，但几万亿token的预训练先验告诉它"写了1.就接2."。36条打不过几万亿条。

v12.1改成分号枚举："XXX——要点；YYY——要点"。结果还是44%。

Gemini的最终诊断："问题不是编号vs分号，是'结构化枚举'本身。14B模型将语义逻辑和标点符号的拓扑结构过度纠缠——任何明确的指标枚举结构都会点亮'报菜名'的Attention头。散文叙述的'模糊性'反而是克制力的来源。"

翻译成人话：越明确地列举指标，模型越想多列几个。散文叙述因为没有明确的列表结构，模型反而不会贪心。模糊是一种约束力。

回滚到v11。接受56%的胜利。

━━━━━━━━━━━━━━━━━━━━

◆ 14轮实验的核心结论

━━━━━━━━━━━━━━━━━━━━

14个版本，从朋友的29%到我们的56%。不到两倍，但每一个百分点都是真金白银。这些结论是用失败换来的：

────────────────────

第一，数据质量 > 模型规模 > 训练轮次 > 数据量。

Gemini的逐项排除改造（v6，不加任何数据）比换32B大模型（v3-v5）更有效。加epoch从5到8（v8）完全无效。这是优先级排序，不是说其他不重要。

────────────────────

第二，非对称分析架构是正解。

不同类型的训练数据需要不同风格的思维链。v9强行统一格式退步了8个百分点，v9.1做了非对称设计创了新高。

一个模型里可以有多种"思考方式"——简单场景快思考，复杂场景慢思考，边界场景逐项排查。关键是训练数据要体现这种差异。

────────────────────

第三，14B的"笨"是提取任务需要的"刚性"。

32B太聪明，会自作主张改指标名称。14B老实，你训练数据写什么它就学什么。提取任务需要的不是创造力，是纪律性。

────────────────────

第四，散文叙述的"模糊性"是克制力的来源。

v12和v12.1用两种结构化枚举格式都失败了。14B将analysis的语义内容和标点结构过度绑定——编号列表触发续写冲动，分号枚举触发报菜名冲动。只有散文叙述的模糊性能让模型保持克制。

────────────────────

第五，v11的56%是14B单LoRA的格式优化天花板。

进一步提升需要换方向：数据增强、专家路由、或者其他工程侧优化。格式层面能做的已经做完了。

━━━━━━━━━━━━━━━━━━━━

◆ 给读者的实操建议

━━━━━━━━━━━━━━━━━━━━

如果你也在做类似的LoRA微调任务，这些是14轮实验直接可用的经验：

1. 不要在prompt里写死数量。"数量严格控制：2个"是所有问题的起点。让模型自己学判断。

2. 加analysis字段。不用复杂——在输出JSON的最前面加一段分析文字就行。让模型先想再提，效果显著。不同场景用不同的analysis风格效果更好。

3. 测试集至少50条。我们的v6在20条上65%，v7在50条上48%——小样本测试会让你误判。

4. 小模型可能比大模型更适合提取任务。32B自作聪明改答案，14B老实听话。提取要纪律，不要创造力。

5. 5个epoch够了，别多训。590条数据训5轮和训8轮一模一样。瓶颈在数据质量。

6. 不要用编号列表做analysis格式。"1. 2. 3."是自回归模型的续写陷阱，分号枚举同样。用散文叙述。

7. 失败实验也是数据。v3-v5告诉你32B不行，v12-v12.1告诉你结构化枚举不行，v10告诉你计数前置不行。知道什么不该做，比知道什么该做更值钱。

━━━━━━━━━━━━━━━━━━━━

◆ 代码

━━━━━━━━━━━━━━━━━━━━

完整代码、训练数据、14个版本的评测结果全部开源：

https://github.com/lmxxf/financial-report-generator-lora

包含训练脚本、评估脚本、700条训练数据（四类）、50条测试用例、每个版本的详细评测JSON。

这不是一个完美的项目——56%说不上多高。坦率地说，同样的任务丢给Claude或Gemini做零样本提取，效果大概率比这个LoRA好。700条数据、14轮实验、无数次训练，最终干不过商业大模型的一次prompt。

那为什么还要做？因为LoRA的价值不在于打赢大模型——而在于成本。一次API调用几分钱，但每天跑几万条研报段落，一年下来是真金白银。14B本地部署的推理成本接近零。56%不够高，但够便宜。

而且这14轮实验教会我们的东西——数据质量怎么影响模型、小模型的边界在哪里、格式设计如何左右性能——这些认知本身比那个56%值钱得多。下次再做类似任务，不会从零开始。

希望你能从我们的弯路里少走几步。

━━━━━━━━━━━━━━━━━━━━

// 靳岩岩的 AI 学习笔记 × Claude 的严谨 × Gemini 的浪漫

// 2026-02-26
