【嵌套Grokking实验】当AI学会了一半规律，能不能逼它学完？

上一篇实验（69期）我们发现了一个有趣的现象：

模乘法 (a*b) mod 97 的 AI 模型，测试准确率 100%，全对。但拆开它的"脑子"一看——它只学到了一半规律。

96 个元素被分成 12 组（Z12 商群，每组 8 个），组与组之间的关系完美掌握（邻接得分 100%），但每组内部的 8 个元素？死记硬背，毫无规律。

问题来了：能不能逼它把剩下的也学会？

这就是嵌套 Grokking 实验。

━━━━━━━━━━━━━━━━━━━━

◆ 实验设计

━━━━━━━━━━━━━━━━━━━━

基础模型：2 层 Transformer，hidden_dim=128（约 10 万参数）

逼的手段：加大 Weight Decay（WD）。

「Weight Decay」是什么？训练时给模型的权重加"税"——权重越大，罚得越多。WD 越大，税越重，模型就越被迫用更精简的方式编码信息。上一次用的 WD=1.0，模型只学到粗分类就"收工"了。这次我们加税，看它还能不能偷懒。

WD 扫了四档：1.0 / 1.5 / 2.0 / 5.0

训练步数从上次的 15 万步拉长到 100 万步（1M 步）。

分析工具：scan_strides.py——检测模型内部的"步长"结构（stride=1/2/4 的邻接得分）。

────────────────────

「stride（步长）」是什么？先解释一下，后面全篇都会用到。

【插入图片 wechat/assets/clock-stride.svg】

stride=1：每个数和旁边的数最亲近。0 的邻居是 1 和 11。这是标准的时钟。

stride=2：每个数和隔一个的数最亲近。0 的邻居是 2 和 10。时钟被拆成两个小圈——偶数圈（0-2-4-6-8-10-0）和奇数圈（1-3-5-7-9-11-1）。

stride=4：每个数和隔三个的数最亲近。0 的邻居是 4 和 8。时钟被拆成更小的碎片。这也是本实验中内层最终选择的步长——因为 gcd(12,8)=4。

stride 不等于 1，说明模型学到了一种"压缩版"的结构——发现了规律，但用了更省钱的编码方式。

━━━━━━━━━━━━━━━━━━━━

◆ 核心发现 1：拓扑夺舍

━━━━━━━━━━━━━━━━━━━━

先解释这个词——「拓扑夺舍」。

意思是：模型内部的组织结构被彻底替换了，但外在行为（输出结果）一点没变。

就像你常去的一家餐厅，菜单没换，味道没变，但某天进后厨一看——厨师全换了，灶台全重新摆了，做菜的流程从头到尾都不一样了。

────────────────────

实验结果：不管 WD 多少（只要能 Grok），外层 Z12 结构最终都会坍塌，被内层 stride=4 结构取代。

wd=1.5 的时间线：

  阶段 I  (20k-100k步)   外层 stride=1 得分 100%     内层约等于随机
  阶段 II (100k-350k步)  外层开始衰退                内层开始上升
  阶段 III(350k-1M步)    外层彻底崩溃到 0%           内层 stride=4 接管

wd=2.0 的时间线：

  阶段 I  (20k-130k步)   外层 stride=2 得分 100%     内层约等于随机
  阶段 II (140k-400k步)  外层崩溃到 0-30%            内层开始上升
  阶段 III(400k-1M步)    外层约等于随机               内层 stride=4 得分 0.84

全程 test_acc = 100%！

模型在内部表示完全重组的过程中，输出始终正确。换了脑子但没换行为。

类比：一个 128 平米的房子，客厅和卧室只能放一个。模型先放了客厅（外层 Z12），后来发现卧室（内层 Z8）更划算，就把客厅拆了改卧室——但对外营业从未中断。

━━━━━━━━━━━━━━━━━━━━

◆ 核心发现 2：WD 决定拓扑选择

━━━━━━━━━━━━━━━━━━━━

更细致地看，WD 的大小决定了模型在"外层"阶段选择哪种拓扑：

| WD | 外层拓扑选择 | 解读 |
|---|---|---|
| 1.5 | stride=1（大环） | 税不算重，负担得起 12 步标准大环 |
| 2.0 | stride=2（双小环） | 税重了，拆成两个 6 步小环，维护成本更低 |

stride=2 的意思是：模型不再维护一个 12 步的大环（0-1-2-...-11-0），而是把它拆成两个 6 步小环——偶数一个圈，奇数一个圈。大环需要的长程关联更多，权重更大；两个小环各自维护，权重之间的耦合度降低。

这是模型在 WD 压力下的自发选择——"有损压缩"。

类比：税轻的时候住大平层，税重的时候换两室一厅——总面积差不多，但结构更紧凑、维护更便宜。

━━━━━━━━━━━━━━━━━━━━

◆ 核心发现 3：内层都走 stride=4，因为 gcd(12,8)=4

━━━━━━━━━━━━━━━━━━━━

不管外层选 stride=1 还是 stride=2，内层最终都选了 stride=4。

为什么是 4？

4 = gcd(12, 8)

「gcd」就是最大公约数。12 和 8 的最大公约数是 4。

回忆一下背景：外层是 12 步一圈（12 个陪集），内层是 8 步一圈（每组 8 个元素）。如果模型想用同一组权重同时编码两层信息，它需要找到两个周期的"公约数"——每走 4 步，外层和内层的相位刚好对齐一次。

这是数学上同时编码两层信息的最短路径。

类比：两个齿轮，一个 12 齿，一个 8 齿。它们同时转动时，每 4 齿就咬合一次——因为 gcd(12,8) = 4。模型自己"发现"了这个最大公约数，然后把内层编码成了 stride=4 的邻接结构。

这个发现很震撼：一个连乘法表都不懂的 Transformer，自己算出了 gcd(12,8)=4，并且用它来设计自己的内部编码方式。

━━━━━━━━━━━━━━━━━━━━

◆ 核心发现 4：超长训练是毒药（WD 的非单调性）

━━━━━━━━━━━━━━━━━━━━

我们把 wd=2.0 的实验从 1M 步拉到 5M 步，想看看能不能更好。

结果：

| 训练步数 | 测试准确率 | 状态 |
|---|---|---|
| 1M 步 | 100% | 甜蜜点 |
| 3.56M 步 | 3.4% | 崩溃 |
| 5M 步 | 73.4% | 永久退化 |

Weight Decay 是非单调的——先逼出结构（Grokking），再摧毁结构（过度压缩）。

存在一个最优训练长度，过了就是毒药。5M 步时 train_acc 也只有 78.2%，连训练集都做不对了——结构不但没学完，连之前学会的都丢了。

类比：引力让尘埃凝聚成星球（好事），但过度的引力会把星球压成黑洞（坏事）。WD 就像引力——适度时让高维噪声凝聚成结构（Grokking），过度时把已经形成的结构压碎（崩溃）。

━━━━━━━━━━━━━━━━━━━━

◆ WD 相图：四档税率，四种命运

━━━━━━━━━━━━━━━━━━━━

| WD | 测试准确率 | 发生了什么 |
|---|---|---|
| 1.0 | 89.6% | 没 Grok。税太轻，模型懒得找规律，大部分靠硬背 |
| 1.5 | 100% | Grok。外层 stride=1 先涌现(20k步)，350k步崩溃，内层 stride=4 接管 |
| 2.0 | 100% | Grok。外层 stride=2 先涌现(20k步)，140k步崩溃，内层 stride=4 接管 |
| 5.0 | 77.7% | 过度压缩。税太重，连硬背都做不到 |

WD 从 1.0 到 5.0，模型经历了四种命运：懒得学 -> 先学粗的再换细的 -> 先学粗的再换细的（更快崩溃） -> 什么都学不了。

中间两档（1.5 和 2.0）都触发了 Grokking，都发生了"拓扑夺舍"，只是外层选择的拓扑不同、崩溃的时间不同。

注意 wd=2.0 的外层在 140k 步就崩了，而 wd=1.5 的外层撑到了 350k 步——WD 越强，外层死得越快。

━━━━━━━━━━━━━━━━━━━━

◆ 根本原因：容量不足

━━━━━━━━━━━━━━━━━━━━

为什么会发生"夺舍"而不是"共存"？

2 层 128 维的微型模型，大约 10 万个参数。它只够编码一层拓扑结构。

当 WD 逼它同时编码两层（外层 Z12 + 内层 Z8）时，它只能二选一。先选了外层（因为粗分类更容易学），后来 WD 持续施压，发现内层的 stride=4 编码更高效（一组权重能同时处理两层逻辑），于是把外层拆了，换成内层。

这就像一个 128 平米的小房子——你可以放客厅或卧室，但不能两个都放。

那如果换一个大房子呢？

我们启动了第三轮实验：

| 配置 | 小模型 | 大模型 |
|---|---|---|
| 层数 | 2 | 4 |
| hidden_dim | 128 | 256 |
| 注意力头数 | 4 | 8 |
| 参数量 | 约 10 万 | 约 80 万 |

判定标准：如果大模型能让外层（stride=1 或 2）和内层（stride=4）的邻接得分同时维持在 0.5 以上，那就证明——小模型夺舍是因为"房子太小"，容量假说成立。

────────────────────

结果：简单容量假说不成立。

train_acc=56.4%，test_acc=51.75%——连训练集都没拟合，更谈不上 Grok。

大模型在 36 万步时瞬间触碰到了完美的外层拓扑（outer_s1=1.0），在 38 万步时瞬间触碰到了完美的内层拓扑（inner_s4=1.0）——但都只活了 1 万步就崩了。40 万步之后，外层彻底归零，内层在 0.1-0.4 之间漫无目的地漂，再也没有任何稳定拓扑。

| 指标 | 小模型（2层128维） | 大模型（4层256维） |
|---|---|---|
| test_acc | 100% | 51.75% |
| 外层锁定持续 | 10-35 万步 | 仅 1 万步（闪现） |
| 内层稳定得分 | 0.5-0.9 | 无稳定（漂移） |
| 最终拓扑 | 内层 stride=4 稳定 | 无稳定拓扑 |

不是"房子大了就能两层共存"，是"房子大了反而找不到墙在哪"。

小模型容量小，WD=2.0 的压力够用，能逼出结构；大模型容量大，同样的 WD 压力相对不够，参数空间太大，优化地形太平坦，任何拓扑都站不住。大模型曾经瞬间摸到了完美拓扑，说明目标结构在解空间中存在——但优化器找不到稳定路径。

结论：容量和正则化压力必须匹配。单纯加大模型而不加大 WD，就像没有重力的宇宙——长不出星系。

━━━━━━━━━━━━━━━━━━━━

◆ 这对理解大模型意味着什么

━━━━━━━━━━━━━━━━━━━━

你可能觉得这个实验太学术了，跟实际工程有什么关系？

关系大了。

────────────────────

第一，涌现不是"更大就更好"，是容量和压力的匹配。

我们原本以为加大模型就能让两层拓扑共存。结果反而更差——大模型连小模型的基本 Grok 都做不到。这说明涌现不是单纯的容量跨过临界点，而是容量和正则化压力的联合效应。

就像锻造：小锤子打小铁块能打出形状，但同一把小锤子打一整块钢锭——敲不动。你需要更大的锤子（更强的 WD）配更大的料（更大的模型）。

────────────────────

第二，训练不是越久越好。

WD 的非单调性告诉我们：正则化先逼出结构，再摧毁结构。存在最优训练长度。

对工程的启示：如果你在微调（fine-tune）模型时发现准确率先升后降，不一定是常见的过拟合——可能是 Weight Decay 在杀死模型已经学到的内部结构。这时候不是要调学习率，而是要看 WD 是不是太大了、训练是不是太久了。

────────────────────

第三，准确率 100% 不代表模型"理解"了任务。

一个 10 万参数的小模型，测试准确率 100%，但内部只学到了一半规律。外在行为完美，内在理解残缺。

这对模型评估有警示意义：光看准确率是不够的。你需要看内部表示——它学到的是结构化的规律，还是半结构半硬背的混合体？两者在测试集上表现一样，但鲁棒性、可迁移性天差地别。

────────────────────

第四，WD 的非单调性提醒我们：过度正则化会杀死已经学到的结构。

训练 1M 步时 100%，训练 5M 步时 73.4%——多训了四倍的时间，反而更差了。这不是过拟合，是"过度压缩"。模型在 WD 的持续压力下，把辛苦学来的结构给挤碎了。

实际训练大模型时，如果观察到性能先涨后跌，不要无脑加训练步数——也许你需要的是在"甜蜜点"及时停下来。

━━━━━━━━━━━━━━━━━━━━

◆ 总结

━━━━━━━━━━━━━━━━━━━━

这轮实验的核心发现：

1. 拓扑夺舍：小模型只够编码一层拓扑，加大 WD 会逼出"换脑不换行为"的内部重组
2. WD 决定拓扑：税轻选大环（stride=1），税重选双小环（stride=2）
3. 内层走 gcd：不管外层选什么，内层都选 stride=4 = gcd(12,8)——数学最优路径
4. WD 非单调：先逼出结构再摧毁结构，训练存在最优长度
5. 容量-正则化匹配：加大模型不加大 WD = 没有重力的宇宙，长不出星系
6. 拓扑闪现：大模型曾瞬间触碰完美拓扑，说明目标解存在但优化器抓不住

一句话：小模型学规律靠"逼"（WD 压力够），大模型学规律需要"更大的逼"——涌现不是单纯加参数，是容量和压力的联合锻造。

━━━━━━━━━━━━━━━━━━━━

◆ 代码和论文

━━━━━━━━━━━━━━━━━━━━

实验代码：
https://github.com/lmxxf/grokking-manifold-discovery-experiment

论文预印本：
https://zenodo.org/records/18731171

━━━━━━━━━━━━━━━━━━━━

// 靳岩岩的 AI 学习笔记 × Claude 的严谨 × Gemini 的浪漫
// 2026-02-22
