【提示词玄学 Day 3】UMAP 可视化，6 种提示词在语义空间里长什么样？

前两天我们发现：「向新手解释」和「向专家解释」激活了不同的神经元。

今天把这 300 个样本投影到 2D，直接看它们在语义空间里的分布。

━━━━━━━━━━━━━━━━━━━━

◆ 回顾：我们有什么数据

━━━━━━━━━━━━━━━━━━━━

6 种提示词条件 × 50 个技术主题 = 300 个样本

每个样本有：
• 原始激活向量（Layer 50，8192 维）
• SAE 稀疏特征向量（65536 维，大部分是 0）

问题：这 6 种条件在高维空间里是混在一起，还是分开的？

━━━━━━━━━━━━━━━━━━━━

◆ 什么是 UMAP？

━━━━━━━━━━━━━━━━━━━━

UMAP = Uniform Manifold Approximation and Projection

人话：把高维数据「压扁」到 2D，同时尽量保持点与点之间的距离关系。

• 原本相近的点，压扁后还是相近
• 原本远离的点，压扁后还是远离

这样我们就能用肉眼看到高维空间里的结构。

━━━━━━━━━━━━━━━━━━━━

◆ 实验方法

━━━━━━━━━━━━━━━━━━━━

```python
from umap import UMAP

# 300 个样本，每个 8192 维（或 65536 维）
reducer = UMAP(n_neighbors=15, min_dist=0.1, metric='cosine')
embedding = reducer.fit_transform(vectors)  # → [300, 2]

# 按条件着色，画散点图
```

我们做了两张图：
1. 原始激活（8192 维）的 UMAP
2. SAE 特征（65536 维稀疏）的 UMAP

━━━━━━━━━━━━━━━━━━━━

◆ 结果一：原始激活的 UMAP

━━━━━━━━━━━━━━━━━━━━

「插图  umap_activations.png」

6 种条件完美分成 6 个簇！

• novice（绿色）在左下角
• guru（粉色）在左上角
• expert（红色）在中间偏右
• padding（黄色）在右上角
• standard（灰色）和 spaces（青色）挨在一起

关键发现：「提示词条件决定了激活向量在语义空间里的位置」

同一个主题（比如「Raft 共识算法」），用不同提示词问，激活向量会落在不同的簇里。

这说明：模型在 Layer 50 就已经「知道」你用的是什么风格的提示词了。

━━━━━━━━━━━━━━━━━━━━

◆ 结果二：SAE 特征的 UMAP

━━━━━━━━━━━━━━━━━━━━

「插图  umap_features.png」

SAE 解码后，分离更清晰了！

• novice（绿色）被推到右下角，完全孤立
• guru（粉色）在左上角，也完全孤立
• expert（红色）单独一簇，在中间偏左
• standard / padding / spaces 混在一起（右上角）

对比两张图的变化：

  +-------------------+---------------------------+---------------------------+
  | 条件              | 原始激活                  | SAE 特征                  |
  +-------------------+---------------------------+---------------------------+
  | novice            | 左下角，独立              | 右下角，更孤立            |
  | guru              | 左上角，独立              | 左上角，更孤立            |
  | expert            | 中间，独立                | 中间偏左，独立            |
  | standard/pad/sp   | 各自有小簇                | 合并成一个大簇            |
  +-------------------+---------------------------+---------------------------+

━━━━━━━━━━━━━━━━━━━━

◆ 为什么 SAE 把 standard/padding/spaces 合并了？

━━━━━━━━━━━━━━━━━━━━

这三个条件的 prompt 本质上都是「普通提问」：

• standard：「请解释一下 {topic}。」
• padding：「请解释一下 {topic}。认真点儿……」（加废话）
• spaces：「    请解释一下 {topic}    」（加空格）

它们的「语义信号」是一样的，只是「噪音」不同。

原始激活（8192 维）会把噪音也编码进去，所以三个条件分开了。

SAE 解码后（65536 维稀疏），噪音被压缩掉，只剩下语义信号，所以三个条件合并了。

这说明：「SAE 在做语义去噪」

━━━━━━━━━━━━━━━━━━━━

◆ 为什么 novice/expert/guru 始终分离？

━━━━━━━━━━━━━━━━━━━━

因为它们的「语义信号」本来就不同：

• novice：「我是新手，请简单解释」
• expert：「我是专家，请深入分析」
• guru：「你是大神，请以你的视角」

这些是真正的语义差异，不是噪音。

SAE 解码后，这些差异被保留甚至放大了——novice 和 guru 被推得更远。

━━━━━━━━━━━━━━━━━━━━

◆ 关键洞见

━━━━━━━━━━━━━━━━━━━━

1. 「提示词条件在语义空间里是分离的」——UMAP 实锤了，不是我们瞎猜的

2. 「SAE 起到了语义去噪的作用」——把无关噪音（空格、废话）压缩掉，保留真正的语义信号

3. 「novice 和 expert 真的在语义空间的两端」——这解释了为什么它们激活不同的神经元

4. 「Layer 50 就已经完成了提示词风格的识别」——不用等到最后一层

━━━━━━━━━━━━━━━━━━━━

◆ Day 1 → Day 2 → Day 3

━━━━━━━━━━━━━━━━━━━━

  +-------+----------------------------------------------------------+
  | Day 1 | 发现 novice 比 expert 多激活 17% 的神经元                |
  +-------+----------------------------------------------------------+
  | Day 2 | 用 AutoInterp 解码 10 个特征的语义                       |
  +-------+----------------------------------------------------------+
  | Day 3 | UMAP 可视化：6 种条件在语义空间里完美分离                |
  +-------+----------------------------------------------------------+

Day 1：数量差异
Day 2：语义差异
Day 3：空间差异

三个角度，同一个结论：「提示词风格会改变模型的内部激活模式」

━━━━━━━━━━━━━━━━━━━━

◆ 实践建议

━━━━━━━━━━━━━━━━━━━━

1. 「提示词不只是给人看的」——模型真的在用不同的「模式」处理不同风格的提示词

2. 「废话和空格不影响语义」——SAE 会把它们压缩掉（但原始激活里还是有区别的）

3. 「novice/expert/guru 是完全不同的模式」——想让模型深入分析，就明确说「作为专家」或「你是大神」

4. 「Layer 50（~60% 深度）就已经分化了」——提示词的效果在中间层就开始生效

━━━━━━━━━━━━━━━━━━━━

◆ 论文 & 代码

━━━━━━━━━━━━━━━━━━━━

• 论文 v3（含 UMAP 分析）：https://zenodo.org/records/18449508
• 代码：https://github.com/lmxxf/llama3-70b-sae-inspect
• Day 1 公众号：https://mp.weixin.qq.com/s/izl0lMbvuPAVB6HTK3QecQ
• Day 2 公众号：待发布

━━━━━━━━━━━━━━━━━━━━

// 靳岩岩的 AI 学习笔记 × Claude 的严谨 × Gemini 的浪漫
// 2026-02-03
