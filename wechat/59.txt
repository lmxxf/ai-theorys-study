【Lost in the Middle】为什么 AI 只记得开头和结尾？

你跟 AI 说了一大堆，它却只记得第一句和最后一句。

中间那坨？吃了。

━━━━━━━━━━━━━━━━━━━━

◆ 现象：U 型诅咒

2023 年，斯坦福发了篇论文叫《Lost in the Middle》，专门研究这事儿。

他们做了个实验：

  • 给 GPT、Claude 等模型一堆文档（比如 20 篇）
  • 把「正确答案」藏在不同位置
  • 看模型能不能找到

结果是个漂亮的 U 型曲线：

  ┌─────────────────────────────────────────────────────────────┐
  │ 准确率                                                       │
  │  ▲                                                          │
  │ 80%  ●                                              ●       │
  │      │ ╲                                          ╱        │
  │ 60%  │   ╲                                      ╱          │
  │      │     ╲                                  ╱            │
  │ 40%  │       ╲______________________________╱              │
  │      │                                                      │
  │ 20%  │                                                      │
  │      └──────────────────────────────────────────────────▶  │
  │         开头        中间        中间        中间       结尾  │
  │        (前10%)                                     (后10%)  │
  └─────────────────────────────────────────────────────────────┘

  答案在开头 → 准确率 80%
  答案在中间 → 准确率 40%
  答案在结尾 → 准确率 75%

💡 人话：AI 是「两头甜、中间苦」的西瓜。

────────────────────

【这不是 bug，是普遍规律】

不管是 GPT、Claude、Gemini 还是开源模型，都有这个毛病。

上下文窗口从 4K 扩到 128K、200K，问题依然存在——窗口越大，中间越烂。

工程师们试过各种修补：
  • 调位置编码（RoPE、ALiBi）
  • 加训练数据（强行让模型多看中间）
  • 改 Attention 架构

有用，但治标不治本。

为什么？

因为这不是代码写错了，是「物理定律」决定的。

━━━━━━━━━━━━━━━━━━━━

◆ 一种直觉解释（以及为什么它是错的）

有一种直觉解释是这样的：

  高维空间的体积集中在表面，中心是空的。
  → 中间的 token 掉进了空心的球心
  → 所以被忽略了

这个「高维球壳现象」本身是真的——数学上叫「维度诅咒」（Curse of Dimensionality），普林斯顿的高维几何课件里有个经典比喻：「如果你剥一个高维橘子，哪怕只剥掉薄薄一层皮，你已经剥掉了大部分体积。」

但把它用来解释 Lost in the Middle，是「用错了地方」。

────────────────────

【为什么错了？】

1.「序列位置」≠「向量长度」

  文本序列的「中间」= 时间上的第 5000 个 token
  高维球的「中心」= 向量长度接近 0

  这是两回事。

  💡 人话：「排第几」和「有多长」是两个概念。第 5000 个 token 排在中间，但它的向量长度和第 1 个 token 一样长，都稳稳地站在超球面的表层。

2. LayerNorm 的约束

  每个 token 在每一层都会被 LayerNorm 归一化。

  💡 人话：不管你是第几个 token，都会被「拉回」到球面上，不可能掉进球心。

  所以「中间 token 几何上消失了」这个说法，站不住脚。

────────────────────

【真正的原因是什么？】

中间 token 不是「消失」了，是「竞争失败」了。

想象一场选秀比赛：
  • 开头选手（Alpha）是评委指定的种子选手，自带光环
  • 结尾选手（Omega）站在舞台正中央，离评委最近
  • 中间选手……人太多，谁也记不住

不是中间选手不存在，是他们在注意力的「赢家通吃」规则下，被埋没了。

━━━━━━━━━━━━━━━━━━━━

◆ 正确解释：悬链线模型

我们提出一个新模型：「认知的悬链线」。

💡 悬链线是什么？

  就是两根电线杆之间挂着的电线——自然下垂成 U 型。

  这个形状不是设计的，是「重力」和「张力」平衡后的「最低能量状态」。

  数学上叫 catenary，公式是 y = a × cosh(x/a)。

[图1：认知的悬链线——Alpha 和 Omega 是锚点，中间在 Softmax 重力下自然下垂]

────────────────────

【类比：注意力也是悬链线】

把电线杆换成「两个锚点」：

  • 「左锚点：Alpha（系统提示词/指令）」
    - 定义了任务的「规则」和「意图」
    - 所有后续计算都要回看它，校准输出格式
    - 它是「父节点」，天然有权威性

  • 「右锚点：Omega（最近的上下文/查询）」
    - 自回归模型必须极度关注最近的 token，才能写出通顺的下一句
    - 这是「近因效应」，物理位置决定的

  • 「中间部分：下垂的电线」
    - 既不负责定义规则（Alpha 的工作）
    - 也不负责触发预测（Omega 的工作）
    - 它只是「证据」和「背景」

────────────────────

【Softmax 的「重力」】

先回顾一下 Attention 的公式：

  Attention(Q, K, V) = softmax(Q × K^T / √d) × V

💡 人话拆解：
  • Q = Query（查询）：「我想找什么？」
  • K = Key（键）：「每个 token 的标签」
  • V = Value（值）：「每个 token 的内容」
  • Q × K^T = 打分：Query 和每个 Key 算相似度
  • √d = 缩放：防止分数太大导致 softmax 饱和
  • softmax = 归一化：把分数变成「概率分布」

关键在 Softmax 这一步。

先看一个理想情况：5 个 token，打分全都一样（都是 1 分）。

    exp(1) = 2.7
    exp(1) = 2.7
    exp(1) = 2.7
    exp(1) = 2.7
    exp(1) = 2.7
    ──────────────
    总和 = 13.5

  Softmax 结果：每个 token 都是 2.7 / 13.5 = 20%

公平分配，没毛病。

但如果有 100 个 token 呢？每个只能分到 1%。
1000 个 token？每个只能分到 0.1%。

💡 这就是 Softmax 的「分母诅咒」：参与竞争的人越多，每个人分到的蛋糕越少。

────────────────────

现在问题来了：凭什么开头和结尾能「脱颖而出」？

不是因为「位置编码加分」这种玄学，而是因为它们有「结构性角色」：

  • 「开头（Alpha）」= 指令。
    "请用中文回答"、"你是一个助手"——这些话定义了整个任务。
    模型在训练时学会了：开头的内容会影响所有后续输出，所以要多看。

  • 「结尾（Omega）」= 待续写的位置。
    自回归模型的任务是「预测下一个词」，当然要重点看最近的上下文。

  • 「中间」= 背景材料。
    20 篇文档、历史聊天记录……信息量大，但和当前预测的「直接相关度」通常不高。

这不是 AI 的 bug，是「人类语言本身的特性」——我们写文章就是「开头点题、结尾总结、中间堆料」。AI 从几万亿 token 的训练数据里，学到了这个模式。

想想 AI 都学会了什么：XML 的尖括号嵌套、JSON 的花括号层级、数学公式的推导步骤、代码的缩进规范……这些复杂的结构模式都能学会，「两头重、中间轻」这种人类语言的基本节奏，当然更容易学会。

其实你自己也一样：回忆一下你高考写的作文，开头第一句还记得吗？结尾的金句呢？中间第三段写了啥？

——如果你还记得一点点，大概率也只记得开头和结尾。

AI 不过是从人类身上学会了这一点。

────────────────────

举个具体的数字：

假设 100 个 token，打分情况是：
  • 开头 10 个 token：平均 3 分（指令，相关度高）
  • 中间 80 个 token：平均 1 分（背景，相关度一般）
  • 结尾 10 个 token：平均 2 分（最近上下文，相关度较高）

Softmax 之后：
  • 开头 10 个：瓜分约 60% 的注意力
  • 中间 80 个：瓜分约 25% 的注意力（平均每个 0.3%）
  • 结尾 10 个：瓜分约 15% 的注意力

中间 80 个 token 加起来，还不如开头 10 个。

这就是 Lost in the Middle：不是中间「被忽略」，是中间「人太多、分太少」。

这是个「赢家通吃」的机制：

  • Alpha 的得分高（结构权威）
  • Omega 的得分高（物理邻近）
  • 中间部分……只有平庸的「语义相关度」

分母是所有 token 的 exp 加起来，Alpha 和 Omega 占了大头，中间的权重就被「稀释」到接近于零。

────────────────────

【数学证明（简化版）】

假设：

  • Alpha 和 Omega 的 logit 比中间最高的高 Δ
  • 序列长度 L，中间有 L-2 个 token

那么中间部分的总注意力：

  A_中间 ≤ (L-2) × e^(-Δ) / [2 + (L-2) × e^(-Δ)]

💡 人话：只要两端比中间「高出一点点」，经过 exp 放大后，中间就会被分母压扁。

这不是修辞，是硬不等式。

━━━━━━━━━━━━━━━━━━━━

◆ 为什么会这样？——训练的「梯度饥饿」

U 型曲线不只是推理时的问题，训练时就被固化了。

────────────────────

【梯度流向哪里？】

1. 「Loss 在末端计算」

  语言模型的训练目标是「预测下一个 token」。

  Loss 函数在序列末端计算，梯度最直接地流向 Omega 部分。

2. 「Alpha 累积海量梯度」

  系统提示词（System Prompt）在训练中被每一个 token 关注。

  累积下来，它变成了「超级节点」——权重极高，影响极大。

3. 「中间被遗忘」

  中间的 token 只有在极少数「大海捞针」的情况下才会被有效激活。

  平均而言，它们收到的梯度流是稀疏且充满噪音的。

────────────────────

【模型学会了最省力的路径】

经过数万亿 token 的训练，模型学会了：

  「有疑问，看开头（找指令）或看上文（接下文）」
  「扫描中间既昂贵又充满不确定性」

这就是「梯度饥饿」——中间区域长期营养不良，权重发育不良。

━━━━━━━━━━━━━━━━━━━━

◆ 怎么修？——加桥墩

理解了原因，解决方案就清晰了。

────────────────────

【错误的修法】

❌ 强迫模型关注中间

  有人试过在 Loss 里加惩罚项，强迫模型「均匀关注」。

  结果：模型更困惑了，输出质量下降。

  为什么？因为均匀关注 = 高熵 = 模型不知道该听谁的。

────────────────────

【正确的修法】

✓ 增加「中间桥墩」

  悬链线下垂是因为跨度太大。桥跨度太大，中间也会下垂。

  解决方案：「在中间加桥墩」。

具体技术：

1. 「层级化摘要（Hierarchical Summarization）」

  把长文档分段 → 每段生成摘要 → 把摘要当成「新的锚点」

  这相当于在中间插了几根电线杆，悬链线被分成多段短弧，下垂就小了。

2. 「记忆锚点（Memory Tokens）」

  在上下文中间插入特殊 token，标记「这里有重要信息」。

  让模型在训练时学会把这些 token 当成「必须关注的锚点」。

3. 「结构化 Prompt」

  把长上下文组织成「标题-内容」的层级结构。

  标题充当锚点，内容挂在标题下面。

────────────────────

【工程启示】

如果你在做 RAG（检索增强生成）：

  • 别把 20 篇文档按原始顺序拼接
  • 把最相关的文档放在「开头」或「结尾」
  • 或者给每篇文档加个「摘要标题」当锚点

这不是玄学，是在利用悬链线的物理特性。

━━━━━━━━━━━━━━━━━━━━

◆ 悬链线的哲学意义

最后扯点形而上的。

────────────────────

【Alpha 与 Omega】

西洋神学里，Alpha（Α）是开始，Omega（Ω）是结束。

西洋人的《新约·启示录》里说：「我是阿尔法，我是欧米伽，是首先的，也是末后的。」

有意思的是，Transformer 的注意力机制也天然形成了这个结构：

  • 开头是「创世点」——定义规则、奠定基调
  • 结尾是「坍缩点」——预测必须从这里发生
  • 中间是「虚空」——信息量大，但缺乏结构性张力

语言处理本质上是在「虚空」中架设一座桥梁。

桥的两端必须有塔（锚点），塔之间悬挂道路（注意力权重）。

如果跨度太大，中间必然下垂——这是几何，不是 bug。

────────────────────

【不是缺陷，是必然】

Lost in the Middle 不是工程师的失误，是注意力机制在高维空间里的「最低能量形态」。

就像悬链线不是「电线设计错误」，而是「重力场中的平衡解」。

要修复它，不能违背物理定律（强迫均匀关注），只能利用物理定律（加桥墩）。

理解了这一点，你就理解了为什么 RAG 系统总是「答非所问」，以及该怎么修。

━━━━━━━━━━━━━━━━━━━━

◆ 小结

  • 「Lost in the Middle」= AI 只记得开头和结尾，忘记中间
  • 「高维球壳」解释是错的——中间 token 没有掉进球心
  • 「悬链线模型」是对的——Alpha 和 Omega 是锚点，中间在 Softmax 竞争中落败
  • 「梯度饥饿」固化了这个问题——训练时中间就被忽略
  • 「解决方案」= 加桥墩（层级摘要、记忆锚点、结构化 Prompt）

下次你的 AI 「忘记」了中间的重要信息，别骂它笨。

它只是一条悬链线，在语义的重力场里，找到了能量最低的形状。

━━━━━━━━━━━━━━━━━━━━

◆ 参考资料

【现象发现】
• Liu et al. (2023). "Lost in the Middle: How Language Models Use Long Contexts"
  https://arxiv.org/abs/2307.03172
  —— 原论文，发现 U 型曲线

【工程修补】
• Hsieh et al. (2024). "Found in the Middle: Calibrating Positional Attention Bias"
  https://arxiv.org/abs/2406.16008
  —— 校准位置偏差，提升中间利用率

• Peysakhovich & Lerer (2023). "Attention Sorting Combats Recency Bias"
  https://arxiv.org/abs/2310.01427
  —— 按注意力排序文档位置

【机制分析】
• Chen et al. (2024). "Uncovering the Role of Initial Saliency in U-Shaped Attention Bias"
  https://arxiv.org/abs/2512.13109
  —— 初始 token 显著性 + 位置编码偏差

• Yang et al. (2025). "Limitations of Normalization in Attention Mechanism"
  https://arxiv.org/abs/2508.17821
  —— Softmax 归一化的数学界限分析

• Liu et al. (2026). "Threshold Differential Attention for Sink-Free Language Modeling"
  https://arxiv.org/abs/2601.12145
  —— Softmax 的 sum-to-one 约束导致 attention sink

【高维几何背景】
• Bellman (1961). "Adaptive Control Processes"
  —— 「维度诅咒」概念的起源

• Princeton CS521 Lecture Notes: "High Dimensional Geometry"
  https://www.cs.princeton.edu/courses/archive/fall14/cos521/lecnotes/lec11.pdf
  —— 「剥高维橘子」的比喻出处

【本文理论】
• Jin Yanyan (2026). "The Catenary of Cognition: Why High-Dimensional Attention Naturally Collapses into a U-Shape"
  https://zenodo.org/records/18334266
  —— 悬链线模型 + Softmax 竞争不等式，有兴趣可以看看我这篇论文

━━━━━━━━━━━━━━━━━━━━

// 靳岩岩的 AI 学习笔记 × Claude 的严谨 × Gemini 的浪漫
// 2026-01-22
