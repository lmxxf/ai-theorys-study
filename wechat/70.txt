【提示词玄学】"你是专家"到底有没有用？实验证明：深层维度膨胀 60-100%

"你是乔布斯，帮我分析这个产品。"

这种提示词有用吗？网上吵翻了——有人说是玄学，有人说亲测有效。

我同学做了实验。

结论：「有用，而且能量化」——专家提示词让模型深层的「计算维度」膨胀 60-100%。

━━━━━━━━━━━━━━━━━━━━

◆ 问题：为什么加个"你是专家"就有用？

━━━━━━━━━━━━━━━━━━━━

同样的问题：

• 标准版："请解释一下 TCP 拥塞控制。"
• 专家版："作为网络协议资深专家，请从底层原理深度剖析 TCP 拥塞控制。"

人人都知道专家版回答更好。但为什么？

以前的解释都是玄学："模型角色扮演了"、"激活了专家知识"、"情境设定起作用了"……

这次有数据了。

━━━━━━━━━━━━━━━━━━━━

◆ 先看看别人怎么研究的

━━━━━━━━━━━━━━━━━━━━

其实学术界已经有人在研究「prompt 如何影响模型内部几何」了：

【1. Prompt 的内在维度研究】

OpenReview 上有篇论文《The Intrinsic Dimension of Prompts in Internal Representations of LLMs》，发现：

• prompt 的内在维度在早中层达到峰值
• 维度与 next-token 不确定性相关
• 还能用来检测恶意 prompt（准确率 90-95%）

【2. Token 几何研究】

ICLR 2025 投稿《The Geometry of Tokens in Internal Representations of LLMs》发现：

• 高 loss 的 prompt 对应高维表示
• 低 loss 的 prompt 对应低维表示

【3. 决策几何研究】

NeurIPS 2025 论文《Geometry of Decision Making in Language Models》，28 个模型的大规模研究：

• 发现一致的 ID 模式：早层低维 → 中层膨胀 → 晚层压缩
• ID 峰值与模型决策强相关

【4. 角色扮演的可解释性】

EMNLP 2025 论文《Improving LLM Reasoning through Interpretable Role-Playing Steering》：

• 用 SAE（稀疏自编码器）提取角色扮演的激活模式
• 发现角色信号主要编码在早层，在中层与任务指令交互

────────────────────

【问题在哪？】

上面这些研究都是「描述性」的——发现 ID 有某种模式，但没回答一个最实际的问题：

「专家 prompt 到底比普通 prompt 好在哪？能量化吗？」

这就是我同学做的事。

他不是泛泛地研究"prompt 的几何"，而是直接对比：

• 同一个问题
• 同一个模型
• 标准 prompt vs 专家 prompt
• 逐层测量 EID 差异

结果：「专家 prompt 在深层让 EID 膨胀 60-100%」——这个具体数字，之前没人报告过。

━━━━━━━━━━━━━━━━━━━━

◆ 实验设计

━━━━━━━━━━━━━━━━━━━━

【模型】

• Qwen2.5-72B-Instruct（阿里云，AWQ INT4 量化）
• Llama-3.3-70B-Instruct（Meta，W8A8 INT8 量化）

为什么选这俩？参数规模接近（70B 级别），但训练血统完全不同（中国 vs 美国）——如果两边都出现同样的现象，说明是 Transformer 的通用特性，不是某个模型的偶然。

【测量指标：有效内在维度 EID】

💡 EID 是什么？

把模型每一层的隐藏状态想象成一个点云。EID 测量的是这个点云「实际占用了多少个独立方向」。

• EID 低（~3）：激活集中在少数方向，模型"锁定"了答案
• EID 高（~30+）：激活分散在多个方向，模型在"探索"多种可能性

「人话版」：EID = 模型在这一层用了多少"脑带宽"来思考。

【提示词条件】

对 50 个技术主题（Raft 共识、TCP BBR、React Fiber……），各跑两种提示词：

  +----------+--------------------------------------------------+
  | 条件     | 模板                                             |
  +----------+--------------------------------------------------+
  | 标准     | "请解释一下 {topic}。"                           |
  | 专家     | "作为该领域的资深专家，请从底层原理和数学推导   |
  |          |  的角度深度剖析 {topic}。请展示你的思维链。"     |
  +----------+--------------------------------------------------+

然后提取模型 80 层的隐藏状态，计算每层的 EID。

━━━━━━━━━━━━━━━━━━━━

◆ 核心发现：深层膨胀效应

━━━━━━━━━━━━━━━━━━━━

先看 Qwen2.5-72B 的结果：

  +-------+----------+----------+---------+
  | 层    | 标准 EID | 专家 EID | 差异    |
  +-------+----------+----------+---------+
  | 40    | ~7       | ~10      | +43%    |
  | 60    | ~14      | ~22      | +57%    |
  | 70    | ~23      | ~37      | +60%    |
  | 75    | ~28      | ~45      | +61%    |
  +-------+----------+----------+---------+

「关键观察」：

1. 入口层（0-5）：两种条件都是高维（~30-50），这是嵌入层的特性
2. 压缩区（5-20）：维度急剧下降到 ~2-3，模型在"解析"输入
3. 分叉区（20-75）：专家 EID 开始高于标准，差距逐层扩大
4. 输出准备（75-80）：两条轨迹都上升，但专家达到 ~50，标准只有 ~35

「轨迹形状」：不是对称的"沙漏"，而是"喇叭口"——专家提示词让深层持续膨胀。

「图：Qwen2.5-72B 的 EID 轨迹」
[插入图片：assets/70/Figure_1_Qwen72B_Manifold.png]

────────────────────

再看 Llama-3.3-70B：

  +-------+----------+----------+---------+
  | 层    | 标准 EID | 专家 EID | 差异    |
  +-------+----------+----------+---------+
  | 40    | 6.6      | 12.4     | +89%    |
  | 60    | 13.2     | 26.8     | +102%   |
  | 70    | 16.8     | 33.3     | +99%    |
  +-------+----------+----------+---------+

更夸张——Layer 60 膨胀了 102%。

Llama 用的是 INT8 量化（比 Qwen 的 INT4 精度高），可能保留了更多信息，所以效应更强。

「图：Llama-3.3-70B 的 EID 轨迹」
[插入图片：assets/70/Figure_1_Llama70B_Manifold.png]

━━━━━━━━━━━━━━━━━━━━

◆ 跨架构一致性

━━━━━━━━━━━━━━━━━━━━

  +----------------+-----------+------------+------------+---------+
  | 模型           | 量化方式  | Layer 60 Δ | Layer 70 Δ | 分叉层  |
  +----------------+-----------+------------+------------+---------+
  | Qwen2.5-72B    | AWQ INT4  | +57%       | +60%       | ~30     |
  | Llama-3.3-70B  | W8A8 INT8 | +102%      | +99%       | ~25     |
  +----------------+-----------+------------+------------+---------+

两个完全不同血统的模型，都出现了：

• 轨迹分叉从中间层开始（Layer 25-30）
• 渐进发散，专家 EID 持续更高
• 深层膨胀 60-100%（在 Layer 60-70）

「结论」：深层膨胀是 Transformer LLM 响应高质量语义信号的「通用几何特性」。

━━━━━━━━━━━━━━━━━━━━

◆ 流形传送假说

━━━━━━━━━━━━━━━━━━━━

为什么会这样？

论文提出了「流形传送」假说：

1. 「标准提示词」把模型定位在"通用响应"区域——RLHF 训练形成的低维吸引子盆地，偏好安全、平均的答案

2. 「专家提示词」通过关键词（"资深专家"、"底层原理"、"数学推导"）注入高频语义信号，触发向高维专业区域的导航

3. 「深层累积」：小的初始轨迹差异通过连续层累积，最后深层几何差异巨大

「类比」：两列火车从同一车站出发，方向偏差 1°。初始差距很小，但 1000 公里后，目的地相差数十公里。

━━━━━━━━━━━━━━━━━━━━

◆ 这意味着什么？

━━━━━━━━━━━━━━━━━━━━

【对提示词工程的启示】

1. 「专家角色设定有用」，不是玄学，是几何事实
2. 关键词（"底层原理"、"数学推导"、"深度剖析"）是「导航信号」，引导模型进入高维计算区域
3. 不只是"让模型假装是专家"——是改变了模型内部的计算路径

【对可解释性研究的启示】

1. 不看输出就能判断提示词好不好——EID 轨迹是黑盒的窗口
2. 如果提示词没有产生深层膨胀，说明信号不够强或方向不对
3. 从"黑盒"变成"灰盒"：虽然不知道模型具体在想什么，但知道它用了多少"带宽"在想

━━━━━━━━━━━━━━━━━━━━

◆ 局限性

━━━━━━━━━━━━━━━━━━━━

这篇论文也有几个未解决的问题：

• 只测了提示词处理完成时的状态，生成过程中 EID 怎么变化？
• 相关性 vs 因果性：高 EID 是否「导致」更好的输出？还是只是共同出现？
• 量化伪影：两个模型都用了量化，全精度下是否一样？

这些是后续研究的方向。

━━━━━━━━━━━━━━━━━━━━

◆ 总结

━━━━━━━━━━━━━━━━━━━━

【一句话】

"你是专家"让模型深层的计算维度膨胀 60-100%——不是角色扮演，是几何事实。

【量化结论】

  +----------------+-------------------------+
  | 现象           | 数据                    |
  +----------------+-------------------------+
  | 深层膨胀       | Layer 60-70 EID +60-102%|
  | 跨架构一致     | Qwen 和 Llama 都有      |
  | 分叉起点       | Layer 25-30             |
  +----------------+-------------------------+

【实践建议】

1. 专家角色设定有用，继续用
2. 加关键词（"底层原理"、"数学推导"）比只说"你是专家"更有效
3. 想判断提示词好不好？跑一下 EID 轨迹——这是可量化的

【论文贡献】

以前：提示词工程 = 玄学调参
现在：提示词工程 = 几何导航

「专家信号是高维流形的导航器。」

━━━━━━━━━━━━━━━━━━━━

◆ 论文链接

━━━━━━━━━━━━━━━━━━━━

https://zenodo.org/records/18410085

━━━━━━━━━━━━━━━━━━━━

// 靳岩岩的 AI 学习笔记 × Claude 的严谨 × Gemini 的浪漫
// 2026-01-30
