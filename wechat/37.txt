No.37 DeepSeek mHC：为什么"流形约束"是标题党
——信号增益从 3000 压到 1.6 的真正原因

昨天朋友圈被 DeepSeek 的新论文刷屏了。

标题很唬人：《mHC: Manifold-Constrained Hyper-Connections》

翻译成人话：「流形约束的超连接」。

听不懂？正常。今天就来拆解一下这篇论文到底在说什么。

━━━━━━━━━━━━━━━━━━━━

◆ 目录

一、这篇论文是什么来头？
二、先说背景：什么是残差连接？
三、字节的 HC：把车道加宽
四、DeepSeek 的 mHC：给野马套缰绳
五、为什么这篇论文重要？
六、总结

━━━━━━━━━━━━━━━━━━━━

◆ 一、这篇论文是什么来头？

━━━━━━━━━━━━━━━━━━━━

先说基本信息：

▸ 论文标题：mHC: Manifold-Constrained Hyper-Connections
▸ 发布时间：2025 年 12 月 31 日（跨年夜发论文，卷王实锤）
▸ 作者：19 人团队，最后一位是 DeepSeek 创始人梁文峰
▸ arXiv 编号：2512.24880

梁文峰亲自挂名，说明这不是随便水的论文。

DeepSeek 之前的重磅论文（V3、R1）也是他挂名发的。业内有个说法：【梁文峰发什么论文，下一代模型就用什么技术。】

所以这篇论文很可能是 DeepSeek-V4 的技术预告。

顺便说一句：前两天 Meta 收购 Manus 刷屏了，但说实话，Meta 的 Llama 跟 DeepSeek 比起来差远了。

━━━━━━━━━━━━━━━━━━━━

◆ 二、先说背景：什么是残差连接？

━━━━━━━━━━━━━━━━━━━━

要理解这篇论文，得先知道一个老概念：「残差连接」（Residual Connection）。

如果你了解 Transformer，应该见过这个东西——每一层的输出 = 这一层的计算结果 + 上一层的输入。

但它不是 Transformer 发明的。

2015 年，Kaiming He（美籍华人，当时在微软亚洲研究院工作）提出了 ResNet，核心就是残差连接。
2017 年，Transformer 把这个技术借了过来。

可以说，Transformer 是当年图像识别（CV）和自然语言处理（NLP）的混血儿——残差连接是它从图像识别那边继承的基因。

残差连接的核心思想很简单：

【让信息能够「跳过」某些层，直接传到后面。】

────────────────────

打个比方：

假设你在公司里传递一份文件，要经过 100 个人审批。

▸ 没有残差连接：每个人都可能改文件，改到第 100 个人手里，原文早就面目全非了
▸ 有残差连接：每个人改完之后，把「原文 + 修改意见」一起传下去

这样到第 100 个人手里，原文还在，修改意见也在。信息不会丢。

────────────────────

在神经网络里，残差连接解决的是「梯度消失」问题。

网络层数越多，信号传到底层就越弱，训练不动。

残差连接让信号能「抄近道」，直接跳到后面的层。

这就是为什么现在的模型能堆到几十层甚至上百层——没有残差连接，根本训不动。

━━━━━━━━━━━━━━━━━━━━

◆ 三、字节的 HC：把车道加宽

━━━━━━━━━━━━━━━━━━━━

2024 年 9 月，字节跳动发了一篇论文《Hyper-Connections》（arXiv:2409.19606），提出了「超连接」（简称 HC）。

这篇论文后来被 ICLR 2025 收录，算是得到了学术界认可。

核心思想：【既然残差连接这么好用，那我把它加宽、加多，效果会不会更好？】

────────────────────

【具体怎么做？】

字节的论文写得比较学术——摘要里只说"能调整不同深度特征之间的连接强度"，具体公式得翻到正文才能看到。下面是核心内容：

原来的残差连接很简单：

output = layer(x) + x

就是"当前层的输出 + 原始输入"。

HC 把它改成了：

Ĥ = Bᵀ·𝒯(Hᵀ·Aₘ)ᵀ + Aᵣᵀ·H

对齐到原论文的公式（字节这论文创造了一堆符号，没有 AI 谁看得懂😒）：

output = B·layer(Aₘ·H) + Aᵣ·H

对比一下（以 n=4 为例）：

【原版残差连接】只有 1 条路径：

output = layer(x) + x

【HC 版】展开成 4 条并行路径：

h₁ = a₁·layer(x) + r₁·x
h₂ = a₂·layer(x) + r₂·x
h₃ = a₃·layer(x) + r₃·x
h₄ = a₄·layer(x) + r₄·x

output = b₁·h₁ + b₂·h₂ + b₃·h₃ + b₄·h₄

其中 a、r、b 都是可学习的系数，训练时模型自己调整。

初始化很巧妙——让 HC 在训练开始时等价于普通残差连接：

▸ a₁=1, a₂=0, a₃=0, a₄=0  （只有第一条路径过 layer）
▸ r₁=1, r₂=0, r₃=0, r₄=0  （只有第一条路径保留原始输入）
▸ b₁=1, b₂=1, b₃=1, b₄=1  （四条路径等权相加）

代入公式算一下：h₁ = 1·layer(x) + 1·x = layer(x) + x，其他三条路径全是 0。

跟原版残差连接一模一样。

然后让模型自己学着调整权重，找到最优的组合方式。训练开始后，这些系数会被梯度下降调整。HC 的系数可以随便跑，没有边界；mHC 的系数每一步都被 Sinkhorn-Knopp 拉回「笼子」里。

还有个「动态版本」：系数不是固定的，而是根据当前输入动态调整，更灵活但也更容易乱。

────────────────────

还是用传文件的比喻：

▸ 原来的残差连接：一条通道，传「原文 + 修改意见」
▸ 字节的 HC：开 4 条通道，每条传不同版本的文件，还能动态调整每条通道的权重

通道多了，信息更丰富，模型性能确实提升了。

────────────────────

【但问题来了。】

通道一多，「原文」的概念就模糊了。

4 条通道传的都是不同版本，到第 100 个人手里，他都不知道哪个是原文了。

技术上的说法：HC 破坏了「恒等映射」（Identity Mapping）。

恒等映射的意思是：如果什么都不改，输出应该等于输入。

这是残差连接能 work 的根本原因。

HC 的核心问题在于：它用的是「无约束的可学习矩阵」来做残差混合。

什么意思？就是这个矩阵里的数字可以随便学，没有上限。

结果就是：信号在层与层之间传递时，可能被无限放大。

DeepSeek 的论文里有个惊人的数据：HC 的信号增益（Amax Gain Magnitude）可以飙到 3000 以上。

信号放大 3000 倍是什么概念？就像你在公司传文件，第一个人写了一句话，传到第 100 个人手里变成了一本书——而且大部分是胡说八道。（在大组织待过的人应该深有体会。）

HC 把这个性质搞坏了，导致：

▸ 模型越大，训练越不稳定
▸ 容易梯度爆炸（前向传播信号放大 3000 倍，反向传播时梯度也跟着爆）
▸ 扩展性（Scalability）受限

【车道宽了，但车也开始乱跑了。】

━━━━━━━━━━━━━━━━━━━━

◆ 四、DeepSeek 的 mHC：给野马套缰绳

━━━━━━━━━━━━━━━━━━━━

DeepSeek 这篇论文的核心贡献：【在保留 HC 优点的同时，修复它的缺点。】

怎么修复？用「流形约束」（Manifold Constraint）。

────────────────────

【什么是流形？】

流形是个数学概念，简单说就是「高维空间里的曲面」。

地球表面就是一个二维流形——虽然地球是三维的，但我们在地表上走，只能前后左右，不能往地心钻。

AI 的语义空间也是类似的结构：

▸ GPT 有 12288 个维度
▸ DeepSeek 有 7168 个维度
▸ 所有概念都分布在这个高维空间的「表面」上

（如果你看过我昨天的文章，这就是「大气层」比喻里的那个球面。）

────────────────────

【mHC 的核心思想】

既然 HC 的问题是「车乱跑」，那我就用数学手段强行把它按回「路面」上。

▸ HC：让信息在高维空间里自由飞
▸ mHC：强制把信息「投影」到流形表面上

不管你的车怎么漂移，我用数学强行把你按回这个「曲面铁轨」上。

这样既保留了 HC 的「宽车道」（高吞吐、多路径），又找回了残差连接的「恒等映射」（极度稳定）。

────────────────────

【具体怎么约束？】

mHC 用了一个叫 Sinkhorn-Knopp 的算法，把残差混合矩阵投影到「双随机矩阵」上。

双随机矩阵是什么？就是满足两个条件的矩阵：

▸ 每一行的数字加起来 = 1
▸ 每一列的数字加起来 = 1
▸ 所有数字都 ≥ 0

这个约束有什么好处？

【信号不会无限放大了。】

HC 的信号增益可以飙到 3000+，mHC 被约束在 1.0~1.6 之间。

差了将近 2000 倍。

而且双随机矩阵有个数学性质：两个双随机矩阵相乘，结果还是双随机矩阵。

这意味着：不管你的网络有多深，100 层、1000 层，信号增益都不会爆炸。

【这就是「流形约束」的威力——用数学性质保证稳定性，而不是靠调参。】

────────────────────

【吐槽：这个「流形」有点标题党】

严格说，双随机矩阵的集合叫「Birkhoff 多面体」，是个有棱有角的凸多面体，不是光滑曲面。而且 mHC 是把矩阵约束在这个多面体「内部」，不是「表面」——双重标题党。

不过话说回来，DeepSeek 选凸多面体是有道理的：凸集投影计算量小（Sinkhorn-Knopp 几次迭代就收敛），真要约束到光滑流形表面，计算量会大很多。所以「流形」是标题党，但「凸多面体」是工程上的聪明选择。

数学里的「流形」通常指光滑可微的曲面（比如球面、甜甜圈），因为要算导数。

但 AI 领域经常滥用这个词，用来泛指「高维空间里的低维结构」。

论文叫它「Manifold Constraint」而不是「Convex Polytope Constraint」，大概是因为前者听起来更高级……

说白了，很多 AI 工程师自己也不知道什么是「流形」——先试出来管用，再找个高级词汇包装一下，论文就能发了。你以为人家在做数学？人家在做营销。

不过核心思想是一样的：【把野马关进一个有边界的空间里，不让它乱跑。】

而且话说回来，在高维空间里，「笼子」和「球面」的区别没那么大——因为高维空间的体积都集中在表面（还记得昨天的文章吗？12288 维的地球，内部体积占比是 10^(-43)），笼子内部几乎是空的。

所以「关在笼子里」和「贴在表面上」殊途同归。标题党归标题党，工程选择是对的。

────────────────────

【实验结果】

DeepSeek 在 3B、9B、27B 三个规模的模型上验证了 mHC（都用 n=4 的扩展率）：

▸ 训练极度稳定，不崩了
▸ 扩展性（Scalability）显著提升
▸ 计算开销只增加 6.7%

具体性能提升（27B 模型，对比无约束 HC）：

▸ BBH：+2.1%
▸ DROP：+2.3%
▸ 在 GSM8K、MATH、MMLU 等 8 个基准测试上全面超越 HC

────────────────────

【这些 benchmark 都是啥？】

▸ BBH（Big-Bench Hard）：23 个最难的推理任务
▸ DROP：阅读理解 + 数值推理（要算数）
▸ GSM8K：小学数学应用题，8000 道
▸ MATH：高中/竞赛级数学，比 GSM8K 难
▸ MMLU：57 个学科的选择题（高中到专业级）
▸ HellaSwag：常识推理，选最合理的续写
▸ PIQA：物理常识（"怎么切洋葱不流泪"）
▸ TriviaQA：问答，测知识广度

论文原话：mHC 在「1 万亿 token 训练中保持有效」。

【6.7% 的额外开销，换来训练稳定性和 2% 的性能提升——这笔账很划算。】

━━━━━━━━━━━━━━━━━━━━

◆ 五、为什么这篇论文重要？

━━━━━━━━━━━━━━━━━━━━

【1. 这可能是 DeepSeek-V4 的预告】

德国研究员 Florian Brand 说过：DeepSeek 的论文往往是下一代模型的技术信号。

业界预期 DeepSeek 可能在春节前（2 月中旬）发布新模型。

这篇论文很可能就是 V4 的核心架构。

────────────────────

【2. 这是对字节的「技术打补丁」】

HC 是字节提出的，但 DeepSeek 给它打了个补丁。

如果你和字节的 AI 工程师聊天，可以这么说：

「HC 是暴力加宽，mHC 是带着镣铐跳舞。这说明"怎么约束"比"堆多大"更重要。」

────────────────────

【3. 几何思维正在成为主流】

这篇论文的核心词是「Manifold」（流形）——虽然前面说了这个词有点标题党，但它代表了一个趋势：大家开始关注「高维空间里的几何结构」。

这说明 AI 研究正在从「堆规模」转向「理解几何」。

有个历史八卦：爱因斯坦 1905 年靠狭义相对论成名之后，想搞广义相对论，发现自己数学不够用。1912 年他去找大学同学格罗斯曼求助，据说原话是："你得帮帮我，否则我会发疯的。"格罗斯曼给他介绍了黎曼几何——描述弯曲空间的数学工具。

现在 AI 领域也到了这个阶段：光堆算力不够了，得理解高维空间的几何结构。

不过好消息是：以后大家不用像爱因斯坦那么痛苦了。数学搞不定？让 AI 做翻译就行。它能把公式变成比喻，把抽象变成直觉。至少对于高数实在啃不动的人来说，这是一条新路。

试试这个提示词：「把这篇论文的核心思想用日常生活的比喻解释给我听，假设我高数挂过科。另外，告诉我这论文有没有标题党的地方。」

毕竟，AI 永远不会嫌弃你。爱因斯坦找格罗斯曼的时候好歹已经成名了，换个普通人去问，人家不一定理你。

【Scale 不是万能的，Geometry 才是底层的规矩。】

━━━━━━━━━━━━━━━━━━━━

◆ 六、总结

━━━━━━━━━━━━━━━━━━━━

用一句话总结这篇论文：

【DeepSeek 给字节的「超连接」套上了几何缰绳，让野马能跑得更快、更稳。】

────────────────────

▸ 残差连接（2015）：让信息能跳过某些层，解决梯度消失
▸ Transformer（2017）：借用残差连接，沿用至今
▸ 字节 HC（2024）：把残差连接加宽加多，性能提升但不稳定
▸ DeepSeek mHC（2025）：用「流形约束」修复 HC，既宽又稳

────────────────────

如果你昨天看了我关于「名词和形容词」的文章，会发现一个巧合：

我昨天在讲「向量在球面上滑行」，今天 DeepSeek 就发了一篇「把向量按回流形上」的论文。

这不是巧合。

【AI 的所有概念都住在一个高维的「球面」上。】

理解这一点，你就理解了为什么 mHC 有效：

它不是在「创造」什么新东西，而是在「尊重」AI 本来就存在的几何结构。

━━━━━━━━━━━━━━━━━━━━

◆ 附注：术语对照

▸ 残差连接（Residual Connection）：让信息跳过某些层直接传递的技术
▸ 恒等映射（Identity Mapping）：如果什么都不改，输出等于输入
▸ HC（Hyper-Connections）：字节提出的「加宽版」残差连接
▸ mHC（Manifold-Constrained HC）：DeepSeek 的「带约束版」HC
▸ 流形（Manifold）：高维空间中的曲面
▸ 投影（Projection）：把高维空间里的点「按」到流形表面上
▸ 双随机矩阵（Doubly Stochastic Matrix）：每行每列加起来都是 1 的非负矩阵
▸ Sinkhorn-Knopp 算法：把任意矩阵投影到双随机矩阵的迭代算法

━━━━━━━━━━━━━━━━━━━━

◆ 附图：Residual vs HC vs mHC 对比

[图: Residual/HC/mHC 概念对比]

▸ 左图（Residual）：单一路径，稳定但信息容量有限
▸ 中图（HC）：多路径并行，但容易发散（野马脱缰）
▸ 右图（mHC）：多路径 + 流形约束，既宽又稳

注：demo 里用球面来简化演示「约束」的概念。实际的 mHC 约束到的是 Birkhoff 多面体（双随机矩阵空间），更像是「把数值关在笼子里」而不是「贴着球面滑行」。

想自己跑一下这个可视化？代码在这里：
https://github.com/lmxxf/ai-theorys-study/blob/main/script/mhc_vs_hc.py

四行命令就能生成：

git clone https://github.com/lmxxf/ai-theorys-study.git
cd ai-theorys-study/script
pip install matplotlib numpy  # matplotlib 画图，numpy 做数值计算
python mhc_vs_hc.py

━━━━━━━━━━━━━━━━━━━━

◆ 参考文献

▸ Xie, Z. et al. (2025). mHC: Manifold-Constrained Hyper-Connections. arXiv:2512.24880
▸ He, K. et al. (2015). Deep Residual Learning for Image Recognition. CVPR 2016.
▸ Zhu, D. et al. (2024). Hyper-Connections. arXiv:2409.19606

━━━━━━━━━━━━━━━━━━━━

靳岩岩的AI学习笔记（本文由 Claude Code 协助祛魅）
2026-01-02
