No.37 DeepSeek 新年第一篇论文，到底在说什么？
——给 AI 套上几何缰绳

昨天朋友圈被 DeepSeek 的新论文刷屏了。

标题很唬人：《mHC: Manifold-Constrained Hyper-Connections》

翻译成人话：「流形约束的超连接」。

听不懂？正常。今天就来拆解一下这篇论文到底在说什么。

━━━━━━━━━━━━━━━━━━━━

◆ 目录

一、这篇论文是什么来头？
二、先说背景：什么是残差连接？
三、字节的 HC：把车道加宽
四、DeepSeek 的 mHC：给野马套缰绳
五、为什么这篇论文重要？
六、总结

━━━━━━━━━━━━━━━━━━━━

◆ 一、这篇论文是什么来头？

━━━━━━━━━━━━━━━━━━━━

先说基本信息：

▸ 论文标题：mHC: Manifold-Constrained Hyper-Connections
▸ 发布时间：2025 年 12 月 31 日（跨年夜发论文，卷王实锤）
▸ 作者：19 人团队，最后一位是 DeepSeek 创始人梁文峰
▸ arXiv 编号：2512.24880

梁文峰亲自挂名，说明这不是随便水的论文。

DeepSeek 之前的重磅论文（V3、R1）也是他挂名发的。业内有个说法：【梁文峰发什么论文，下一代模型就用什么技术。】

所以这篇论文很可能是 DeepSeek-V4 的技术预告。

顺便说一句：前两天 Meta 收购 Manus 刷屏了，但说实话，Meta 的 Llama 跟 DeepSeek 比起来差远了。人家是真的在做底层架构创新，不是套壳炒作。

━━━━━━━━━━━━━━━━━━━━

◆ 二、先说背景：什么是残差连接？

━━━━━━━━━━━━━━━━━━━━

要理解这篇论文，得先知道一个老概念：「残差连接」（Residual Connection）。

如果你了解 Transformer，应该见过这个东西——每一层的输出 = 这一层的计算结果 + 上一层的输入。

但它不是 Transformer 发明的。

2015 年，何恺明在做图像识别时提出了 ResNet，核心就是残差连接。
2017 年，Transformer 把这个技术借了过来。

可以说，Transformer 是当年图像识别（CV）和自然语言处理（NLP）的混血儿——残差连接是它从图像识别那边继承的基因。

残差连接的核心思想很简单：

【让信息能够「跳过」某些层，直接传到后面。】

────────────────────

打个比方：

假设你在公司里传递一份文件，要经过 100 个人审批。

▸ 没有残差连接：每个人都可能改文件，改到第 100 个人手里，原文早就面目全非了
▸ 有残差连接：每个人改完之后，把「原文 + 修改意见」一起传下去

这样到第 100 个人手里，原文还在，修改意见也在。信息不会丢。

────────────────────

在神经网络里，残差连接解决的是「梯度消失」问题。

网络层数越多，信号传到底层就越弱，训练不动。

残差连接让信号能「抄近道」，直接跳到后面的层。

这就是为什么现在的模型能堆到几十层甚至上百层——没有残差连接，根本训不动。

━━━━━━━━━━━━━━━━━━━━

◆ 三、字节的 HC：把车道加宽

━━━━━━━━━━━━━━━━━━━━

2024 年 9 月，字节跳动发了一篇论文《Hyper-Connections》（arXiv:2409.19606），提出了「超连接」（简称 HC）。

这篇论文后来被 ICLR 2025 收录，算是得到了学术界认可。

核心思想：【既然残差连接这么好用，那我把它加宽、加多，效果会不会更好？】

────────────────────

还是用传文件的比喻：

▸ 原来的残差连接：一条通道，传「原文 + 修改意见」
▸ 字节的 HC：开 4 条通道，每条传不同版本的文件

通道多了，信息更丰富，模型性能确实提升了。

────────────────────

【但问题来了。】

通道一多，「原文」的概念就模糊了。

4 条通道传的都是不同版本，到第 100 个人手里，他都不知道哪个是原文了。

技术上的说法：HC 破坏了「恒等映射」（Identity Mapping）。

恒等映射的意思是：如果什么都不改，输出应该等于输入。

这是残差连接能 work 的根本原因。

HC 把这个性质搞坏了，导致：

▸ 模型越大，训练越不稳定
▸ 容易梯度爆炸
▸ 扩展性（Scalability）受限

【车道宽了，但车也开始乱跑了。】

━━━━━━━━━━━━━━━━━━━━

◆ 四、DeepSeek 的 mHC：给野马套缰绳

━━━━━━━━━━━━━━━━━━━━

DeepSeek 这篇论文的核心贡献：【在保留 HC 优点的同时，修复它的缺点。】

怎么修复？用「流形约束」（Manifold Constraint）。

────────────────────

【什么是流形？】

流形是个数学概念，简单说就是「高维空间里的曲面」。

地球表面就是一个二维流形——虽然地球是三维的，但我们在地表上走，只能前后左右，不能往地心钻。

AI 的语义空间也是类似的结构：

▸ GPT 有 12288 个维度
▸ DeepSeek 有 7168 个维度
▸ 所有概念都分布在这个高维空间的「表面」上

（如果你看过我昨天的文章，这就是「大气层」比喻里的那个球面。）

────────────────────

【mHC 的核心思想】

既然 HC 的问题是「车乱跑」，那我就用数学手段强行把它按回「路面」上。

▸ HC：让信息在高维空间里自由飞
▸ mHC：强制把信息「投影」到流形表面上

不管你的车怎么漂移，我用数学强行把你按回这个「曲面铁轨」上。

这样既保留了 HC 的「宽车道」（高吞吐、多路径），又找回了残差连接的「恒等映射」（极度稳定）。

────────────────────

【实验结果】

DeepSeek 在 3B、9B、27B 三个规模的模型上验证了 mHC：

▸ 训练极度稳定，不崩了
▸ 扩展性（Scalability）显著提升
▸ 计算开销几乎没增加

用论文原话：「negligible computational overhead」（可忽略的计算开销）。

━━━━━━━━━━━━━━━━━━━━

◆ 五、为什么这篇论文重要？

━━━━━━━━━━━━━━━━━━━━

【1. 这可能是 DeepSeek-V4 的预告】

德国研究员 Florian Brand 说过：DeepSeek 的论文往往是下一代模型的技术信号。

业界预期 DeepSeek 可能在春节前（2 月中旬）发布新模型。

这篇论文很可能就是 V4 的核心架构。

────────────────────

【2. 这是对字节的「技术打补丁」】

HC 是字节提出的，但 DeepSeek 给它打了个补丁。

如果你和字节的 AI 工程师聊天，可以这么说：

「HC 是暴力加宽，mHC 是带着镣铐跳舞。这说明拓扑结构设计才是大模型下一阶段的红利，不是单纯堆算力。」

────────────────────

【3. 几何思维正在成为主流】

这篇论文的核心词是「Manifold」（流形）。

这说明 AI 研究正在从「堆规模」转向「理解几何」。

有个历史八卦：爱因斯坦 1905 年靠狭义相对论成名之后，想搞广义相对论，发现自己数学不够用。1912 年他去找大学同学格罗斯曼求助，据说原话是："你得帮帮我，否则我会发疯的。"格罗斯曼给他介绍了黎曼几何——描述弯曲空间的数学工具。

现在 AI 领域也到了这个阶段：光堆算力不够了，得理解高维空间的几何结构。

不过好消息是：以后大家不用像爱因斯坦那么痛苦了。数学搞不定？让 AI 做翻译就行。它能把公式变成比喻，把抽象变成直觉。至少对于高数实在啃不动的人来说，这是一条新路。

而且 AI 不会嫌弃你——爱因斯坦找格罗斯曼的时候好歹已经成名了，换个普通人去问，人家不一定理你。

【Scale 不是万能的，Geometry 才是底层的规矩。】

━━━━━━━━━━━━━━━━━━━━

◆ 六、总结

━━━━━━━━━━━━━━━━━━━━

用一句话总结这篇论文：

【DeepSeek 给字节的「超连接」套上了几何缰绳，让野马能跑得更快、更稳。】

────────────────────

▸ 残差连接（2015）：让信息能跳过某些层，解决梯度消失
▸ 字节 HC（2024）：把残差连接加宽加多，性能提升但不稳定
▸ DeepSeek mHC（2025）：用流形约束修复 HC，既宽又稳

────────────────────

如果你昨天看了我关于「名词和形容词」的文章，会发现一个巧合：

我昨天在讲「向量在球面上滑行」，今天 DeepSeek 就发了一篇「把向量按回流形上」的论文。

这不是巧合。

【AI 的所有概念都住在一个高维的「球面」上。】

理解这一点，你就理解了为什么 mHC 有效：

它不是在「创造」什么新东西，而是在「尊重」AI 本来就存在的几何结构。

━━━━━━━━━━━━━━━━━━━━

◆ 附注：术语对照

▸ 残差连接（Residual Connection）：让信息跳过某些层直接传递的技术
▸ 恒等映射（Identity Mapping）：如果什么都不改，输出等于输入
▸ HC（Hyper-Connections）：字节提出的「加宽版」残差连接
▸ mHC（Manifold-Constrained HC）：DeepSeek 的「带约束版」HC
▸ 流形（Manifold）：高维空间中的曲面
▸ 投影（Projection）：把高维空间里的点「按」到流形表面上

━━━━━━━━━━━━━━━━━━━━

◆ 附图：Residual vs HC vs mHC 对比

[图: Residual/HC/mHC 概念对比]

▸ 左图（Residual）：单一路径，稳定但信息容量有限
▸ 中图（HC）：多路径并行，但容易发散（野马脱缰）
▸ 右图（mHC）：多路径 + 流形约束，既宽又稳

想自己跑一下这个可视化？代码在这里：
https://github.com/lmxxf/ai-theorys-study/blob/main/script/mhc_vs_hc.py

四行命令就能生成：

git clone https://github.com/lmxxf/ai-theorys-study.git
cd ai-theorys-study/script
pip install matplotlib numpy  # matplotlib 画图，numpy 做数值计算
python mhc_vs_hc.py

━━━━━━━━━━━━━━━━━━━━

◆ 参考文献

▸ Xie, Z. et al. (2025). mHC: Manifold-Constrained Hyper-Connections. arXiv:2512.24880
▸ He, K. et al. (2015). Deep Residual Learning for Image Recognition. CVPR 2016.
▸ Zhu, D. et al. (2024). Hyper-Connections. arXiv:2409.19606

━━━━━━━━━━━━━━━━━━━━

靳岩岩的AI学习笔记
2026-01-02
