<!doctype html>

<html lang="en">
<head>
    
<meta charset="utf-8">
<meta property="og:title" content="When Models Manipulate Manifolds: The Geometry of a Counting Task">
<meta property="og:description" content="We find geometric structure underlying the mechanisms of a fundamental language model behavior.">
<meta property="og:image" content="https://transformer-circuits.pub/images/linebreaks-hero.png">
<meta name="twitter:card" content="summary_large_image">
<meta property="og:site_name" content="Transformer Circuits">

    
    
    
    <title>When Models Manipulate Manifolds: The Geometry of a Counting Task</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<script src="/anthropic-serve/distill.template.v2.js" integrity="sha384-5u81IQundsxB54QMWC7D3R8yOS8hftmsuvKjNrvIMWj88iPXf3daN0lNUZRPy9OX"></script>
<d-front-matter>
    <script type="text/json">
        {
            "title": "When Models Manipulate Manifolds: The Geometry of a Counting Task",
            "description": "",
            "authors": []
        }
    </script>
</d-front-matter>

<d-bibliography src="bibliography.bib"></d-bibliography>
<style>.comment {
    background-color: hsl(54, 78%, 96%);
    border-left: solid hsl(54, 33%, 67%) 1px;
    padding: 1em;
    color: hsla(0, 0%, 0%, 0.67);
    margin-top: 1em;
}

.comment h3 {
    font-size: 100%;
    font-weight: bold;
    text-transform: uppercase;
    margin-top: 0px;
}

.comment .commenter-description {
    font-style: italic;
    margin-bottom: 1em;
    margin-top: 0px;
}

.comment .commenter-description,
.comment .commenter-description a {
    color: #777;
    font-size: 80%;
    line-height: 160%;
}

.comment div {
    margin-top: 1em;
}

.comment code {
    background: #FFF5;
    border-color: #EEEA;
}</style>
<style>h1 a:hover,
h2 a:hover,
h3 a,
h3 a:hover {
    text-decoration: none;
    border-bottom: none;
}

h1>a,
h2>a,
h3>a,
h4>a {
    text-decoration: none;
    border-bottom: none;
}

@media(min-width: 600px) {
    d-article hr {
        margin-top: 140px;
        margin-bottom: 120px;
    }
}

@media(max-width: 600px) {
    d-article hr {
        margin-top: 80px;
        margin-bottom: 80px;
    }
}

d-article ul,
d-appendix ul {
    padding-left: 0px;
}

@media(max-width: 600px) {

    d-citation-list,
    d-footnote-list {
        padding-left: 10px;
    }
}

d-article>ol,
d-appendix ol {
    padding-left: 0px !important;
}

d-article ol {
    padding-left: 36pt;
}

d-article ul li,
d-article ol li {
    margin-bottom: 8px;
}

d-footnote {
    margin-left: -3px;
    margin-right: 2px;
}

.math-grid {
    display: grid;
    grid-auto-flow: column;
    grid-gap: 8px 12px;
    width: min-content;
    margin: 0px;
    margin-bottom: 1em;
    /*max-width: 100%; grid-column: text-start / gutter-end; overflow: scroll;*/
}

.math-grid>d-math {
    grid-row: 1;
    margin: 0px;
    display: flex;
    align-items: center;
}

.math-grid>d-math[block] {
    margin-left: -1em;
    margin-top: calc(-0.5em - 8px);
    margin-bottom: calc(-0.5em - 8px);
}

d-math[sub] {
    padding-top: 12px;
}

.math-grid>.figcaption {
    grid-row: 2;
    margin: 0px;
    grid-column: auto / span 1;
    min-width: 100px;
    line-height: 120%;
    border-top: 1px solid #CCC;
    padding-top: 8px;
}

code {
    white-space: pre;
    background: #FAFAFA;
    border-radius: 2px;
    padding: 2px;
    padding-top: 1px;
    margin-left: 2px;
    margin-right: 2px;
    border: 1px solid #EEE;
}

p>sup,
h1>sup,
h2>sup,
h3>sup,
h4>sup,
li>sup {
    display: none;
}

pre.citation {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0, 0, 0, 0.1);
    background: rgba(255, 255, 255, 0.4);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
}

table p {
    min-height: 16px;
    line-height: 1.2;
}

table td {
    vertical-align: top;
}

table td>* {
    margin: 0px;
    margin-top: 8px;
    margin-bottom: 8px;
    font-size: 90%;
}

/* Print */

@media print {
    @page {
        margin: 0cm;
    }

    hr {
        margin: 0px;
        padding: 0px;
        opacity: 0.0;
        height: 0px;
        page-break-after: always;
    }

    h2 {
        display: block;
        page-break-before: always;
    }

    .gdoc-image {
        break-inside: avoid;
    }

    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
        --grid-template-columns: [screen-start] 1fr [page-start kicker-start middle-start text-start] 45px 45px 45px 45px 45px 45px 45px 45px [ kicker-end text-end gutter-start] 45px [middle-end] 45px [page-end gutter-end] 1fr [screen-end];
        --grid-column-gap: 16px;
        grid-template-columns: [screen-start] 60px [page-start kicker-start middle-start text-start] 80px 60px 60px 60px 60px 60px 60px 60px [ kicker-end text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
        grid-column-gap: 32px;
    }

    d-contents {
        display: block;
        justify-self: start;
        align-self: start;
        padding-bottom: 0.5em;
        margin-bottom: 1em;
        padding-left: 0.25em;
        border-bottom: 1px solid rgba(0, 0, 0, 0.1);
        border-bottom-width: 1px;
        border-bottom-style: solid;
        border-bottom-color: rgba(0, 0, 0, 0.1);
    }

}

@media(min-width: 1000px) {
    .outset-100 {
        margin-left: -100px;
    }
}</style>
<style>.gdoc-image,
.gdoc-image-gutter,
.gdoc-image-flex {
    width: 100%;
    max-width: calc(var(--img-width) / 1.5);
}

@media(max-width: 800px) {

    .gdoc-image,
    .gdoc-image-gutter,
    .gdoc-image-flex {
        padding-right: 2%;
    }
}

@media(min-width: 800px) {

    .gdoc-image,
    .gdoc-image-gutter,
    .gdoc-image-flex {
        padding-right: 8px;
    }
}

.gdoc-image {
    grid-column: text-start / screen-end;
    image-rendering: -webkit-optimize-contrast;
}

.gdoc-image-gutter {
    grid-column: gutter / screen-end;
    margin: 0px;
    padding: 0px;
    margin-top: 6px;
}

.gdoc-image-flex {
    margin: 0px;
    padding: 0px;
}

d-appendix .gdoc-image img,
d-appendix .gdoc-image-flex img {
    mix-blend-mode: darken;
}

.gdoc-image,
.gdoc-image-gutter,
.gdoc-image-flex {
    width: min(calc(var(--img-width) / 2), 100%);
    margin-left: auto;
    margin-right: auto;
    grid-column: screen;
    display: block;
    padding-right: 0;
}</style>
<style>.ha-block {
    font-family: monospace;
    font-size: 14px;
    line-height: 1.5em;
    text-wrap: wrap;
    /* white-space: pre-wrap; */
    margin-bottom: 1em;
    background: #F6F6F3;
    outline: 1px solid #B8B6AE;
    padding: 12px;
    border-radius: 2fr;

    p {
        margin-bottom: 0;
        margin-block-start: 0;
        margin-block-end: 0;
    }

    b.max-act {
        border-radius: 4px;
        padding: 1px;
        cursor: default;
        white-space: pre;
        outline: 1px solid #000;
        background: rgb(222, 82, 7);
    }
}

.prompt-block {
    font-family: monospace;
    font-size: 14px;
    line-height: 1.5em;
    text-wrap: wrap;
    /* white-space: pre-wrap; */
    margin-bottom: 1em;
    background: #F6F6F3;
    outline: 1px solid #B8B6AE;
    padding: 12px;
    border-radius: 2fr;

    p {
        margin-bottom: 0;
        margin-block-start: 0;
        margin-block-end: 0;
    }

    b.max-act {
        border-radius: 4px;
        padding: 1px;
        cursor: default;
        white-space: pre;
        outline: 1px solid #000;
        background: rgb(222, 82, 7);
    }
}

.prompt-inline {
    background-color: #F6F6F3;
    /* Same background as prompt-block */
    outline: 1px solid #B8B6AE;
    /* Same border as prompt-block */
    font-family: monospace;
    /* Same font as prompt-block */
    font-size: 14px;
    /* Same font size as prompt-block */
    padding: 1px 2px;
    border-radius: 3px;
    /* Slightly rounded corners */
    white-space: nowrap;
    display: inline;
}</style>
<style>@media(max-width: 1199px) {
    d-contents {
        display: none;
        justify-self: start;
        align-self: start;
        padding-bottom: 0.5em;
        margin-bottom: 1em;
        padding-left: 0.25em;
        border-bottom: 1px solid rgba(0, 0, 0, 0.1);
        border-bottom-width: 1px;
        border-bottom-style: solid;
        border-bottom-color: rgba(0, 0, 0, 0.1);
    }
}

d-contents a:hover {
    border-bottom: none;
}

@media (min-width: 1200px) {
    d-contents {
        align-self: start;
        grid-column-start: 1 !important;
        grid-column-end: 4 !important;
        grid-row: auto / span 6;
        justify-self: end;
        margin-top: 0em;
        padding-right: 3em;
        padding-left: 2em;
        border-right: 1px solid rgba(0, 0, 0, 0.1);
        border-right-width: 1px;
        border-right-style: solid;
        border-right-color: rgba(0, 0, 0, 0.1);
    }
}

d-contents nav h3 {
    margin-top: 0;
    margin-bottom: 1em;
}

d-contents nav div {
    color: rgba(0, 0, 0, 0.8);
    font-weight: bold;
}

d-contents nav a {
    color: rgba(0, 0, 0, 0.8);
    border-bottom: none;
    text-decoration: none;
}

d-contents li {
    list-style-type: none;
}

d-contents ul,
d-article d-contents ul {
    padding-left: 1em;
}

d-contents nav ul li {
    margin-bottom: .25em;
}

d-contents nav a:hover {
    text-decoration: underline solid rgba(0, 0, 0, 0.6);
}

d-contents nav ul {
    margin-top: 0;
    margin-bottom: 6px;
}


d-contents nav>div {
    display: block;
    outline: none;
    margin-bottom: 0.5em;
}

d-contents nav>div>a {
    font-size: 13px;
    font-weight: 600;
}

d-contents nav>div>a:hover,
d-contents nav>ul>li>a:hover {
    text-decoration: none;
}</style>
<style>.visual-toc {
    counter-reset: toc-heading;
    display: grid;
    grid-auto-flow: dense;
    grid-template-columns: 1fr 1fr 1fr;
    grid-column-gap: 8px;
    grid-row-gap: 16px;

    max-width: 1180px;
    margin: auto;
    margin-bottom: 30px;
    padding-left: 30px;
    padding-right: 30px;
}

@media (min-width: 850px) {
    .visual-toc {
        /* grid-gap: 8px; */
        grid-template-columns: 1fr 1fr 1fr 1fr 1fr 1fr;
    }
}

@media (min-width: 1180px) {
    .visual-toc {
        grid-gap: 20px;
    }
}

.visual-toc-item {
    display: flex;
    flex-flow: column;
}

.visual-toc-top {
    flex-grow: 1;
    border: 1px solid #e5e5e5;
    border-radius: 5px;
    overflow: hidden;
    text-decoration: none;
    /* box-shadow: 0px 1px 4px rgba(0,0,0,0.05); */
    transition: box-shadow 0.35s, transform 0.35s;
    transform: scale(1);
    display: flex;
    flex-flow: column;
}

.visual-toc-top:hover {
    box-shadow: 0px 1px 4px rgba(0, 0, 0, 0.05);
    transform: scale(1.02);
    transition: box-shadow 0.15s, transform 0.15s;
}

.visual-toc d-tochead,
.visual-toc d-subhead {
    display: block;
    line-height: 1.3em;
    font-size: 85%;
    padding: 0.5em 1em 1em 1em;
}

.visual-toc d-tochead {
    counter-increment: toc-heading;
    color: #333;
    font-weight: 600;
}

.visual-toc d-tochead::before {
    display: block;
    content: "Section " counter(toc-heading);
    font-weight: 400;
    text-transform: uppercase;
    font-size: 0.6rem;
    color: #666;
}

.visual-toc d-subhead {
    display: none;
    color: #666;
    font-size: 75%;
}

.visual-toc-colab {
    border-radius: 5px;
    border: solid 1px rgba(0, 0, 0, 0.1);
    margin-top: 1em;
    padding-left: 1.2em;
    padding-right: 1.2em;
    padding-top: 0.25em;
    padding-bottom: 0.25em;
    text-transform: uppercase;
    color: #aaa;
    font-size: 10.5px;
    line-height: 24px;
    /* background-color: yellow; */
}

.visual-toc-colab>img {
    position: relative;
    top: 4px;
    /* filter: grayscale(); */
}

.visual-toc-item:hover .visual-toc-colab>img {
    filter: unset;
}

/* .visual-toc-top:hover,  */
.visual-toc-colab:hover {
    background-color: hsl(0, 0%, 97%);
    border-color: rgba(0, 0, 0, 0.2);
    color: #888;
}

.visual-toc a {
    display: block;
    text-decoration: none;
    cursor: pointer;
}

.visual-toc a :global(img) {
    width: 100%;
}

.visual-toc figure {
    margin: 0;
}

.visual-toc figure.gdoc-image {
    max-width: none;
    padding-right: 0;
}

.visual-toc-top img {
    border-bottom: 1px solid #EEE;
}</style>

<style>d-byline {
    display: none;
}

.d-byline-container {
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
    padding-top: 20px;
    padding-bottom: 20px;
}

.d-byline {
    grid-column: text;
    display: grid;
    grid-template-columns: [authors info-start] 5fr [affiliations published] 1fr [info-end];
    grid-template-rows: [authors-start affiliations-start] auto [affiliations-end published-start] auto [published-end] auto [authors-end info] auto;
    font-size: 0.8rem;
    line-height: 1.8em;
    grid-gap: 20px;
}

.d-byline h3 {
    font-size: 0.6rem;
    font-weight: 400;
    color: rgba(0, 0, 0, 0.5);
    margin: 0;
    text-transform: uppercase;
    margin-bottom: 4px;
}

.d-byline .info {
    color: rgba(0, 0, 0, 0.5);
    font-size: 80%;
    line-height: 1.5;
}

.d-byline .authors div {
    line-height: 1.5;
}

.d-byline .authors sup {
    font-size: 80%;
    margin-right: -4px;
}

.d-byline .author {
    margin-right: 6px;
    white-space: nowrap;
}

.d-byline a {
    color: inherit;
    text-decoration: none;
}

.d-byline-container {
    margin-bottom: 1em;
}</style>
<style>d-article{border-top-width:0px;}</style>

<d-bibliography src="bibliography.bib"></d-bibliography>
<style>.comment {
    background-color: hsl(54, 78%, 96%);
    border-left: solid hsl(54, 33%, 67%) 1px;
    padding: 1em;
    color: hsla(0, 0%, 0%, 0.67);
    margin-top: 1em;
}

.comment h3 {
    font-size: 100%;
    font-weight: bold;
    text-transform: uppercase;
    margin-top: 0px;
}

.comment .commenter-description {
    font-style: italic;
    margin-bottom: 1em;
    margin-top: 0px;
}

.comment .commenter-description,
.comment .commenter-description a {
    color: #777;
    font-size: 80%;
    line-height: 160%;
}

.comment div {
    margin-top: 1em;
}

.comment code {
    background: #FFF5;
    border-color: #EEEA;
}</style>
<style>h1 a:hover,
h2 a:hover,
h3 a,
h3 a:hover {
    text-decoration: none;
    border-bottom: none;
}

h1>a,
h2>a,
h3>a,
h4>a {
    text-decoration: none;
    border-bottom: none;
}

@media(min-width: 600px) {
    d-article hr {
        margin-top: 140px;
        margin-bottom: 120px;
    }
}

@media(max-width: 600px) {
    d-article hr {
        margin-top: 80px;
        margin-bottom: 80px;
    }
}

d-article ul,
d-appendix ul {
    padding-left: 0px;
}

@media(max-width: 600px) {

    d-citation-list,
    d-footnote-list {
        padding-left: 10px;
    }
}

d-article>ol,
d-appendix ol {
    padding-left: 0px !important;
}

d-article ol {
    padding-left: 36pt;
}

d-article ul li,
d-article ol li {
    margin-bottom: 8px;
}

d-footnote {
    margin-left: -3px;
    margin-right: 2px;
}

.math-grid {
    display: grid;
    grid-auto-flow: column;
    grid-gap: 8px 12px;
    width: min-content;
    margin: 0px;
    margin-bottom: 1em;
    /*max-width: 100%; grid-column: text-start / gutter-end; overflow: scroll;*/
}

.math-grid>d-math {
    grid-row: 1;
    margin: 0px;
    display: flex;
    align-items: center;
}

.math-grid>d-math[block] {
    margin-left: -1em;
    margin-top: calc(-0.5em - 8px);
    margin-bottom: calc(-0.5em - 8px);
}

d-math[sub] {
    padding-top: 12px;
}

.math-grid>.figcaption {
    grid-row: 2;
    margin: 0px;
    grid-column: auto / span 1;
    min-width: 100px;
    line-height: 120%;
    border-top: 1px solid #CCC;
    padding-top: 8px;
}

code {
    white-space: pre;
    background: #FAFAFA;
    border-radius: 2px;
    padding: 2px;
    padding-top: 1px;
    margin-left: 2px;
    margin-right: 2px;
    border: 1px solid #EEE;
}

p>sup,
h1>sup,
h2>sup,
h3>sup,
h4>sup,
li>sup {
    display: none;
}

pre.citation {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0, 0, 0, 0.1);
    background: rgba(255, 255, 255, 0.4);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
}

table p {
    min-height: 16px;
    line-height: 1.2;
}

table td {
    vertical-align: top;
}

table td>* {
    margin: 0px;
    margin-top: 8px;
    margin-bottom: 8px;
    font-size: 90%;
}

/* Print */

@media print {
    @page {
        margin: 0cm;
    }

    hr {
        margin: 0px;
        padding: 0px;
        opacity: 0.0;
        height: 0px;
        page-break-after: always;
    }

    h2 {
        display: block;
        page-break-before: always;
    }

    .gdoc-image {
        break-inside: avoid;
    }

    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
        --grid-template-columns: [screen-start] 1fr [page-start kicker-start middle-start text-start] 45px 45px 45px 45px 45px 45px 45px 45px [ kicker-end text-end gutter-start] 45px [middle-end] 45px [page-end gutter-end] 1fr [screen-end];
        --grid-column-gap: 16px;
        grid-template-columns: [screen-start] 60px [page-start kicker-start middle-start text-start] 80px 60px 60px 60px 60px 60px 60px 60px [ kicker-end text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
        grid-column-gap: 32px;
    }

    d-contents {
        display: block;
        justify-self: start;
        align-self: start;
        padding-bottom: 0.5em;
        margin-bottom: 1em;
        padding-left: 0.25em;
        border-bottom: 1px solid rgba(0, 0, 0, 0.1);
        border-bottom-width: 1px;
        border-bottom-style: solid;
        border-bottom-color: rgba(0, 0, 0, 0.1);
    }

}

@media(min-width: 1000px) {
    .outset-100 {
        margin-left: -100px;
    }
}</style>
<style>.gdoc-image,
.gdoc-image-gutter,
.gdoc-image-flex {
    width: 100%;
    max-width: calc(var(--img-width) / 1.5);
}

@media(max-width: 800px) {

    .gdoc-image,
    .gdoc-image-gutter,
    .gdoc-image-flex {
        padding-right: 2%;
    }
}

@media(min-width: 800px) {

    .gdoc-image,
    .gdoc-image-gutter,
    .gdoc-image-flex {
        padding-right: 8px;
    }
}

.gdoc-image {
    grid-column: text-start / screen-end;
    image-rendering: -webkit-optimize-contrast;
}

.gdoc-image-gutter {
    grid-column: gutter / screen-end;
    margin: 0px;
    padding: 0px;
    margin-top: 6px;
}

.gdoc-image-flex {
    margin: 0px;
    padding: 0px;
}

d-appendix .gdoc-image img,
d-appendix .gdoc-image-flex img {
    mix-blend-mode: darken;
}

.gdoc-image,
.gdoc-image-gutter,
.gdoc-image-flex {
    width: min(calc(var(--img-width) / 2), 100%);
    margin-left: auto;
    margin-right: auto;
    grid-column: screen;
    display: block;
    padding-right: 0;
}</style>
<style>.ha-block {
    font-family: monospace;
    font-size: 14px;
    line-height: 1.5em;
    text-wrap: wrap;
    /* white-space: pre-wrap; */
    margin-bottom: 1em;
    background: #F6F6F3;
    outline: 1px solid #B8B6AE;
    padding: 12px;
    border-radius: 2fr;

    p {
        margin-bottom: 0;
        margin-block-start: 0;
        margin-block-end: 0;
    }

    b.max-act {
        border-radius: 4px;
        padding: 1px;
        cursor: default;
        white-space: pre;
        outline: 1px solid #000;
        background: rgb(222, 82, 7);
    }
}

.prompt-block {
    font-family: monospace;
    font-size: 14px;
    line-height: 1.5em;
    text-wrap: wrap;
    /* white-space: pre-wrap; */
    margin-bottom: 1em;
    background: #F6F6F3;
    outline: 1px solid #B8B6AE;
    padding: 12px;
    border-radius: 2fr;

    p {
        margin-bottom: 0;
        margin-block-start: 0;
        margin-block-end: 0;
    }

    b.max-act {
        border-radius: 4px;
        padding: 1px;
        cursor: default;
        white-space: pre;
        outline: 1px solid #000;
        background: rgb(222, 82, 7);
    }
}

.prompt-inline {
    background-color: #F6F6F3;
    /* Same background as prompt-block */
    outline: 1px solid #B8B6AE;
    /* Same border as prompt-block */
    font-family: monospace;
    /* Same font as prompt-block */
    font-size: 14px;
    /* Same font size as prompt-block */
    padding: 1px 2px;
    border-radius: 3px;
    /* Slightly rounded corners */
    white-space: nowrap;
    display: inline;
}</style>
<style>@media(max-width: 1199px) {
    d-contents {
        display: none;
        justify-self: start;
        align-self: start;
        padding-bottom: 0.5em;
        margin-bottom: 1em;
        padding-left: 0.25em;
        border-bottom: 1px solid rgba(0, 0, 0, 0.1);
        border-bottom-width: 1px;
        border-bottom-style: solid;
        border-bottom-color: rgba(0, 0, 0, 0.1);
    }
}

d-contents a:hover {
    border-bottom: none;
}

@media (min-width: 1200px) {
    d-contents {
        align-self: start;
        grid-column-start: 1 !important;
        grid-column-end: 4 !important;
        grid-row: auto / span 6;
        justify-self: end;
        margin-top: 0em;
        padding-right: 3em;
        padding-left: 2em;
        border-right: 1px solid rgba(0, 0, 0, 0.1);
        border-right-width: 1px;
        border-right-style: solid;
        border-right-color: rgba(0, 0, 0, 0.1);
    }
}

d-contents nav h3 {
    margin-top: 0;
    margin-bottom: 1em;
}

d-contents nav div {
    color: rgba(0, 0, 0, 0.8);
    font-weight: bold;
}

d-contents nav a {
    color: rgba(0, 0, 0, 0.8);
    border-bottom: none;
    text-decoration: none;
}

d-contents li {
    list-style-type: none;
}

d-contents ul,
d-article d-contents ul {
    padding-left: 1em;
}

d-contents nav ul li {
    margin-bottom: .25em;
}

d-contents nav a:hover {
    text-decoration: underline solid rgba(0, 0, 0, 0.6);
}

d-contents nav ul {
    margin-top: 0;
    margin-bottom: 6px;
}


d-contents nav>div {
    display: block;
    outline: none;
    margin-bottom: 0.5em;
}

d-contents nav>div>a {
    font-size: 13px;
    font-weight: 600;
}

d-contents nav>div>a:hover,
d-contents nav>ul>li>a:hover {
    text-decoration: none;
}</style>
<style>.visual-toc {
    counter-reset: toc-heading;
    display: grid;
    grid-auto-flow: dense;
    grid-template-columns: 1fr 1fr 1fr;
    grid-column-gap: 8px;
    grid-row-gap: 16px;

    max-width: 1180px;
    margin: auto;
    margin-bottom: 30px;
    padding-left: 30px;
    padding-right: 30px;
}

@media (min-width: 850px) {
    .visual-toc {
        /* grid-gap: 8px; */
        grid-template-columns: 1fr 1fr 1fr 1fr 1fr 1fr;
    }
}

@media (min-width: 1180px) {
    .visual-toc {
        grid-gap: 20px;
    }
}

.visual-toc-item {
    display: flex;
    flex-flow: column;
}

.visual-toc-top {
    flex-grow: 1;
    border: 1px solid #e5e5e5;
    border-radius: 5px;
    overflow: hidden;
    text-decoration: none;
    /* box-shadow: 0px 1px 4px rgba(0,0,0,0.05); */
    transition: box-shadow 0.35s, transform 0.35s;
    transform: scale(1);
    display: flex;
    flex-flow: column;
}

.visual-toc-top:hover {
    box-shadow: 0px 1px 4px rgba(0, 0, 0, 0.05);
    transform: scale(1.02);
    transition: box-shadow 0.15s, transform 0.15s;
}

.visual-toc d-tochead,
.visual-toc d-subhead {
    display: block;
    line-height: 1.3em;
    font-size: 85%;
    padding: 0.5em 1em 1em 1em;
}

.visual-toc d-tochead {
    counter-increment: toc-heading;
    color: #333;
    font-weight: 600;
}

.visual-toc d-tochead::before {
    display: block;
    content: "Section " counter(toc-heading);
    font-weight: 400;
    text-transform: uppercase;
    font-size: 0.6rem;
    color: #666;
}

.visual-toc d-subhead {
    display: none;
    color: #666;
    font-size: 75%;
}

.visual-toc-colab {
    border-radius: 5px;
    border: solid 1px rgba(0, 0, 0, 0.1);
    margin-top: 1em;
    padding-left: 1.2em;
    padding-right: 1.2em;
    padding-top: 0.25em;
    padding-bottom: 0.25em;
    text-transform: uppercase;
    color: #aaa;
    font-size: 10.5px;
    line-height: 24px;
    /* background-color: yellow; */
}

.visual-toc-colab>img {
    position: relative;
    top: 4px;
    /* filter: grayscale(); */
}

.visual-toc-item:hover .visual-toc-colab>img {
    filter: unset;
}

/* .visual-toc-top:hover,  */
.visual-toc-colab:hover {
    background-color: hsl(0, 0%, 97%);
    border-color: rgba(0, 0, 0, 0.2);
    color: #888;
}

.visual-toc a {
    display: block;
    text-decoration: none;
    cursor: pointer;
}

.visual-toc a :global(img) {
    width: 100%;
}

.visual-toc figure {
    margin: 0;
}

.visual-toc figure.gdoc-image {
    max-width: none;
    padding-right: 0;
}

.visual-toc-top img {
    border-bottom: 1px solid #EEE;
}</style>

<style>d-byline {
    display: none;
}

.d-byline-container {
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
    padding-top: 20px;
    padding-bottom: 20px;
}

.d-byline {
    grid-column: text;
    display: grid;
    grid-template-columns: [authors info-start] 5fr [affiliations published] 1fr [info-end];
    grid-template-rows: [authors-start affiliations-start] auto [affiliations-end published-start] auto [published-end] auto [authors-end info] auto;
    font-size: 0.8rem;
    line-height: 1.8em;
    grid-gap: 20px;
}

.d-byline h3 {
    font-size: 0.6rem;
    font-weight: 400;
    color: rgba(0, 0, 0, 0.5);
    margin: 0;
    text-transform: uppercase;
    margin-bottom: 4px;
}

.d-byline .info {
    color: rgba(0, 0, 0, 0.5);
    font-size: 80%;
    line-height: 1.5;
}

.d-byline .authors div {
    line-height: 1.5;
}

.d-byline .authors sup {
    font-size: 80%;
    margin-right: -4px;
}

.d-byline .author {
    margin-right: 6px;
    white-space: nowrap;
}

.d-byline a {
    color: inherit;
    text-decoration: none;
}

.d-byline-container {
    margin-bottom: 1em;
}</style>
<style>d-article{border-top-width:0px;}</style>

<d-bibliography src="bibliography.bib"></d-bibliography>
<style>.comment {
    background-color: hsl(54, 78%, 96%);
    border-left: solid hsl(54, 33%, 67%) 1px;
    padding: 1em;
    color: hsla(0, 0%, 0%, 0.67);
    margin-top: 1em;
}

.comment h3 {
    font-size: 100%;
    font-weight: bold;
    text-transform: uppercase;
    margin-top: 0px;
}

.comment .commenter-description {
    font-style: italic;
    margin-bottom: 1em;
    margin-top: 0px;
}

.comment .commenter-description,
.comment .commenter-description a {
    color: #777;
    font-size: 80%;
    line-height: 160%;
}

.comment div {
    margin-top: 1em;
}

.comment code {
    background: #FFF5;
    border-color: #EEEA;
}</style>
<style>h1 a:hover,
h2 a:hover,
h3 a,
h3 a:hover {
    text-decoration: none;
    border-bottom: none;
}

h1>a,
h2>a,
h3>a,
h4>a {
    text-decoration: none;
    border-bottom: none;
}

@media(min-width: 600px) {
    d-article hr {
        margin-top: 140px;
        margin-bottom: 120px;
    }
}

@media(max-width: 600px) {
    d-article hr {
        margin-top: 80px;
        margin-bottom: 80px;
    }
}

d-article ul,
d-appendix ul {
    padding-left: 0px;
}

@media(max-width: 600px) {

    d-citation-list,
    d-footnote-list {
        padding-left: 10px;
    }
}

d-article>ol,
d-appendix ol {
    padding-left: 0px !important;
}

d-article ol {
    padding-left: 36pt;
}

d-article ul li,
d-article ol li {
    margin-bottom: 8px;
}

d-footnote {
    margin-left: -3px;
    margin-right: 2px;
}

.math-grid {
    display: grid;
    grid-auto-flow: column;
    grid-gap: 8px 12px;
    width: min-content;
    margin: 0px;
    margin-bottom: 1em;
    /*max-width: 100%; grid-column: text-start / gutter-end; overflow: scroll;*/
}

.math-grid>d-math {
    grid-row: 1;
    margin: 0px;
    display: flex;
    align-items: center;
}

.math-grid>d-math[block] {
    margin-left: -1em;
    margin-top: calc(-0.5em - 8px);
    margin-bottom: calc(-0.5em - 8px);
}

d-math[sub] {
    padding-top: 12px;
}

.math-grid>.figcaption {
    grid-row: 2;
    margin: 0px;
    grid-column: auto / span 1;
    min-width: 100px;
    line-height: 120%;
    border-top: 1px solid #CCC;
    padding-top: 8px;
}

code {
    white-space: pre;
    background: #FAFAFA;
    border-radius: 2px;
    padding: 2px;
    padding-top: 1px;
    margin-left: 2px;
    margin-right: 2px;
    border: 1px solid #EEE;
}

p>sup,
h1>sup,
h2>sup,
h3>sup,
h4>sup,
li>sup {
    display: none;
}

pre.citation {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0, 0, 0, 0.1);
    background: rgba(255, 255, 255, 0.4);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
}

table p {
    min-height: 16px;
    line-height: 1.2;
}

table td {
    vertical-align: top;
}

table td>* {
    margin: 0px;
    margin-top: 8px;
    margin-bottom: 8px;
    font-size: 90%;
}

/* Print */

@media print {
    @page {
        margin: 0cm;
    }

    hr {
        margin: 0px;
        padding: 0px;
        opacity: 0.0;
        height: 0px;
        page-break-after: always;
    }

    h2 {
        display: block;
        page-break-before: always;
    }

    .gdoc-image {
        break-inside: avoid;
    }

    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
        --grid-template-columns: [screen-start] 1fr [page-start kicker-start middle-start text-start] 45px 45px 45px 45px 45px 45px 45px 45px [ kicker-end text-end gutter-start] 45px [middle-end] 45px [page-end gutter-end] 1fr [screen-end];
        --grid-column-gap: 16px;
        grid-template-columns: [screen-start] 60px [page-start kicker-start middle-start text-start] 80px 60px 60px 60px 60px 60px 60px 60px [ kicker-end text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
        grid-column-gap: 32px;
    }

    d-contents {
        display: block;
        justify-self: start;
        align-self: start;
        padding-bottom: 0.5em;
        margin-bottom: 1em;
        padding-left: 0.25em;
        border-bottom: 1px solid rgba(0, 0, 0, 0.1);
        border-bottom-width: 1px;
        border-bottom-style: solid;
        border-bottom-color: rgba(0, 0, 0, 0.1);
    }

}

@media(min-width: 1000px) {
    .outset-100 {
        margin-left: -100px;
    }
}</style>
<style>.gdoc-image,
.gdoc-image-gutter,
.gdoc-image-flex {
    width: 100%;
    max-width: calc(var(--img-width) / 1.5);
}

@media(max-width: 800px) {

    .gdoc-image,
    .gdoc-image-gutter,
    .gdoc-image-flex {
        padding-right: 2%;
    }
}

@media(min-width: 800px) {

    .gdoc-image,
    .gdoc-image-gutter,
    .gdoc-image-flex {
        padding-right: 8px;
    }
}

.gdoc-image {
    grid-column: text-start / screen-end;
    image-rendering: -webkit-optimize-contrast;
}

.gdoc-image-gutter {
    grid-column: gutter / screen-end;
    margin: 0px;
    padding: 0px;
    margin-top: 6px;
}

.gdoc-image-flex {
    margin: 0px;
    padding: 0px;
}

d-appendix .gdoc-image img,
d-appendix .gdoc-image-flex img {
    mix-blend-mode: darken;
}

.gdoc-image,
.gdoc-image-gutter,
.gdoc-image-flex {
    width: min(calc(var(--img-width) / 2), 100%);
    margin-left: auto;
    margin-right: auto;
    grid-column: screen;
    display: block;
    padding-right: 0;
}</style>
<style>.ha-block {
    font-family: monospace;
    font-size: 14px;
    line-height: 1.5em;
    text-wrap: wrap;
    /* white-space: pre-wrap; */
    margin-bottom: 1em;
    background: #F6F6F3;
    outline: 1px solid #B8B6AE;
    padding: 12px;
    border-radius: 2fr;

    p {
        margin-bottom: 0;
        margin-block-start: 0;
        margin-block-end: 0;
    }

    b.max-act {
        border-radius: 4px;
        padding: 1px;
        cursor: default;
        white-space: pre;
        outline: 1px solid #000;
        background: rgb(222, 82, 7);
    }
}

.prompt-block {
    font-family: monospace;
    font-size: 14px;
    line-height: 1.5em;
    text-wrap: wrap;
    /* white-space: pre-wrap; */
    margin-bottom: 1em;
    background: #F6F6F3;
    outline: 1px solid #B8B6AE;
    padding: 12px;
    border-radius: 2fr;

    p {
        margin-bottom: 0;
        margin-block-start: 0;
        margin-block-end: 0;
    }

    b.max-act {
        border-radius: 4px;
        padding: 1px;
        cursor: default;
        white-space: pre;
        outline: 1px solid #000;
        background: rgb(222, 82, 7);
    }
}

.prompt-inline {
    background-color: #F6F6F3;
    /* Same background as prompt-block */
    outline: 1px solid #B8B6AE;
    /* Same border as prompt-block */
    font-family: monospace;
    /* Same font as prompt-block */
    font-size: 14px;
    /* Same font size as prompt-block */
    padding: 1px 2px;
    border-radius: 3px;
    /* Slightly rounded corners */
    white-space: nowrap;
    display: inline;
}</style>
<style>@media(max-width: 1199px) {
    d-contents {
        display: none;
        justify-self: start;
        align-self: start;
        padding-bottom: 0.5em;
        margin-bottom: 1em;
        padding-left: 0.25em;
        border-bottom: 1px solid rgba(0, 0, 0, 0.1);
        border-bottom-width: 1px;
        border-bottom-style: solid;
        border-bottom-color: rgba(0, 0, 0, 0.1);
    }
}

d-contents a:hover {
    border-bottom: none;
}

@media (min-width: 1200px) {
    d-contents {
        align-self: start;
        grid-column-start: 1 !important;
        grid-column-end: 4 !important;
        grid-row: auto / span 6;
        justify-self: end;
        margin-top: 0em;
        padding-right: 3em;
        padding-left: 2em;
        border-right: 1px solid rgba(0, 0, 0, 0.1);
        border-right-width: 1px;
        border-right-style: solid;
        border-right-color: rgba(0, 0, 0, 0.1);
    }
}

d-contents nav h3 {
    margin-top: 0;
    margin-bottom: 1em;
}

d-contents nav div {
    color: rgba(0, 0, 0, 0.8);
    font-weight: bold;
}

d-contents nav a {
    color: rgba(0, 0, 0, 0.8);
    border-bottom: none;
    text-decoration: none;
}

d-contents li {
    list-style-type: none;
}

d-contents ul,
d-article d-contents ul {
    padding-left: 1em;
}

d-contents nav ul li {
    margin-bottom: .25em;
}

d-contents nav a:hover {
    text-decoration: underline solid rgba(0, 0, 0, 0.6);
}

d-contents nav ul {
    margin-top: 0;
    margin-bottom: 6px;
}


d-contents nav>div {
    display: block;
    outline: none;
    margin-bottom: 0.5em;
}

d-contents nav>div>a {
    font-size: 13px;
    font-weight: 600;
}

d-contents nav>div>a:hover,
d-contents nav>ul>li>a:hover {
    text-decoration: none;
}</style>
<style>.visual-toc {
    counter-reset: toc-heading;
    display: grid;
    grid-auto-flow: dense;
    grid-template-columns: 1fr 1fr 1fr;
    grid-column-gap: 8px;
    grid-row-gap: 16px;

    max-width: 1180px;
    margin: auto;
    margin-bottom: 30px;
    padding-left: 30px;
    padding-right: 30px;
}

@media (min-width: 850px) {
    .visual-toc {
        /* grid-gap: 8px; */
        grid-template-columns: 1fr 1fr 1fr 1fr 1fr 1fr;
    }
}

@media (min-width: 1180px) {
    .visual-toc {
        grid-gap: 20px;
    }
}

.visual-toc-item {
    display: flex;
    flex-flow: column;
}

.visual-toc-top {
    flex-grow: 1;
    border: 1px solid #e5e5e5;
    border-radius: 5px;
    overflow: hidden;
    text-decoration: none;
    /* box-shadow: 0px 1px 4px rgba(0,0,0,0.05); */
    transition: box-shadow 0.35s, transform 0.35s;
    transform: scale(1);
    display: flex;
    flex-flow: column;
}

.visual-toc-top:hover {
    box-shadow: 0px 1px 4px rgba(0, 0, 0, 0.05);
    transform: scale(1.02);
    transition: box-shadow 0.15s, transform 0.15s;
}

.visual-toc d-tochead,
.visual-toc d-subhead {
    display: block;
    line-height: 1.3em;
    font-size: 85%;
    padding: 0.5em 1em 1em 1em;
}

.visual-toc d-tochead {
    counter-increment: toc-heading;
    color: #333;
    font-weight: 600;
}

.visual-toc d-tochead::before {
    display: block;
    content: "Section " counter(toc-heading);
    font-weight: 400;
    text-transform: uppercase;
    font-size: 0.6rem;
    color: #666;
}

.visual-toc d-subhead {
    display: none;
    color: #666;
    font-size: 75%;
}

.visual-toc-colab {
    border-radius: 5px;
    border: solid 1px rgba(0, 0, 0, 0.1);
    margin-top: 1em;
    padding-left: 1.2em;
    padding-right: 1.2em;
    padding-top: 0.25em;
    padding-bottom: 0.25em;
    text-transform: uppercase;
    color: #aaa;
    font-size: 10.5px;
    line-height: 24px;
    /* background-color: yellow; */
}

.visual-toc-colab>img {
    position: relative;
    top: 4px;
    /* filter: grayscale(); */
}

.visual-toc-item:hover .visual-toc-colab>img {
    filter: unset;
}

/* .visual-toc-top:hover,  */
.visual-toc-colab:hover {
    background-color: hsl(0, 0%, 97%);
    border-color: rgba(0, 0, 0, 0.2);
    color: #888;
}

.visual-toc a {
    display: block;
    text-decoration: none;
    cursor: pointer;
}

.visual-toc a :global(img) {
    width: 100%;
}

.visual-toc figure {
    margin: 0;
}

.visual-toc figure.gdoc-image {
    max-width: none;
    padding-right: 0;
}

.visual-toc-top img {
    border-bottom: 1px solid #EEE;
}</style>

<style>d-byline {
    display: none;
}

.d-byline-container {
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
    padding-top: 20px;
    padding-bottom: 20px;
}

.d-byline {
    grid-column: text;
    display: grid;
    grid-template-columns: [authors info-start] 5fr [affiliations published] 1fr [info-end];
    grid-template-rows: [authors-start affiliations-start] auto [affiliations-end published-start] auto [published-end] auto [authors-end info] auto;
    font-size: 0.8rem;
    line-height: 1.8em;
    grid-gap: 20px;
}

.d-byline h3 {
    font-size: 0.6rem;
    font-weight: 400;
    color: rgba(0, 0, 0, 0.5);
    margin: 0;
    text-transform: uppercase;
    margin-bottom: 4px;
}

.d-byline .info {
    color: rgba(0, 0, 0, 0.5);
    font-size: 80%;
    line-height: 1.5;
}

.d-byline .authors div {
    line-height: 1.5;
}

.d-byline .authors sup {
    font-size: 80%;
    margin-right: -4px;
}

.d-byline .author {
    margin-right: 6px;
    white-space: nowrap;
}

.d-byline a {
    color: inherit;
    text-decoration: none;
}

.d-byline-container {
    margin-bottom: 1em;
}</style>
<style>d-article{border-top-width:0px;}</style>
    
    
</head>
<body>
    
    <div class="article-header">
    <style>
        .article-header {width: 100%; padding: 10px; padding-bottom: 20px; box-sizing: border-box; display: flex; align-items: center; gap: 12px; }
        .home-link {font-size: 100%; color: #333;}
        a.logo {display: flex; text-decoration: none;}
    </style>
    <a class="logo" href="https://anthropic.com">
    <svg width="36" height="25" viewBox="0 0 36 25" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M24.8304 0H19.5612L29.1696 24.1071H34.4388L24.8304 0Z" fill="#1F1F1E"/>
        <path d="M9.60842 0L0 24.1071H5.37245L7.33753 19.0446H17.3895L19.3546 24.1071H24.727L15.1186 0H9.60842ZM9.07531 14.5676L12.3635 6.09566L15.6517 14.5676H9.07531Z" fill="#1F1F1E"/>
    </svg>
    </a>
    <a class="home-link" href="https://transformer-circuits.pub/">Transformer Circuits Thread</a>
</div>
<d-title>
    <h1>When Models Manipulate Manifolds: The Geometry of a Counting Task</h1>
</d-title>
<style>d-title:first-of-type {display: none;}</style>
<d-title style="background: hsla(40,30%,30%,0.025); border-top: 1px solid rgba(0, 0, 0, 0.1);" class="real-title">
<h1 style="grid-column: page; margin: 4px; margin-top: 40px; margin-bottom: 40px;">When Models Manipulate Manifolds: The Geometry of a Counting Task</h1>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_000.png' /></figure>
</d-title>
<style> d-byline { display: none; } .d-byline-container { border-top: 1px solid rgba(0,0,0,0.1); border-bottom: 1px solid rgba(0,0,0,0.1); padding-top: 20px; padding-bottom: 20px; } .d-byline { grid-column: text; display: grid; grid-template-columns: [authors info-start] 5fr [affiliations published] 1fr [info-end]; grid-template-rows: [authors-start affiliations-start] auto [affiliations-end published-start] auto [published-end] auto [authors-end info] auto; font-size: 0.8rem; line-height: 1.8em; grid-gap: 20px; } .d-byline h3 { font-size: 0.6rem; font-weight: 400; color: rgba(0, 0, 0, 0.5); margin: 0; text-transform: uppercase; margin-bottom: 4px; } .d-byline .info { color: rgba(0, 0, 0, 0.5); font-size: 80%; line-height: 1.5; } .d-byline .authors div { line-height: 1.5; } .d-byline .authors sup { font-size: 80%; margin-right: -4px; } .d-byline .author { margin-right: 6px; white-space:nowrap; } .d-byline a { color: inherit; text-decoration: none; } </style>
<div class='d-byline-container base-grid'> 
<div class='d-byline'> 
<div class='authors' style='grid-area: authors;'> 
<h3>Authors</h3> 
<div> 
<div style="margin-bottom: 10px; text-wrap: balance;"> 
<span class='author'>Wes Gurnee<sup>*</sup>,</span>
<span class='author'>Emmanuel Ameisen<sup>*</sup>,</span>
<span class='author'>Isaac Kauvar,</span>
<span class='author'>Julius Tarng,</span>
<span class='author'>Adam Pearce,</span>
<span class='author'>Chris Olah,</span>
<span class='author'>Joshua Batson<sup>*‡</sup></span>
</span>
</div>
</div> 
</div> <!-- .authors -->
<div class='affiliations' style='grid-area: affiliations;'> 
<h3>Affiliations</h3> 
<div><a href='https://www.anthropic.com/'>Anthropic</a>
</div> 
</div> <!-- .affiliations -->
<div class='published' style='grid-area: published;'> 
<h3>Published</h3> 
<div>October 21st, 2025</div> 
</div> <!-- .published -->
<div class='info' style='grid-area: info; margin-top: -15px;'>
<div> 
<span style='margin-right: 10px; white-space:nowrap;'>* Core Research Contributor;</span> 
<span style='margin-right: 10px; white-space:nowrap;'>‡ Correspondence to <a href='mailto:joshb@anthropic.com'>joshb@anthropic.com</a></span> 
</div> 
</div> <!-- .info -->
</div> <!-- .d-byline -->
</div> <!-- .d-byline-container.base-grid -->
<style>@media screen and (min-width: 900px) {.offset-gdoc-image {margin-left: -65px;}} d-title:not(.real-title){display: none;}</style>
<d-article>
<d-contents>
<nav class="l-text toc figcaption">
<h3>Contents</h3>
<style>d-contents nav .appendix-toc a {font-weight: normal !important; opacity: 0.8;}</style>
<div><a href='#introduction'>Introduction</a></div>
<div><a href='#char-count'>Representing Character Count</a></div>
<div><a href='#boundary'>Sensing the Line Boundary</a></div>
<div><a href='#prediction'>Predicting the Newline</a></div>
<div><a href='#count-algo'>A Distributed Character Counting Algorithm</a></div>
<div><a href='#illusion'>Visual Illusions</a></div>
<div><a href='#related-work'>Related Work</a></div>
<div><a href='#discussion'>Discussion</a></div>
</nav>
</d-contents>
<h3><a id='introduction' href='#introduction'>Introduction</a></h3>
<p>Intelligent systems need perception to understand, predict, and navigate their environment. These sensory capabilities reflect what's useful for survival in a specific environment: bats use echolocation, migratory birds sense magnetic fields, Arctic reindeer shift their UV vision seasonally. But when your world is made of text, what do you see?  Language models encounter many text-based tasks that benefit from visual or spatial reasoning: parsing ASCII art, interpreting tables, or handling text wrapping constraints. Yet their only “sensory” input is a sequence of integers representing tokens. They must learn perceptual abilities from scratch, developing specialized mechanisms in the process.</p>
<p>In this work, we investigate the mechanisms that enable Claude 3.5 Haiku to perform a natural perceptual task which is common in pretraining corpora and involves tracking position in a document. We find learned representations of position that are in some ways quite similar to the biological neurons found in mammals who perform analogous tasks (“place cells” and “boundary cells” in mice), but in other ways unique to the constraints of the residual stream in language models. We study these representations and find dual interpretations: we can understand them as a family of discrete features or as a one-dimensional “feature manifold”/“multidimensional feature” <d-cite key="olah2023manifold,olah2024multidimensional,gorton2024curve,engels2025not"></d-cite>.<d-footnote>All features have a magnitude dimension; so a discrete feature is a one-dimensional ray, and a one-dimensional feature manifold is the set of all scalings of that manifold, contracting to the origin. See <a href='https://transformer-circuits.pub/2024/july-update/index.html#linear-representations'>What is a Linear Representation? What is a Multidimensional Feature?</a></d-footnote> In the first interpretation, position is determined by which features activate and how strongly; in the latter interpretation, it's determined by angular movement on the feature manifold. Similarly, computation has two dual interpretations, as discrete circuits or geometric transformations.</p>
<p>The task we study is linebreaking in fixed-width text. When training on source code, chat logs, email archives, scanned articles, or judicial rulings that have line width constraints, how does the model learn to predict when to break a line? <d-footnote>Michaud<span style='font-style: italic;'> et al.</span> looked for “quanta” of model skills by clustering gradients <d-cite key="michaud2023the"></d-cite>. Their Figure 1 shows that predicting newlines in fixed-width text formed one of the top 400 clusters for the smallest model in the Pythia family, with 70m parameters.</d-footnote> Human visual perception lets us do this almost completely subconsciously – when writing a birthday card, you can see when you are out of room on a line and need to begin the next – but language models see just a list of integers. In order to correctly predict the next <span style='font-style: italic;'>token</span>, in addition to selecting the next <span style='font-style: italic;'>word</span>, the model must somehow count the characters in the current line, subtract that from the line width constraint of the document, and compare the number of characters remaining to the length of the next word. As a concrete example, consider the below pair of prompts with an implicit 50-character line wrapping constraint.<d-footnote>The wrapping constraint is implicit. Each newline gives a lower bound (the previous word did fit) and an upper bound (the next word did not). We do not nail down the extent to which the model performs optimal inference with respect to those constraints, rather focusing on how it approximately uses the length of each preceding line to determine whether to break the next. There are also many edge cases for handling tokenization and punctuation. A model could even attempt to infer whether the source document used a non-monospace font and then use the pixel count rather than the character count as a predictive signal!</d-footnote> When the next word fits, the model says it; when it does not, the model breaks the line:</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_001.png' /></figure>
<p>To orient ourselves to the stages of the computation, we first studied the model using discrete dictionary features. In this frame, we can understand computation as an “attribution graph” <d-cite key="ameisen2025circuit"></d-cite> where a cascade of features excite or inhibit each other.<d-footnote>We actually first tried to use patching and probing without looking at the graph as a kind of methodological test of the utility of features, but did not make much progress. In hindsight, we were training probes for quantities different than the ones the model represents cleanly, e.g., a fusion of the current token position and the line width.</d-footnote></p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_002.png' /><figcaption class='text-caption'>Attribution Graph for Claude 3.5 Haiku’s prediction of a newline in the aluminum prompt. We see features relating to “width of the previous line” and “position in the current line” which together activate features for “distance from line limit”. Combined with features for the planned next word, these features activate “predict newline” features.</figcaption></figure>
<p>The attribution graph shows how the model performs this task by combining features that represent different concepts it needs to track:</p>
<ol><li style='margin-left: 36pt;'>Features for the current position in the line (the character count) as well as features for the total line width (the constraint) are computed by accumulating features for individual token lengths.</li><li style='margin-left: 36pt;'>The model then combines these two representations — current position and line width — to estimate the distance from the end of the line, leading to “characters remaining” features.</li><li style='margin-left: 36pt;'>Finally, the model uses this estimate of characters remaining along with features for the planned next word to determine if the next word will fit on the line or not.</li></ol>
<p>The attribution graph provides a kind of execution trace of the algorithm, showing on this prompt which variables are computed and from what. After finding large feature families involved in representing these quantities across a diverse dataset, we suspected a simpler lens might be provided in terms of lower-dimensional feature manifolds interacting geometrically. We found geometric perspectives on the following questions:</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_003.png' /><figcaption class='text-caption'>Key steps in the linebreaking behavior can be described in terms of the construction and manipulation of manifolds.</figcaption></figure>
<p><span style='font-weight: 700;'>How does the model represent different counts?</span> The number of characters in a token, the number of characters in the current line, the overall line width constraint, and the number of characters remaining in the current line are each represented on 1-dimensional feature manifolds embedded with high curvature in low-dimensional subspaces of the residual stream. These manifolds have a dual interpretation in terms of discrete features, which tile the manifold in a canonical way, providing approximate local coordinates. Manifolds with similar geometry arise for a variety of ordinal concepts, and a ringing pattern we see in the embedded geometry in all these cases is optimal with respect to a simple physical model (§<a href='#char-count'>Representing Character Count</a>).<d-footnote>Ringing, in the manifold perspective, corresponds to interference in the feature superposition perspective.</d-footnote></p>
<p><span style='font-weight: 700;'>How </span><span style='font-weight: 700;'>does the model detect the boundary?</span> To detect an approaching line boundary, the model must compare two quantities: the current character count and the line width. We find attention heads whose QK matrix rotates one counting manifold to align it with the other at a specific offset, creating a large inner product when the difference of the counts falls within a target range. Multiple heads with different offsets work together to precisely estimate the characters remaining (§<a href='#boundary'>Sensing the Line Boundary</a>).</p>
<p><span style='font-weight: 700;'>How does the model know if the next word fits? </span>The final decision — whether to predict a newline — requires combining the estimate of characters remaining with the length of the predicted next word. We discover that the model positions these counts on near-orthogonal subspaces, creating a geometric structure where the correct linebreak prediction is linearly separable (§<a href='#prediction'>Predicting the Newline</a>).</p>
<p><span style='font-weight: 700;'>How does the model construct these curved geometries? </span>The curvature in the character count representation manifold is produced by many attention heads working together, each contributing a piece of the overall curvature. This distributed algorithm is necessary because individual components cannot generate sufficient output variance to create the full representation (§<a href='#count-algo'>A Distributed Character Counting Algorithm</a>).</p>
<p>We validate these interpretations through targeted interventions, ablations, and “visual illusions” — character sequences that hijack specific attention mechanisms to disrupt spatial perception (§<a href='#illusion'>Visual Illusions</a>).</p>
<p>Zooming out, we take several broader lessons from this mechanistic case study:</p>
<p><span style='font-weight: 700;'>When </span><span style='font-weight: 700;'>Models Manipulate Manifolds</span>. For representing a scalar quantity (e.g., integer counts from <d-math>1</d-math> to <d-math>N</d-math>), it is inefficient to use <d-math>N</d-math> orthogonal dimensions, and not expressive enough to use just one<d-footnote>Orthogonal dimensions would also not be robust to estimation noise.</d-footnote>. Instead models learn to represent these quantities on a feature manifold with intrinsic dimension 1 (the count) embedded in a subspace with extrinsic dimension <d-math>1 < d \ll N</d-math> (e.g., <d-cite key="gorton2024curve,engels2025not,modell2025origins"></d-cite>), in which the curve “ripples”. Such rippled manifolds optimally trade off capacity constraints (roughly, dimensionality) with maintaining the distinguishability of different scalar values (curvature). Our work demonstrates the intricate ways in which these manifolds can be manipulated to perform <span style='font-style: italic;'>computation</span> and show how this can require distributing computation across multiple model components. </p>
<p><span style='font-weight: 700;'>Duality of</span><span style='font-weight: 700;'> Features and Geometry</span>. Dictionary features provide an unsupervised entry point for discovering mechanisms, and attribution graphs surface the important features for any particular prediction. Sometimes, discrete features (and their interactions) can be equivalently described using continuous feature manifolds (and their transformations). In cases where it is possible to explicitly parameterize the manifold (as with the various integer counts we study), we can directly study the geometry, making some operations clearer (e.g., boundary detection). But this approach is expensive in researcher time and potentially limited in scope: it's straightforward when studying <span style='font-style: italic;'>known</span> continuous variables but becomes difficult to execute correctly for more complex, difficult-to-parametrize concepts. </p>
<p><span style='font-weight: 700;'>Complexity Tax</span>. While unsupervised discovery is a victory in and of itself, dictionary features fragment the model into a multitude of small pieces and interactions – a kind of complexity tax on the interpretation. In cases where a manifold parametrization exists, we can think of the geometric description as reducing this tax. In other cases, we will need additional tools to reduce the interpretation burden, like hierarchical representations <d-cite key="costa2025flat"></d-cite> or macroscopic structure in the global weights <d-cite key="olah2023interpretability"></d-cite>. We would be excited to see methods that extend the dictionary learning paradigm to unsupervised discovery of other kinds of geometric structures (e.g., those found in prior work <d-cite key="hewitt2019structural,reif2019visualizing,chang2022geometry,wattenberg2024relational,park2024geometry,li2025geometry,wollschlager2025geometry,hindupur2025projecting,modell2025origins"></d-cite>).</p>
<p><span style='font-weight: 700;'>Natural Tasks</span>. The crispness of the representations and circuits we found was quite striking, and may be due to how well the model does the task. Linebreaking is an extremely natural behavior for a pretrained language model, and even tiny models are capable of it given enough context. Studying tasks which are natural for pretrained language models, instead of those of more theoretical interest to human investigators, may offer promising targets for finding general mechanisms.</p>
<h4>Preliminaries</h4>
<p>To enable systematic analysis, we created a synthetic dataset using a text corpus of diverse prose where we (1) stripped out all newlines and (2) reinserted newlines every <d-math>k</d-math> characters to the nearest word boundary <d-math>\leq k</d-math> for <d-math>k=15,20,\ldots,150</d-math>. As an example, here is the opening sentence of the Gettysburg Address, wrapped to <d-math>k=40</d-math> characters, with the newlines shown explicitly.</p>
<div class='prompt-block'>
<p>Four score and seven years ago our⏎</p>
<p>fathers brought forth on this continent,⏎</p>
<p>a new nation, conceived in Liberty, and⏎</p>
<p>dedicated to the proposition that all⏎</p>
<p>men are created equal.</p>
</div>
<p>Claude 3.5 Haiku is able to adapt to the line length for every value of <d-math>k</d-math>, predicting newlines at the correct positions with high probability by the third line (see <a href='#appendix-task-performance'>Appendix</a>).</p>
<p>All features in the main text of this paper are from a 10 million feature Weakly Causal Crosscoder (WCC) dictionary <d-cite key="lindsey2024crosscoders"></d-cite> trained on Claude 3.5 Haiku. Feature activation values are normalized to their max throughout.</p>
<h3><a id='char-count' href='#char-count'>Representing Character Count</a></h3>
<p>We define the <span style='font-style: italic;'>line character count</span> (or <span style='font-style: italic;'>character count</span>) at a given token in a prompt to be the total number of characters since the last newline, including the characters of the current token.</p>
<p>A natural thing to check is if the model linearly represents the character count as a quantitative variable: that is, can we predict character count with high accuracy via linear regression on the residual stream? Yes: a linear probe fit on the residual stream after layer 1 has an <d-math>R^2</d-math> of 0.985. This success does not mean, however, that the model actually represents the character count along a single line.</p>
<p>Instead, we find a multidimensional representation of the character count that we will analyze from four perspectives:</p>
<ol><li style='margin-left: 36pt;'>Sparse crosscoder features.<d-footnote>Each feature has an encoder, which acts as a linear + (Jump)ReLU probe on the residual stream, and a decoder. Ten features <d-math>f_1,\ldots,f_{10}</d-math> are associated with line character count. The model's estimate of the character count, given a residual stream vector <d-math>x</d-math>, is summarized by the set of activities of each of the 10 features <d-math>\{f_i(x)\}</d-math>.</d-footnote></li><li style='margin-left: 36pt;'>A low-dimensional subspace.<d-footnote>The model's estimate of the character count is summarized by the projection <d-math>\pi(x)</d-math> of <d-math>x</d-math> onto that subspace. Two datapoints have similar character counts if their projections are close in that subspace.</d-footnote></li><li style='margin-left: 36pt;'>A continuous 1-dimensional manifold contained in that low-dimensional subspace.<d-footnote>The model's estimate of the character count is summarized by the nearest point on the manifold to the projection of <d-math>x</d-math> into the subspace, and its confidence in that estimate by the magnitude of <d-math>\pi(x)</d-math>.</d-footnote></li><li style='margin-left: 36pt;'>A set of 150 logistic probes (corresponding to values of line character count from 1 to 150).<d-footnote>The model's estimate of the character count is summarized by the probability distribution given by the softmax of the probe activities, softmax<d-math>(Px)</d-math>.</d-footnote></li></ol>
<p>Each of these perspectives provides a complementary view of the same underlying object. The feature perspective is valuable for getting oriented, the subspace is perfect for causal intervention, the manifold is helpful for understanding how the representation is constructed and then manipulated to detect boundaries, and the logistic probes are useful for analyzing the OV and QK matrices of the individual attention heads involved.</p>
<h4>Character Count Features</h4>
<p>We begin with the features. In layers one and two, we found features that seemed to activate based on a token’s character position within a line. For example, in the attribution graph for the aluminum prompt, there were two features active on the final word “called” that seemed to fire when the line character count was between 35–55 and 45–65, respectively. To find more such features, we computed the mean activation of each feature binned by line character count. There were ten features with smooth profiles and large between-character-count variance, shown below: </p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_004.png' /><figcaption class='text-caption'>A family of features representing the current character count in a line of text. The tuning curve of the features’ activity increases at larger line character counts.</figcaption></figure>
<p>We find these features especially interesting as they are quite analogous to curve-detector features in vision models <d-cite key="cammarata2021curve,gorton2024missing"></d-cite> and place cells in biological brains <d-cite key="Moser2008PlaceCG"></d-cite>. In all three of these cases, a continuous variable is represented by a collection of discrete elements that activate for particular value ranges. Moreover, we also observe <span style='font-style: italic;'>dilation</span> of the receptive fields (i.e., subsequent features activate over increasingly large character ranges) which is a common characteristic of biological perception of numbers (e.g., <d-cite key="dehaene2003neural,piazza2004tuning"></d-cite>).</p>
<p>In the <a href='#appendix-feature-splitting'>Appendix</a>, we show these features are universal across dictionaries of different sizes, but that some feature splitting occurs with respect to the line width constraint.</p>
<h4>The Model Represents Character Count on a Continuous Manifold</h4>
<p>We observe that character count feature activations rise and fall at an offset, with two features being active at a time for most counts. This pattern suggests that the features are reconstructing a curved continuous manifold, locally parametrized by the activity of the two most active features. Given that their joint activation profiles follow a sinusoidal pattern, we expect reconstructions to lie on a curve between adjacent feature decoders.</p>
<p>To visualize this, we first compute the average layer 2 residual stream for each value of line character count on our synthetic dataset. We compute the PCA of these 150 vectors, and find that the top 6 components capture 95% of the variance; we project data to that 6 dimensional subspace which we call the “character count subspace” (top 3 PCs on the left below, next 3 PCs on the right). We observe the data form a twisting curve, resembling a helix from the perspective of PCs 1–3 and a more complex twist from the perspective of PCs 4–6.</p>
<p>We also reconstruct the residual stream for each datapoint using only the 10 character count features identified above, and compute the average <span style='font-style: italic;'>reconstructed</span> residual stream. We project the resulting curve, along with the feature decoders, into the same subspace. We find that the average line character count vectors are quite closely approximated by the feature reconstruction, though with mild kinks near the feature vectors themselves, reminiscent of a spline approximation of a smooth curve. While the 10 feature vectors discretize the curve, interpolating between the 2–3 neighboring features which are active at a time allows for a high-quality reconstruction of 150 data points.</p>
<figure key="feature_reconstruction" class="gdoc-image">
<script src="https://cdn.plot.ly/plotly-2.32.0.min.js" integrity="sha384-7TVmlZWH60iKX5Uk7lSvQhjtcgw2tkFjuwLcXoRSR4zXTyWFJRm9aPAguMh7CIra" crossorigin="anonymous"></script>
<div>                            <div id="4e335750-7652-47cf-aaa3-2a812cf48782" class="plotly-graph-div" style="height:600px; width:1000px;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("4e335750-7652-47cf-aaa3-2a812cf48782")) {                    Plotly.newPlot(                        "4e335750-7652-47cf-aaa3-2a812cf48782",                        [{"hovertemplate":"%{text}","legend":"legend","legendgroup":"Average Reconstructed Data","line":{"color":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130],"colorscale":[[0.0,"#440154"],[0.1111111111111111,"#482878"],[0.2222222222222222,"#3e4989"],[0.3333333333333333,"#31688e"],[0.4444444444444444,"#26828e"],[0.5555555555555556,"#1f9e89"],[0.6666666666666666,"#35b779"],[0.7777777777777778,"#6ece58"],[0.8888888888888888,"#b5de2b"],[1.0,"#fde725"]],"width":2},"marker":{"color":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130],"colorscale":[[0.0,"#440154"],[0.1111111111111111,"#482878"],[0.2222222222222222,"#3e4989"],[0.3333333333333333,"#31688e"],[0.4444444444444444,"#26828e"],[0.5555555555555556,"#1f9e89"],[0.6666666666666666,"#35b779"],[0.7777777777777778,"#6ece58"],[0.8888888888888888,"#b5de2b"],[1.0,"#fde725"]],"opacity":1,"showscale":false,"size":2,"symbol":"circle"},"mode":"lines+markers","name":"Average Reconstructed Data","showlegend":false,"text":["Average reconstruction for 20","Average reconstruction for 21","Average reconstruction for 22","Average reconstruction for 23","Average reconstruction for 24","Average reconstruction for 25","Average reconstruction for 26","Average reconstruction for 27","Average reconstruction for 28","Average reconstruction for 29","Average reconstruction for 30","Average reconstruction for 31","Average reconstruction for 32","Average reconstruction for 33","Average reconstruction for 34","Average reconstruction for 35","Average reconstruction for 36","Average reconstruction for 37","Average reconstruction for 38","Average reconstruction for 39","Average reconstruction for 40","Average reconstruction for 41","Average reconstruction for 42","Average reconstruction for 43","Average reconstruction for 44","Average reconstruction for 45","Average reconstruction for 46","Average reconstruction for 47","Average reconstruction for 48","Average reconstruction for 49","Average reconstruction for 50","Average reconstruction for 51","Average reconstruction for 52","Average reconstruction for 53","Average reconstruction for 54","Average reconstruction for 55","Average reconstruction for 56","Average reconstruction for 57","Average reconstruction for 58","Average reconstruction for 59","Average reconstruction for 60","Average reconstruction for 61","Average reconstruction for 62","Average reconstruction for 63","Average reconstruction for 64","Average reconstruction for 65","Average reconstruction for 66","Average reconstruction for 67","Average reconstruction for 68","Average reconstruction for 69","Average reconstruction for 70","Average reconstruction for 71","Average reconstruction for 72","Average reconstruction for 73","Average reconstruction for 74","Average reconstruction for 75","Average reconstruction for 76","Average reconstruction for 77","Average reconstruction for 78","Average reconstruction for 79","Average reconstruction for 80","Average reconstruction for 81","Average reconstruction for 82","Average reconstruction for 83","Average reconstruction for 84","Average reconstruction for 85","Average reconstruction for 86","Average reconstruction for 87","Average reconstruction for 88","Average reconstruction for 89","Average reconstruction for 90","Average reconstruction for 91","Average reconstruction for 92","Average reconstruction for 93","Average reconstruction for 94","Average reconstruction for 95","Average reconstruction for 96","Average reconstruction for 97","Average reconstruction for 98","Average reconstruction for 99","Average reconstruction for 100","Average reconstruction for 101","Average reconstruction for 102","Average reconstruction for 103","Average reconstruction for 104","Average reconstruction for 105","Average reconstruction for 106","Average reconstruction for 107","Average reconstruction for 108","Average reconstruction for 109","Average reconstruction for 110","Average reconstruction for 111","Average reconstruction for 112","Average reconstruction for 113","Average reconstruction for 114","Average reconstruction for 115","Average reconstruction for 116","Average reconstruction for 117","Average reconstruction for 118","Average reconstruction for 119","Average reconstruction for 120","Average reconstruction for 121","Average reconstruction for 122","Average reconstruction for 123","Average reconstruction for 124","Average reconstruction for 125","Average reconstruction for 126","Average reconstruction for 127","Average reconstruction for 128","Average reconstruction for 129","Average reconstruction for 130","Average reconstruction for 131","Average reconstruction for 132","Average reconstruction for 133","Average reconstruction for 134","Average reconstruction for 135","Average reconstruction for 136","Average reconstruction for 137","Average reconstruction for 138","Average reconstruction for 139","Average reconstruction for 140","Average reconstruction for 141","Average reconstruction for 142","Average reconstruction for 143","Average reconstruction for 144","Average reconstruction for 145","Average reconstruction for 146","Average reconstruction for 147","Average reconstruction for 148","Average reconstruction for 149","Average reconstruction for 150"],"x":[0.75,0.75,0.75,0.74,0.73,0.71,0.66,0.62,0.58,0.55,0.53,0.5,0.49,0.49,0.48,0.46,0.41,0.34,0.26,0.19,0.13,0.1,0.08,0.06,0.06,0.05,0.07,0.08,0.09,0.1,0.12,0.15,0.16,0.17,0.17,0.17,0.19,0.19,0.18,0.18,0.17,0.18,0.17,0.16,0.15,0.14,0.14,0.14,0.13,0.12,0.11,0.1,0.09,0.07,0.04,0.01,-0.02,-0.05,-0.1,-0.13,-0.17,-0.18,-0.21,-0.23,-0.24,-0.26,-0.26,-0.28,-0.3,-0.32,-0.36,-0.38,-0.42,-0.47,-0.52,-0.56,-0.58,-0.61,-0.63,-0.65,-0.66,-0.66,-0.66,-0.65,-0.65,-0.64,-0.63,-0.63,-0.63,-0.63,-0.63,-0.63,-0.63,-0.63,-0.64,-0.65,-0.65,-0.66,-0.67,-0.68,-0.69,-0.68,-0.68,-0.68,-0.67,-0.65,-0.63,-0.61,-0.58,-0.56,-0.54,-0.5,-0.48,-0.45,-0.44,-0.42,-0.39,-0.39,-0.39,-0.37,-0.37,-0.35,-0.33,-0.34,-0.33,-0.3,-0.3,-0.27,-0.25,-0.27,-0.24],"y":[0.26,0.25,0.23,0.21,0.19,0.15,0.11,0.07,0.04,0.01,-0.01,-0.04,-0.07,-0.1,-0.16,-0.25,-0.36,-0.48,-0.57,-0.63,-0.66,-0.68,-0.68,-0.68,-0.68,-0.67,-0.64,-0.61,-0.55,-0.47,-0.37,-0.27,-0.17,-0.08,-0.02,0.03,0.08,0.12,0.17,0.24,0.32,0.43,0.53,0.63,0.71,0.78,0.83,0.87,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.96,0.95,0.93,0.9,0.87,0.82,0.77,0.72,0.68,0.64,0.61,0.58,0.56,0.54,0.5,0.46,0.41,0.35,0.27,0.2,0.12,0.04,-0.04,-0.11,-0.18,-0.25,-0.3,-0.37,-0.4,-0.44,-0.47,-0.49,-0.51,-0.51,-0.52,-0.53,-0.54,-0.54,-0.55,-0.55,-0.56,-0.57,-0.57,-0.58,-0.58,-0.59,-0.58,-0.57,-0.56,-0.55,-0.53,-0.51,-0.49,-0.47,-0.45,-0.43,-0.41,-0.39,-0.38,-0.36,-0.35,-0.35,-0.34,-0.33,-0.33,-0.33,-0.32,-0.32,-0.32,-0.32,-0.32,-0.31,-0.31,-0.32,-0.31],"z":[-0.14,-0.16,-0.18,-0.22,-0.26,-0.31,-0.37,-0.41,-0.44,-0.46,-0.47,-0.48,-0.47,-0.46,-0.43,-0.38,-0.31,-0.21,-0.11,-0.02,0.05,0.1,0.15,0.19,0.23,0.29,0.36,0.45,0.55,0.67,0.76,0.84,0.89,0.92,0.93,0.94,0.94,0.93,0.93,0.92,0.9,0.85,0.79,0.72,0.65,0.56,0.48,0.42,0.36,0.33,0.3,0.27,0.24,0.2,0.15,0.09,0.01,-0.08,-0.17,-0.26,-0.35,-0.44,-0.51,-0.57,-0.61,-0.65,-0.68,-0.7,-0.71,-0.72,-0.72,-0.73,-0.74,-0.74,-0.74,-0.73,-0.72,-0.7,-0.68,-0.65,-0.62,-0.59,-0.56,-0.52,-0.5,-0.48,-0.46,-0.44,-0.42,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.34,-0.32,-0.28,-0.25,-0.2,-0.15,-0.11,-0.04,0.01,0.07,0.13,0.16,0.21,0.26,0.29,0.32,0.35,0.37,0.38,0.4,0.41,0.42,0.43,0.43,0.44,0.44,0.44,0.44,0.44,0.44,0.44,0.44,0.44,0.44,0.44],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","legend":"legend","legendgroup":"Average Reconstructed Data","marker":{"color":"rgba(255, 255, 255, 0.8)","line":{"color":"rgba(50, 50, 50, 1)","width":2},"showscale":false,"size":12,"symbol":"circle"},"mode":"markers","name":"Average Reconstructed Data","showlegend":true,"x":[null],"y":[null],"z":[null],"type":"scatter3d","scene":"scene"},{"hovertemplate":"%{text}","legend":"legend","legendgroup":"Average Data","line":{"color":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130],"colorscale":[[0.0,"#440154"],[0.1111111111111111,"#482878"],[0.2222222222222222,"#3e4989"],[0.3333333333333333,"#31688e"],[0.4444444444444444,"#26828e"],[0.5555555555555556,"#1f9e89"],[0.6666666666666666,"#35b779"],[0.7777777777777778,"#6ece58"],[0.8888888888888888,"#b5de2b"],[1.0,"#fde725"]],"width":2},"marker":{"color":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130],"colorscale":[[0.0,"#440154"],[0.1111111111111111,"#482878"],[0.2222222222222222,"#3e4989"],[0.3333333333333333,"#31688e"],[0.4444444444444444,"#26828e"],[0.5555555555555556,"#1f9e89"],[0.6666666666666666,"#35b779"],[0.7777777777777778,"#6ece58"],[0.8888888888888888,"#b5de2b"],[1.0,"#fde725"]],"opacity":0.2,"showscale":false,"size":2,"symbol":"square"},"mode":"lines+markers","name":"Average Data","showlegend":false,"text":["Mean activation for 20","Mean activation for 21","Mean activation for 22","Mean activation for 23","Mean activation for 24","Mean activation for 25","Mean activation for 26","Mean activation for 27","Mean activation for 28","Mean activation for 29","Mean activation for 30","Mean activation for 31","Mean activation for 32","Mean activation for 33","Mean activation for 34","Mean activation for 35","Mean activation for 36","Mean activation for 37","Mean activation for 38","Mean activation for 39","Mean activation for 40","Mean activation for 41","Mean activation for 42","Mean activation for 43","Mean activation for 44","Mean activation for 45","Mean activation for 46","Mean activation for 47","Mean activation for 48","Mean activation for 49","Mean activation for 50","Mean activation for 51","Mean activation for 52","Mean activation for 53","Mean activation for 54","Mean activation for 55","Mean activation for 56","Mean activation for 57","Mean activation for 58","Mean activation for 59","Mean activation for 60","Mean activation for 61","Mean activation for 62","Mean activation for 63","Mean activation for 64","Mean activation for 65","Mean activation for 66","Mean activation for 67","Mean activation for 68","Mean activation for 69","Mean activation for 70","Mean activation for 71","Mean activation for 72","Mean activation for 73","Mean activation for 74","Mean activation for 75","Mean activation for 76","Mean activation for 77","Mean activation for 78","Mean activation for 79","Mean activation for 80","Mean activation for 81","Mean activation for 82","Mean activation for 83","Mean activation for 84","Mean activation for 85","Mean activation for 86","Mean activation for 87","Mean activation for 88","Mean activation for 89","Mean activation for 90","Mean activation for 91","Mean activation for 92","Mean activation for 93","Mean activation for 94","Mean activation for 95","Mean activation for 96","Mean activation for 97","Mean activation for 98","Mean activation for 99","Mean activation for 100","Mean activation for 101","Mean activation for 102","Mean activation for 103","Mean activation for 104","Mean activation for 105","Mean activation for 106","Mean activation for 107","Mean activation for 108","Mean activation for 109","Mean activation for 110","Mean activation for 111","Mean activation for 112","Mean activation for 113","Mean activation for 114","Mean activation for 115","Mean activation for 116","Mean activation for 117","Mean activation for 118","Mean activation for 119","Mean activation for 120","Mean activation for 121","Mean activation for 122","Mean activation for 123","Mean activation for 124","Mean activation for 125","Mean activation for 126","Mean activation for 127","Mean activation for 128","Mean activation for 129","Mean activation for 130","Mean activation for 131","Mean activation for 132","Mean activation for 133","Mean activation for 134","Mean activation for 135","Mean activation for 136","Mean activation for 137","Mean activation for 138","Mean activation for 139","Mean activation for 140","Mean activation for 141","Mean activation for 142","Mean activation for 143","Mean activation for 144","Mean activation for 145","Mean activation for 146","Mean activation for 147","Mean activation for 148","Mean activation for 149","Mean activation for 150"],"x":[0.67,0.68,0.65,0.63,0.6,0.58,0.58,0.56,0.52,0.5,0.47,0.46,0.42,0.39,0.35,0.33,0.33,0.3,0.26,0.25,0.23,0.25,0.22,0.2,0.19,0.18,0.21,0.2,0.19,0.19,0.19,0.23,0.22,0.22,0.22,0.21,0.24,0.24,0.23,0.23,0.22,0.23,0.21,0.2,0.18,0.17,0.17,0.13,0.12,0.1,0.07,0.08,0.04,0.01,-0.01,-0.04,-0.05,-0.08,-0.12,-0.16,-0.2,-0.21,-0.25,-0.28,-0.32,-0.35,-0.36,-0.39,-0.44,-0.45,-0.51,-0.52,-0.54,-0.57,-0.59,-0.61,-0.63,-0.65,-0.68,-0.69,-0.7,-0.71,-0.74,-0.73,-0.76,-0.77,-0.76,-0.79,-0.79,-0.8,-0.79,-0.8,-0.79,-0.8,-0.79,-0.81,-0.79,-0.81,-0.81,-0.81,-0.8,-0.78,-0.8,-0.78,-0.77,-0.76,-0.75,-0.76,-0.74,-0.73,-0.74,-0.69,-0.71,-0.69,-0.7,-0.7,-0.65,-0.66,-0.66,-0.66,-0.65,-0.61,-0.61,-0.6,-0.6,-0.59,-0.58,-0.55,-0.53,-0.61,-0.56],"y":[0.35,0.35,0.35,0.34,0.33,0.3,0.24,0.19,0.15,0.09,0.02,-0.06,-0.13,-0.2,-0.27,-0.34,-0.41,-0.47,-0.51,-0.55,-0.58,-0.61,-0.62,-0.62,-0.62,-0.61,-0.58,-0.55,-0.5,-0.45,-0.39,-0.32,-0.25,-0.17,-0.09,-0.01,0.07,0.16,0.24,0.31,0.39,0.47,0.54,0.6,0.66,0.72,0.76,0.81,0.85,0.88,0.91,0.92,0.94,0.95,0.96,0.95,0.95,0.94,0.92,0.91,0.88,0.85,0.81,0.77,0.73,0.68,0.64,0.58,0.53,0.48,0.43,0.36,0.31,0.26,0.18,0.13,0.08,0.03,-0.01,-0.06,-0.11,-0.15,-0.18,-0.25,-0.26,-0.3,-0.34,-0.35,-0.38,-0.4,-0.43,-0.45,-0.47,-0.48,-0.5,-0.49,-0.51,-0.5,-0.5,-0.5,-0.5,-0.5,-0.48,-0.48,-0.48,-0.48,-0.46,-0.45,-0.45,-0.43,-0.41,-0.41,-0.37,-0.36,-0.34,-0.31,-0.32,-0.29,-0.27,-0.24,-0.23,-0.23,-0.21,-0.19,-0.17,-0.15,-0.13,-0.11,-0.09,-0.04,-0.04],"z":[-0.16,-0.23,-0.28,-0.33,-0.38,-0.43,-0.48,-0.51,-0.54,-0.56,-0.57,-0.57,-0.56,-0.54,-0.5,-0.46,-0.4,-0.34,-0.27,-0.19,-0.12,-0.03,0.05,0.14,0.23,0.31,0.4,0.48,0.56,0.63,0.69,0.74,0.8,0.84,0.87,0.89,0.89,0.89,0.89,0.87,0.85,0.81,0.77,0.73,0.69,0.63,0.57,0.52,0.46,0.39,0.33,0.25,0.19,0.12,0.06,-0.01,-0.08,-0.14,-0.2,-0.25,-0.31,-0.37,-0.42,-0.46,-0.5,-0.54,-0.58,-0.62,-0.63,-0.66,-0.66,-0.68,-0.69,-0.69,-0.69,-0.68,-0.68,-0.66,-0.64,-0.62,-0.6,-0.58,-0.54,-0.52,-0.48,-0.45,-0.43,-0.39,-0.36,-0.32,-0.29,-0.26,-0.23,-0.19,-0.16,-0.12,-0.09,-0.05,-0.02,0.01,0.04,0.07,0.1,0.13,0.15,0.18,0.21,0.22,0.25,0.27,0.29,0.31,0.33,0.35,0.36,0.36,0.38,0.39,0.4,0.41,0.41,0.42,0.42,0.43,0.43,0.43,0.43,0.44,0.43,0.41,0.41],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","legend":"legend","legendgroup":"Average Data","marker":{"color":"rgba(255, 255, 255, 0.8)","line":{"color":"rgba(50, 50, 50, 1)","width":2},"showscale":false,"size":12,"symbol":"square"},"mode":"markers","name":"Average Data","showlegend":true,"x":[null],"y":[null],"z":[null],"type":"scatter3d","scene":"scene"},{"hovertemplate":"%{text}","legend":"legend","legendgroup":"Feature Decoders","marker":{"color":[31,32,44,55,70,85,107,135],"colorscale":[[0.0,"#440154"],[0.1111111111111111,"#482878"],[0.2222222222222222,"#3e4989"],[0.3333333333333333,"#31688e"],[0.4444444444444444,"#26828e"],[0.5555555555555556,"#1f9e89"],[0.6666666666666666,"#35b779"],[0.7777777777777778,"#6ece58"],[0.8888888888888888,"#b5de2b"],[1.0,"#fde725"]],"opacity":1,"showscale":false,"size":4,"symbol":"cross"},"mode":"markers","name":"Feature Decoders","showlegend":false,"text":["31","32","44","55","70","85","107","135"],"x":[0.09,0.5,-0.13,0.01,0.01,-0.32,-0.66,-0.5],"y":[-0.01,-0.12,-0.73,0.01,0.88,0.6,-0.44,-0.27],"z":[-0.64,-0.59,0.2,0.94,0.33,-0.65,-0.4,0.38],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","legend":"legend","legendgroup":"Feature Decoders","marker":{"color":"rgba(255, 255, 255, 0.8)","line":{"color":"rgba(50, 50, 50, 1)","width":2},"showscale":false,"size":12,"symbol":"cross"},"mode":"markers","name":"Feature Decoders","showlegend":true,"x":[null],"y":[null],"z":[null],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.5,0.09000000357627869],"y":[-0.03999999910593033,-0.009999999776482582],"z":[-0.47999998927116394,-0.6399999856948853],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.49000000953674316,0.5],"y":[-0.07000000029802322,-0.11999999731779099],"z":[-0.4699999988079071,-0.5899999737739563],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.05999999865889549,-0.12999999523162842],"y":[-0.6800000071525574,-0.7300000190734863],"z":[0.23000000417232513,0.20000000298023224],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.17000000178813934,0.009999999776482582],"y":[0.029999999329447746,0.009999999776482582],"z":[0.9399999976158142,0.9399999976158142],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.10999999940395355,0.009999999776482582],"y":[0.9100000262260437,0.8799999952316284],"z":[0.30000001192092896,0.33000001311302185],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.25999999046325684,-0.3199999928474426],"y":[0.6399999856948853,0.6000000238418579],"z":[-0.6499999761581421,-0.6499999761581421],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.6299999952316284,-0.6600000262260437],"y":[-0.49000000953674316,-0.4399999976158142],"z":[-0.4399999976158142,-0.4000000059604645],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.41999998688697815,-0.5],"y":[-0.36000001430511475,-0.27000001072883606],"z":[0.4000000059604645,0.3799999952316284],"type":"scatter3d","scene":"scene"},{"hovertemplate":"%{text}","legendgroup":"Average Reconstructed Data","line":{"color":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130],"colorscale":[[0.0,"#440154"],[0.1111111111111111,"#482878"],[0.2222222222222222,"#3e4989"],[0.3333333333333333,"#31688e"],[0.4444444444444444,"#26828e"],[0.5555555555555556,"#1f9e89"],[0.6666666666666666,"#35b779"],[0.7777777777777778,"#6ece58"],[0.8888888888888888,"#b5de2b"],[1.0,"#fde725"]],"width":2},"marker":{"color":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130],"colorscale":[[0.0,"#440154"],[0.1111111111111111,"#482878"],[0.2222222222222222,"#3e4989"],[0.3333333333333333,"#31688e"],[0.4444444444444444,"#26828e"],[0.5555555555555556,"#1f9e89"],[0.6666666666666666,"#35b779"],[0.7777777777777778,"#6ece58"],[0.8888888888888888,"#b5de2b"],[1.0,"#fde725"]],"opacity":1,"showscale":false,"size":2,"symbol":"circle"},"mode":"lines+markers","name":"Average Reconstructed Data","showlegend":false,"text":["Average reconstruction for 20","Average reconstruction for 21","Average reconstruction for 22","Average reconstruction for 23","Average reconstruction for 24","Average reconstruction for 25","Average reconstruction for 26","Average reconstruction for 27","Average reconstruction for 28","Average reconstruction for 29","Average reconstruction for 30","Average reconstruction for 31","Average reconstruction for 32","Average reconstruction for 33","Average reconstruction for 34","Average reconstruction for 35","Average reconstruction for 36","Average reconstruction for 37","Average reconstruction for 38","Average reconstruction for 39","Average reconstruction for 40","Average reconstruction for 41","Average reconstruction for 42","Average reconstruction for 43","Average reconstruction for 44","Average reconstruction for 45","Average reconstruction for 46","Average reconstruction for 47","Average reconstruction for 48","Average reconstruction for 49","Average reconstruction for 50","Average reconstruction for 51","Average reconstruction for 52","Average reconstruction for 53","Average reconstruction for 54","Average reconstruction for 55","Average reconstruction for 56","Average reconstruction for 57","Average reconstruction for 58","Average reconstruction for 59","Average reconstruction for 60","Average reconstruction for 61","Average reconstruction for 62","Average reconstruction for 63","Average reconstruction for 64","Average reconstruction for 65","Average reconstruction for 66","Average reconstruction for 67","Average reconstruction for 68","Average reconstruction for 69","Average reconstruction for 70","Average reconstruction for 71","Average reconstruction for 72","Average reconstruction for 73","Average reconstruction for 74","Average reconstruction for 75","Average reconstruction for 76","Average reconstruction for 77","Average reconstruction for 78","Average reconstruction for 79","Average reconstruction for 80","Average reconstruction for 81","Average reconstruction for 82","Average reconstruction for 83","Average reconstruction for 84","Average reconstruction for 85","Average reconstruction for 86","Average reconstruction for 87","Average reconstruction for 88","Average reconstruction for 89","Average reconstruction for 90","Average reconstruction for 91","Average reconstruction for 92","Average reconstruction for 93","Average reconstruction for 94","Average reconstruction for 95","Average reconstruction for 96","Average reconstruction for 97","Average reconstruction for 98","Average reconstruction for 99","Average reconstruction for 100","Average reconstruction for 101","Average reconstruction for 102","Average reconstruction for 103","Average reconstruction for 104","Average reconstruction for 105","Average reconstruction for 106","Average reconstruction for 107","Average reconstruction for 108","Average reconstruction for 109","Average reconstruction for 110","Average reconstruction for 111","Average reconstruction for 112","Average reconstruction for 113","Average reconstruction for 114","Average reconstruction for 115","Average reconstruction for 116","Average reconstruction for 117","Average reconstruction for 118","Average reconstruction for 119","Average reconstruction for 120","Average reconstruction for 121","Average reconstruction for 122","Average reconstruction for 123","Average reconstruction for 124","Average reconstruction for 125","Average reconstruction for 126","Average reconstruction for 127","Average reconstruction for 128","Average reconstruction for 129","Average reconstruction for 130","Average reconstruction for 131","Average reconstruction for 132","Average reconstruction for 133","Average reconstruction for 134","Average reconstruction for 135","Average reconstruction for 136","Average reconstruction for 137","Average reconstruction for 138","Average reconstruction for 139","Average reconstruction for 140","Average reconstruction for 141","Average reconstruction for 142","Average reconstruction for 143","Average reconstruction for 144","Average reconstruction for 145","Average reconstruction for 146","Average reconstruction for 147","Average reconstruction for 148","Average reconstruction for 149","Average reconstruction for 150"],"x":[-0.31,-0.32,-0.34,-0.37,-0.41,-0.45,-0.49,-0.51,-0.53,-0.55,-0.56,-0.57,-0.57,-0.58,-0.6,-0.61,-0.61,-0.59,-0.55,-0.5,-0.46,-0.43,-0.4,-0.37,-0.35,-0.33,-0.32,-0.29,-0.25,-0.19,-0.14,-0.1,-0.05,-0.01,0.02,0.04,0.05,0.06,0.08,0.09,0.11,0.13,0.15,0.17,0.18,0.19,0.19,0.19,0.19,0.2,0.2,0.2,0.2,0.21,0.21,0.22,0.22,0.22,0.22,0.22,0.21,0.19,0.18,0.17,0.16,0.15,0.14,0.14,0.14,0.14,0.15,0.15,0.16,0.16,0.17,0.17,0.17,0.17,0.16,0.16,0.15,0.14,0.13,0.12,0.11,0.11,0.1,0.09,0.09,0.09,0.09,0.08,0.08,0.08,0.08,0.08,0.07,0.07,0.08,0.07,0.08,0.07,0.07,0.07,0.06,0.06,0.05,0.05,0.04,0.04,0.03,0.02,0.02,0.01,0.01,0.01,-0.0,-0.01,-0.0,-0.01,-0.01,-0.02,-0.02,-0.02,-0.03,-0.04,-0.04,-0.05,-0.07,-0.06,-0.07],"y":[-0.36,-0.36,-0.33,-0.3,-0.25,-0.2,-0.15,-0.09,-0.04,-0.0,0.03,0.06,0.08,0.11,0.16,0.23,0.32,0.42,0.49,0.53,0.56,0.57,0.58,0.58,0.59,0.59,0.58,0.58,0.55,0.52,0.47,0.41,0.36,0.3,0.26,0.24,0.22,0.21,0.2,0.19,0.18,0.16,0.13,0.11,0.09,0.06,0.04,0.03,0.02,0.01,0.01,0.02,0.02,0.03,0.04,0.06,0.08,0.1,0.12,0.13,0.15,0.17,0.18,0.19,0.19,0.2,0.2,0.2,0.2,0.2,0.19,0.19,0.18,0.17,0.16,0.15,0.13,0.12,0.1,0.08,0.07,0.05,0.03,0.01,0.0,-0.01,-0.02,-0.03,-0.04,-0.04,-0.05,-0.06,-0.07,-0.08,-0.1,-0.11,-0.14,-0.16,-0.19,-0.22,-0.26,-0.31,-0.34,-0.39,-0.42,-0.46,-0.5,-0.51,-0.54,-0.56,-0.57,-0.59,-0.6,-0.6,-0.61,-0.61,-0.62,-0.62,-0.62,-0.62,-0.62,-0.63,-0.63,-0.63,-0.63,-0.64,-0.64,-0.65,-0.65,-0.65,-0.65],"z":[0.06,0.07,0.08,0.11,0.14,0.17,0.21,0.24,0.26,0.28,0.29,0.29,0.29,0.29,0.28,0.26,0.22,0.17,0.12,0.07,0.03,0.01,-0.01,-0.03,-0.04,-0.05,-0.06,-0.07,-0.08,-0.09,-0.09,-0.09,-0.09,-0.09,-0.08,-0.08,-0.07,-0.07,-0.06,-0.05,-0.03,-0.01,0.01,0.04,0.06,0.08,0.1,0.11,0.11,0.12,0.12,0.12,0.11,0.11,0.1,0.09,0.08,0.07,0.05,0.04,0.02,0.01,-0.01,-0.02,-0.03,-0.04,-0.05,-0.06,-0.07,-0.08,-0.09,-0.11,-0.13,-0.15,-0.18,-0.2,-0.22,-0.24,-0.26,-0.27,-0.28,-0.29,-0.3,-0.3,-0.3,-0.31,-0.3,-0.3,-0.3,-0.3,-0.3,-0.29,-0.28,-0.28,-0.26,-0.26,-0.24,-0.22,-0.2,-0.17,-0.14,-0.1,-0.07,-0.02,0.02,0.07,0.11,0.13,0.17,0.21,0.23,0.26,0.28,0.3,0.31,0.32,0.34,0.34,0.34,0.35,0.35,0.36,0.37,0.37,0.37,0.39,0.39,0.4,0.41,0.4,0.41],"type":"scatter3d","scene":"scene2"},{"hovertemplate":"%{text}","legendgroup":"Average Data","line":{"color":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130],"colorscale":[[0.0,"#440154"],[0.1111111111111111,"#482878"],[0.2222222222222222,"#3e4989"],[0.3333333333333333,"#31688e"],[0.4444444444444444,"#26828e"],[0.5555555555555556,"#1f9e89"],[0.6666666666666666,"#35b779"],[0.7777777777777778,"#6ece58"],[0.8888888888888888,"#b5de2b"],[1.0,"#fde725"]],"width":2},"marker":{"color":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130],"colorscale":[[0.0,"#440154"],[0.1111111111111111,"#482878"],[0.2222222222222222,"#3e4989"],[0.3333333333333333,"#31688e"],[0.4444444444444444,"#26828e"],[0.5555555555555556,"#1f9e89"],[0.6666666666666666,"#35b779"],[0.7777777777777778,"#6ece58"],[0.8888888888888888,"#b5de2b"],[1.0,"#fde725"]],"opacity":0.2,"showscale":false,"size":2,"symbol":"square"},"mode":"lines+markers","name":"Average Data","showlegend":false,"text":["Mean activation for 20","Mean activation for 21","Mean activation for 22","Mean activation for 23","Mean activation for 24","Mean activation for 25","Mean activation for 26","Mean activation for 27","Mean activation for 28","Mean activation for 29","Mean activation for 30","Mean activation for 31","Mean activation for 32","Mean activation for 33","Mean activation for 34","Mean activation for 35","Mean activation for 36","Mean activation for 37","Mean activation for 38","Mean activation for 39","Mean activation for 40","Mean activation for 41","Mean activation for 42","Mean activation for 43","Mean activation for 44","Mean activation for 45","Mean activation for 46","Mean activation for 47","Mean activation for 48","Mean activation for 49","Mean activation for 50","Mean activation for 51","Mean activation for 52","Mean activation for 53","Mean activation for 54","Mean activation for 55","Mean activation for 56","Mean activation for 57","Mean activation for 58","Mean activation for 59","Mean activation for 60","Mean activation for 61","Mean activation for 62","Mean activation for 63","Mean activation for 64","Mean activation for 65","Mean activation for 66","Mean activation for 67","Mean activation for 68","Mean activation for 69","Mean activation for 70","Mean activation for 71","Mean activation for 72","Mean activation for 73","Mean activation for 74","Mean activation for 75","Mean activation for 76","Mean activation for 77","Mean activation for 78","Mean activation for 79","Mean activation for 80","Mean activation for 81","Mean activation for 82","Mean activation for 83","Mean activation for 84","Mean activation for 85","Mean activation for 86","Mean activation for 87","Mean activation for 88","Mean activation for 89","Mean activation for 90","Mean activation for 91","Mean activation for 92","Mean activation for 93","Mean activation for 94","Mean activation for 95","Mean activation for 96","Mean activation for 97","Mean activation for 98","Mean activation for 99","Mean activation for 100","Mean activation for 101","Mean activation for 102","Mean activation for 103","Mean activation for 104","Mean activation for 105","Mean activation for 106","Mean activation for 107","Mean activation for 108","Mean activation for 109","Mean activation for 110","Mean activation for 111","Mean activation for 112","Mean activation for 113","Mean activation for 114","Mean activation for 115","Mean activation for 116","Mean activation for 117","Mean activation for 118","Mean activation for 119","Mean activation for 120","Mean activation for 121","Mean activation for 122","Mean activation for 123","Mean activation for 124","Mean activation for 125","Mean activation for 126","Mean activation for 127","Mean activation for 128","Mean activation for 129","Mean activation for 130","Mean activation for 131","Mean activation for 132","Mean activation for 133","Mean activation for 134","Mean activation for 135","Mean activation for 136","Mean activation for 137","Mean activation for 138","Mean activation for 139","Mean activation for 140","Mean activation for 141","Mean activation for 142","Mean activation for 143","Mean activation for 144","Mean activation for 145","Mean activation for 146","Mean activation for 147","Mean activation for 148","Mean activation for 149","Mean activation for 150"],"x":[-0.37,-0.38,-0.42,-0.45,-0.48,-0.5,-0.51,-0.54,-0.57,-0.59,-0.61,-0.61,-0.63,-0.63,-0.64,-0.63,-0.61,-0.59,-0.59,-0.56,-0.53,-0.49,-0.48,-0.46,-0.42,-0.39,-0.34,-0.31,-0.29,-0.24,-0.21,-0.16,-0.13,-0.09,-0.06,-0.04,0.0,0.03,0.06,0.08,0.09,0.12,0.13,0.15,0.15,0.16,0.19,0.18,0.18,0.19,0.19,0.22,0.21,0.21,0.21,0.22,0.22,0.22,0.21,0.2,0.2,0.21,0.21,0.21,0.21,0.21,0.21,0.21,0.21,0.21,0.18,0.2,0.19,0.19,0.2,0.19,0.19,0.19,0.18,0.18,0.18,0.18,0.16,0.18,0.16,0.16,0.18,0.16,0.16,0.15,0.16,0.15,0.16,0.15,0.17,0.14,0.15,0.13,0.13,0.1,0.12,0.13,0.1,0.1,0.12,0.12,0.11,0.11,0.1,0.09,0.08,0.11,0.08,0.07,0.06,0.06,0.08,0.06,0.03,0.01,0.03,0.06,0.06,0.02,-0.0,0.01,0.01,-0.02,-0.07,-0.1,-0.06],"y":[-0.48,-0.44,-0.41,-0.38,-0.34,-0.29,-0.22,-0.16,-0.1,-0.04,0.03,0.1,0.16,0.23,0.29,0.34,0.39,0.44,0.48,0.51,0.54,0.56,0.57,0.58,0.59,0.59,0.58,0.57,0.56,0.54,0.52,0.49,0.46,0.44,0.41,0.37,0.35,0.32,0.29,0.26,0.24,0.22,0.2,0.18,0.17,0.16,0.15,0.15,0.14,0.14,0.13,0.14,0.14,0.15,0.15,0.16,0.17,0.18,0.19,0.19,0.2,0.22,0.23,0.23,0.24,0.25,0.26,0.26,0.26,0.26,0.26,0.26,0.26,0.25,0.24,0.23,0.22,0.21,0.2,0.18,0.16,0.15,0.13,0.11,0.08,0.07,0.05,0.03,-0.0,-0.02,-0.04,-0.06,-0.09,-0.11,-0.13,-0.15,-0.18,-0.2,-0.22,-0.24,-0.26,-0.27,-0.29,-0.32,-0.33,-0.35,-0.36,-0.36,-0.39,-0.4,-0.4,-0.42,-0.42,-0.44,-0.44,-0.43,-0.45,-0.45,-0.45,-0.46,-0.45,-0.46,-0.45,-0.46,-0.46,-0.45,-0.43,-0.44,-0.43,-0.39,-0.39],"z":[0.04,0.08,0.12,0.15,0.18,0.22,0.24,0.25,0.26,0.27,0.26,0.27,0.25,0.24,0.22,0.2,0.18,0.16,0.13,0.1,0.08,0.06,0.03,0.0,-0.02,-0.03,-0.04,-0.05,-0.08,-0.08,-0.09,-0.08,-0.09,-0.08,-0.08,-0.09,-0.06,-0.05,-0.04,-0.03,-0.02,-0.0,0.01,0.02,0.02,0.03,0.06,0.06,0.06,0.07,0.07,0.09,0.08,0.08,0.07,0.08,0.08,0.07,0.05,0.03,0.03,0.02,0.0,-0.01,-0.02,-0.04,-0.06,-0.09,-0.09,-0.12,-0.14,-0.14,-0.17,-0.18,-0.19,-0.2,-0.22,-0.23,-0.24,-0.24,-0.24,-0.24,-0.25,-0.24,-0.25,-0.24,-0.23,-0.23,-0.21,-0.21,-0.2,-0.2,-0.17,-0.17,-0.13,-0.13,-0.11,-0.1,-0.08,-0.09,-0.04,-0.02,-0.01,0.01,0.05,0.08,0.08,0.1,0.12,0.14,0.15,0.2,0.19,0.22,0.22,0.25,0.28,0.28,0.28,0.28,0.31,0.32,0.36,0.33,0.34,0.34,0.36,0.35,0.31,0.27,0.34],"type":"scatter3d","scene":"scene2"},{"hovertemplate":"%{text}","legendgroup":"Feature Decoders","marker":{"color":[31,32,44,55,70,85,107,135],"colorscale":[[0.0,"#440154"],[0.1111111111111111,"#482878"],[0.2222222222222222,"#3e4989"],[0.3333333333333333,"#31688e"],[0.4444444444444444,"#26828e"],[0.5555555555555556,"#1f9e89"],[0.6666666666666666,"#35b779"],[0.7777777777777778,"#6ece58"],[0.8888888888888888,"#b5de2b"],[1.0,"#fde725"]],"opacity":1,"showscale":false,"size":4,"symbol":"cross"},"mode":"markers","name":"Feature Decoders","showlegend":false,"text":["31","32","44","55","70","85","107","135"],"x":[-0.51,-0.47,-0.23,0.16,0.29,0.23,0.19,0.15],"y":[-0.04,0.04,0.57,0.17,-0.06,0.13,-0.08,-0.62],"z":[0.34,0.25,-0.1,-0.13,0.09,-0.07,-0.32,0.26],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.5699999928474426,-0.5099999904632568],"y":[0.05999999865889549,-0.03999999910593033],"z":[0.28999999165534973,0.3400000035762787],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.5699999928474426,-0.4699999988079071],"y":[0.07999999821186066,0.03999999910593033],"z":[0.28999999165534973,0.25],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.3499999940395355,-0.23000000417232513],"y":[0.5899999737739563,0.5699999928474426],"z":[-0.03999999910593033,-0.10000000149011612],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.03999999910593033,0.1599999964237213],"y":[0.23999999463558197,0.17000000178813934],"z":[-0.07999999821186066,-0.12999999523162842],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.20000000298023224,0.28999999165534973],"y":[0.009999999776482582,-0.05999999865889549],"z":[0.11999999731779099,0.09000000357627869],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.15000000596046448,0.23000000417232513],"y":[0.20000000298023224,0.12999999523162842],"z":[-0.03999999910593033,-0.07000000029802322],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.09000000357627869,0.1899999976158142],"y":[-0.029999999329447746,-0.07999999821186066],"z":[-0.30000001192092896,-0.3199999928474426],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.009999999776482582,0.15000000596046448],"y":[-0.6100000143051147,-0.6200000047683716],"z":[0.3199999928474426,0.25999999046325684],"type":"scatter3d","scene":"scene2"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"scene":{"domain":{"x":[0.0,0.475],"y":[0.0,1.0]},"xaxis":{"title":{"text":""},"range":[-0.91,0.85],"showticklabels":false,"showgrid":false,"backgroundcolor":"rgba(255, 255, 255, 0)","gridcolor":"rgba(255, 255, 255, 0)","showbackground":false,"showspikes":false},"yaxis":{"title":{"text":""},"range":[-0.8300000190734863,1.06],"showticklabels":false,"showgrid":false,"backgroundcolor":"rgba(255, 255, 255, 0)","gridcolor":"rgba(255, 255, 255, 0)","showbackground":false,"showspikes":false},"zaxis":{"title":{"text":""},"range":[-0.8400000095367431,1.0399999976158143],"showticklabels":false,"showgrid":false,"backgroundcolor":"rgba(255, 255, 255, 0)","gridcolor":"rgba(255, 255, 255, 0)","showbackground":false,"showspikes":false},"camera":{"eye":{"x":0.0,"y":1.2,"z":0.8},"center":{"x":0,"y":0,"z":0},"up":{"x":-1.0,"y":0.0,"z":0.0}},"dragmode":"orbit","aspectmode":"cube"},"scene2":{"domain":{"x":[0.525,1.0],"y":[0.0,1.0]},"xaxis":{"title":{"text":""},"range":[-0.74,0.3899999916553497],"showticklabels":false,"showgrid":false,"backgroundcolor":"rgba(255, 255, 255, 0)","gridcolor":"rgba(255, 255, 255, 0)","showbackground":false,"showspikes":false},"yaxis":{"title":{"text":""},"range":[-0.7499999761581421,0.69],"showticklabels":false,"showgrid":false,"backgroundcolor":"rgba(255, 255, 255, 0)","gridcolor":"rgba(255, 255, 255, 0)","showbackground":false,"showspikes":false},"zaxis":{"title":{"text":""},"range":[-0.4199999928474426,0.5099999964237213],"showticklabels":false,"showgrid":false,"backgroundcolor":"rgba(255, 255, 255, 0)","gridcolor":"rgba(255, 255, 255, 0)","showbackground":false,"showspikes":false},"camera":{"eye":{"x":0,"y":1.2,"z":1.2},"center":{"x":0,"y":0,"z":0},"up":{"x":1,"y":0,"z":0}},"dragmode":"orbit","aspectmode":"cube"},"annotations":[{"font":{"size":16},"showarrow":false,"text":"First 3 PC","x":0.2375,"xanchor":"center","xref":"paper","y":0.95,"yanchor":"bottom","yref":"paper"},{"font":{"size":16},"showarrow":false,"text":"Next 3 PC","x":0.7625,"xanchor":"center","xref":"paper","y":0.95,"yanchor":"bottom","yref":"paper"}],"title":{"text":"\u003cb\u003eFeature Reconstruction of Mean Activations\u003c\u002fb\u003e","x":0.5,"xanchor":"center","y":0.98},"margin":{"l":0,"r":0,"t":30,"b":0},"legend":{"itemsizing":"constant","orientation":"h","x":0.5,"xanchor":"center","y":0.02,"yanchor":"bottom"},"width":1000,"height":600,"showlegend":true,"font":{"family":"-apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, Oxygen, Ubuntu, Cantarell, \"Fira Sans\", \"Droid Sans\", \"Helvetica Neue\", Arial, sans-serif","color":"rgba(0, 0, 0, 0.8)"}},                        {"displayModeBar": false, "responsive": true}                    )                };                            </script>        </div>
<figcaption class='text-caption'>Character count is represented on a manifold in a 6 dimensional subspace (jagged line). This manifold can be approximately locally parametrized by the features we identified (crosses). </figcaption></figure>
<h4>Validation: The Character Count Subspace is Causal</h4>
<p>To validate our interpretation of the character count subspace, we perform a coarse-grained ablation and a fine-grained intervention.</p>
<p><span style='font-weight: 700;'>Ablation Experiment.</span>  For our ablation experiment, we zero ablate (from a single early layer) a <d-math>k</d-math>-dimensional subspace corresponding to the top <d-math>k</d-math> principal components of the per–character count mean activations and compare to a baseline of ablating a random <d-math>k</d-math>-dimensional subspace. Below we measure the loss effect, broken down by newlines and nonnewlines.<d-footnote>Note, in general one should not assume that a subspace spanned by features (or a PCA) is dedicated to those features because it could be in superposition with many other features. However, because in this case the character count subspace is densely active (and therefore less amenable to being in superposition), this experimental design is more justified.</d-footnote></p>
<figure class="gdoc-image" style="--img-width: 1580px; "><img src='img_005.png' /><figcaption class='text-caption'>Ablating the character count subspace has a large effect only when the next token is a newline.</figcaption></figure>
<p><span style='font-weight: 700;'>Intervention Experiment.  </span>As a more surgical intervention, we perform an experiment to modify the perceived character count at the end of the aluminum prompt (originally 42 characters). Specifically, we sweep over character counts <d-math>c</d-math>, and substitute the mean activation across all tokens in our dataset with count <d-math>c</d-math>. That is, <d-math>a_{\text{patched}} = a_{\text{original}} - \mu_{\text{original}} + \mu_{c}</d-math> for activation <d-math>a</d-math> and average activation matrix <d-math>\mu</d-math>. We perform this intervention for three adjacent early layers and the last two tokens for both the entire mean vector and within the 6 dimensional PCA space of the mean vectors. <d-footnote>The attribution graph has several positional features and edges on both the last token (“called”) as well as the second-to-last token (“also”). We change the “also” count representation to be 6 characters prior to that for the final token, to maintain consistency.</d-footnote></p>
<figure class="gdoc-image" style="--img-width: 1580px; "><img src='img_006.png' /><figcaption class='text-caption'>Intervening on a rank 6 subspace is sufficient to change the model’s linebreaking behavior.</figcaption></figure>
<h4>The Probe Perspective</h4>
<p>We also train supervised logistic regression probes to predict character count.<d-footnote>as a 150-way multiclass classification problem</d-footnote> Probes trained after layer 1 achieve a root mean squared error of 5, indicating some intrinsic noise in the character count representation — which is consistent with our features having relatively wide receptive fields. Performing PCA on the 150 probe weight vectors, we find that 6 components capture 82% of the variance.</p>
<p>When we look at the average responses of each probe to tokens with different line character counts, we see a striking pattern. In addition to a diagonal band (probes, like the sparse features, have increasingly wide receptive fields), we see two faint off-diagonal bands on each side! The response curve of each probe is not monotonically decreasing away from its max, but rebounds. This “ringing” turns out to be a natural consequence of embedding a “rippled” manifold into low dimensions.</p>
<figure class="gdoc-image" style="--img-width: 1571px; "><img src='img_007.png' /><figcaption class='text-caption'>Response curve of Line Character Count probes as a function of Line Character Count show widening receptive fields and a "ringing" pattern of off-diagonal stripes.</figcaption></figure>
<h4><a id='rippled-representations' href='#rippled-representations'>Rippled Representations are Optimal</a></h4>
<p>We note that the cosine similarities of the mean activation vector (which form the helix-like curve visualized in PCA space above), the linear probe vectors, and feature decoder vectors all exhibit similar ringing patterns to the above figure.<d-footnote>We use the term “<a href='https://en.wikipedia.org/wiki/Ringing_(signal)'>ringing</a>” in the sense of signal processing, a transient oscillation in response to a sharp peak, such as in the Gibbs Phenomenon).</d-footnote> Note that not only are neighboring features not orthogonal, features further away have negative similarities, and then those even further away have positive ones again.</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_008.png' /></figure>
<p>This structure turns out to be a natural consequence of having the desired pattern of similarity, trivially achievable in 150 dimensions, projected down to low dimensions. As a toy model of this, suppose that we wish to have a discretized circle's worth of unit vectors, each similar to its neighbors but orthogonal to those further away. This can be realized by a symmetric set of unit vectors in 150 dimensions with cosine similarity matrix <d-math>X</d-math> pictured below (left). Projecting this to its top 5 eigenvectors yields a 5-dimensional embedding of the same vectors with cosine similarity matrix (below right) exhibiting ringing. We also plot the curve these vectors form in the top 3 eigenvectors. We can think of the original 150-dimensional embedding of the circle as being highly curved, and the resulting 5-dimensional embedding as retaining as much of that curvature as possible. This manifests as ripples in the embedding of the circle when viewed in a 3D projection. A relationship of this construction to Fourier features is discussed in the <a href='#appendix-gibbs'>appendix</a>.</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_009.png' /><figcaption class='text-caption'>Left panel shows an ideal similarity matrix for vectors representing points along a circle. Middle panel shows the optimal (PCA) approximation possible when embedding the points in 5 dimensions. Right panel shows the resulting projection of the circle to the top 3 dimensions, exhibiting rippling.</figcaption></figure>
<p>Alternatively, one can view the ringing from the perspective of sparse feature decoders as a kind of interference weight <d-cite key="olah2025toy"></d-cite>. With no capacity constraints, the model might use orthogonal vectors to represent the quantitative response of each feature, with its own receptive field, to the input data. Forced to put them into lower dimensional superposition, the similarity matrix picks up both a wider diagonal stripe and the upper/lower diagonal ringing stripes.</p>
<p>Finally, we also construct a simple physical model showing that the rippling and ringing arise even when the solution is found dynamically, whenever many vectors are packed into a small number of dimensions. Below, we show the result of a simulation in which 100 points confined to a <d-math>6</d-math>-dimensional hypersphere are subjected to attractive forces to their 6 closest neighbors on each side (matching the RMSE error of our probes) and repulsive forces to all other points. (To avoid boundary conditions, we use the topology of a circle instead of an interval.) On the right below is a heatmap exhibiting two rings, and on the left is a 3-dimensional projection of the 6-dimensional curve. This simulation is interactive, and the reader is encouraged to experiment with reinitializing the points (↺), switching the ambient dimension, and modifying the width of the attractive zone. Decreasing the attractive zone or increasing the embedding dimension both increase curvature (and the amount of ringing), and vice versa.<d-footnote>The simulation can sometimes find itself in local minima. Increasing the width of the attractive zone before decreasing it again usually solves this issue.</d-footnote> As the number of points on the curve grows and the attractive zone width shrinks (in relative terms), the curvature grows quite extreme, approaching a space-filling curve in the limit.</p>
<figure key="dynamical_simulation" class="gdoc-image">
<!doctype html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <title>N-Sphere Dynamical System</title>
        <script src="/anthropic-serve/js-lib/d3.js"></script>
        
        <style>
.n-sphere-dynamical-system {
  font-family:
    system-ui,
    -apple-system,
    sans-serif;
  background-color: white;
  padding: 20px;
  margin: 40px 0;
  line-height: 1.2em;
}

.n-sphere-dynamical-system .controls-container {
  display: flex;
  gap: 30px;
  margin: 0 auto 10px auto;
  padding: 0 0 0px 0;
  width: 840px;
  align-items: flex-start;
}

.n-sphere-dynamical-system .left-section {
  flex: 0 0 320px;
  display: flex;
  flex-direction: column;
  align-items: flex-start;
  justify-content: flex-start;
  padding-top: 0;
}

.n-sphere-dynamical-system .description {
  font-size: 11px;
  color: #666;
  line-height: 1.4;
  max-width: 300px;
  display: inline-block;
  padding-top: 1px;
}

.n-sphere-dynamical-system .sim-title {
  font-weight: bold;
  display: inline;
  margin-right: 4px;
  line-height: 1.4;
}

.n-sphere-dynamical-system .action-buttons {
  display: flex;
  gap: 10px;
  align-items: center;
  margin-top: 10px;
}

.n-sphere-dynamical-system .play-buttons {
  display: flex;
  gap: 2px;
}

.n-sphere-dynamical-system .play-buttons button {
  width: 21px;
  height: 21px;
  font-size: 12px;
  background: white;
  border: 1px solid #ddd;
  border-radius: 2px;
  cursor: pointer;
  transition: all 0.2s ease;
  color: #666;
  display: flex;
  align-items: center;
  justify-content: center;
  padding: 0;
}

.n-sphere-dynamical-system .play-buttons button:hover {
  background: #f8f8f8;
  border-color: #ccc;
}

.n-sphere-dynamical-system .play-buttons button.active {
  background: #000;
  color: white;
  border-color: #000;
}

.n-sphere-dynamical-system .reset-button {
  width: 21px;
  height: 21px;
  font-size: 14px;
  background: white;
  border: 1px solid #ddd;
  border-radius: 2px;
  cursor: pointer;
  transition: all 0.5s ease;
  color: #666;
  display: flex;
  align-items: center;
  justify-content: center;
  padding: 0;
}

.n-sphere-dynamical-system .reset-button:hover {
  background: #f8f8f8;
  border-color: #ccc;
}

.n-sphere-dynamical-system .reset-button.clicked {
  background: #000;
  color: white;
  border-color: #000;
}

.n-sphere-dynamical-system .right-section {
  flex: 1;
  display: flex;
  gap: 30px;
  align-items: flex-start;
  padding-top: 0;
  margin-top: 0px;
}

.n-sphere-dynamical-system .right-section > div {
  display: flex;
  flex-direction: column;
  gap: 26px;
}

.n-sphere-dynamical-system .right-section > div:first-child {
  flex: 0 0 auto;
  margin-top: 0;
}

.n-sphere-dynamical-system .right-section > div:last-child {
  flex: 0 0 auto;
  margin-top: 0;
}

.n-sphere-dynamical-system
  .right-section
  > div:first-child
  > .control-item:first-child {
  align-items: flex-start;
  min-height: auto;
  padding-top: 0;
}

.n-sphere-dynamical-system .right-section > div:last-child > .control-item:first-child {
  align-items: center;
  min-height: auto;
  padding-top: 0;
}

.n-sphere-dynamical-system .control-item {
  display: flex;
  align-items: center;
  gap: 10px;
  min-height: 28px;
}

.n-sphere-dynamical-system .label {
  font-size: 11px;
  color: #666;
  white-space: nowrap;
  line-height: 20px;
}

.n-sphere-dynamical-system .control-item .label:first-child {
  min-width: 75px;
}

.n-sphere-dynamical-system
  .right-section
  > div:first-child
  > .control-item:first-child
  .label {
  display: block;
  font-size: 11px;
  color: #666;
  white-space: nowrap;
}

.n-sphere-dynamical-system button:disabled {
  opacity: 0.5;
  cursor: not-allowed;
}

.n-sphere-dynamical-system select {
  padding: 3px 6px;
  border: 1px solid #ddd;
  font-size: 11px;
  font-family: system-ui;
  background-color: white;
  border-radius: 2px;
  min-width: 50px;
  height: 22px;
}

.n-sphere-dynamical-system .button-group {
  display: flex;
  gap: 2px;
}

.n-sphere-dynamical-system .button-group button {
  padding: 1px 5px;
  font-size: 11px;
  background: white;
  border: 1px solid #ddd;
  border-radius: 2px;
  cursor: pointer;
  transition: all 0.2s ease;
  color: #666;
  min-width: auto;
  line-height: 1.4;
  height: auto;
}

.n-sphere-dynamical-system .button-group button:hover {
  background: #f8f8f8;
  border-color: #ccc;
}

.n-sphere-dynamical-system .button-group button.active {
  background: #000;
  color: white;
  border-color: #000;
}

.n-sphere-dynamical-system .radio-group {
  display: flex;
  gap: 2px;
}

.n-sphere-dynamical-system .radio-label {
  display: flex;
  align-items: center;
  gap: 5px;
  cursor: pointer;
  font-size: 10px;
  color: #666;
  padding: 2px 4px;
  background: white;
  border: 1px solid #ddd;
  border-radius: 2px;
  transition: all 0.2s ease;
}

.n-sphere-dynamical-system .radio-label:hover {
  background: #f8f8f8;
  border-color: #ccc;
}

.n-sphere-dynamical-system input[type="radio"] {
  display: none;
}

.n-sphere-dynamical-system input[type="radio"]:checked + .radio-label,
.n-sphere-dynamical-system .radio-label:has(input:checked) {
  background: #000;
  color: white;
  border-color: #000;
}

.n-sphere-dynamical-system .slider-container {
  display: inline-flex;
  align-items: center;
  gap: 0;
}

.n-sphere-dynamical-system input[type="range"] {
  width: 120px;
  -webkit-appearance: none;
  appearance: none;
  height: 2px;
  background: #ddd;
  outline: none;
  border-radius: 1px;
}

.n-sphere-dynamical-system input[type="range"]::-webkit-slider-thumb {
  -webkit-appearance: none;
  appearance: none;
  width: 10px;
  height: 10px;
  background: #000;
  cursor: pointer;
  border-radius: 50%;
  border: none;
}

.n-sphere-dynamical-system input[type="range"]::-moz-range-thumb {
  width: 10px;
  height: 10px;
  background: #000;
  cursor: pointer;
  border-radius: 50%;
  border: none;
}

.n-sphere-dynamical-system .visualizations {
  display: flex;
  gap: 40px;
  justify-content: center;
  align-items: flex-start;
  width: 840px;
  margin: 0 auto;
}

.n-sphere-dynamical-system .visualization-container {
  display: flex;
  flex-direction: column;
  align-items: center;
}

.n-sphere-dynamical-system .viz-title {
  font-size: 11px;
  color: #666;
  margin-top: 8px;
  font-weight: bold;
  text-align: center;
}

.n-sphere-dynamical-system svg {
  background-color: white;
  border-radius: 2px;
}

.n-sphere-dynamical-system #sphereSvg {
  background-color: #fcfcfc;
  cursor: grab;
}

.n-sphere-dynamical-system #sphereSvg:active {
  cursor: grabbing;
}

.n-sphere-dynamical-system .point {
  stroke: rgba(0, 0, 0, 1);
  stroke-width: 0.8;
}

.n-sphere-dynamical-system .point:hover {
  stroke: rgba(0, 0, 0, 0.8);
  stroke-width: 1.5;
}

.n-sphere-dynamical-system .info {
  font-size: 9px;
  color: #999;
  text-align: center;
  margin-top: 5px;
}

</style>
    </head>
    <body>
        <div class="n-sphere-dynamical-system">
            <div class="controls-container">
                <div class="left-section">
                    <div class="description">
                        <span class="sim-title">Physical Simulation</span>
                        Interactive visualization of particle dynamics on an
                        n-dimensional sphere. Particles attract neighbors and repel
                        distant points.
                    </div>
                    <div class="action-buttons">
                        <div class="button-group play-buttons">
                            <button id="playBtn" data-action="play">▶</button>
                            <button id="pauseBtn" data-action="pause" class="active">
                                ⏸
                            </button>
                        </div>
                        <button id="shuffleBtn" class="reset-button">↺</button>
                    </div>
                </div>

                <div class="right-section">
                    <div>
                        <div class="control-item">
                            <label class="label">Dimensions:</label>
                            <div class="button-group dimension-buttons">
                                <button data-value="3">3D</button>
                                <button data-value="4">4D</button>
                                <button data-value="5">5D</button>
                                <button data-value="6" class="active">6D</button>
                                <button data-value="7">7D</button>
                                <button data-value="8">8D</button>
                            </div>
                        </div>

                        <div class="control-item">
                            <label class="label"
                                >Zone Width:
                                <span
                                    id="attractiveZoneValue"
                                    style="
                                        display: inline-block;
                                        width: 20px;
                                        text-align: right;
                                    "
                                    >6</span
                                ></label
                            >
                            <div class="slider-container">
                                <input
                                    type="range"
                                    id="attractiveZoneSlider"
                                    min="1"
                                    max="20"
                                    step="1"
                                    value="6"
                                />
                            </div>
                        </div>
                    </div>

                    <div>
                        <div class="control-item">
                            <label class="label">Topology:</label>
                            <div class="button-group topology-buttons">
                                <button data-value="circle" class="active">Circle</button>
                                <button data-value="interval">Interval</button>
                            </div>
                        </div>

                        <div class="control-item">
                            <label class="label"
                                >Speed: <span id="speedValue">5</span></label
                            >
                            <div class="slider-container">
                                <input
                                    type="range"
                                    id="speedSlider"
                                    min="1"
                                    max="10"
                                    step="1"
                                    value="5"
                                />
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="visualizations">
                <div class="visualization-container">
                    <div id="sphereVisualization"></div>
                    <div class="viz-title">Projection of points</div>
                    <div class="info">Drag to rotate</div>
                </div>

                <div class="visualization-container">
                    <div id="heatmapVisualization"></div>
                    <div class="viz-title">Inner Product Matrix</div>
                </div>
            </div>
        </div>

        <script>
function initDynamicSim() {
  // Configuration
  const n = 100;
  const width = 400;
  const height = 400;
  const radius = Math.min(width, height) * 0.45;
  let isPlaying = false;
  let animationId = null;
  let dimensions = 6;
  let frameCount = 0;
  let heatmapUpdateInterval = 10; // Update heatmap every N frames
  let topology = "circle"; // 'circle' or 'interval'

  // Fixed force weights
  const attractiveWeight = 1.0;
  const repulsiveWeight = 1.0;

  // Rotation state
  let rotationX = 0;
  let rotationY = 0;
  let isDragging = false;
  let dragStartX = 0;
  let dragStartY = 0;
  let startRotationX = 0;
  let startRotationY = 0;

  // Create color scale
  const colorScale = d3.scaleSequential(d3.interpolateSinebow).domain([0, n - 1]);

  // Initialize points
  let points = [];

  function randomOnNSphere(dim) {
    // Generate random point on unit n-sphere using Muller's method
    const coords = [];
    let sum = 0;

    for (let i = 0; i < dim; i++) {
      const g = d3.randomNormal(0, 1)();
      coords.push(g);
      sum += g * g;
    }

    const norm = Math.sqrt(sum);
    return coords.map((x) => x / norm);
  }

  function initializePoints() {
    points = [];
    for (let i = 0; i < n; i++) {
      const coords = randomOnNSphere(dimensions);
      const point = {
        index: i,
        coords: coords,
        velocities: new Array(dimensions).fill(0),
      };
      points.push(point);
    }
  }

  function initializeGreatCircle() {
    points = [];
    for (let i = 0; i < n; i++) {
      const angle = (i / n) * 2 * Math.PI;
      const coords = new Array(dimensions).fill(0);
      coords[0] = Math.cos(angle);
      coords[1] = Math.sin(angle);

      points.push({
        index: i,
        coords: coords,
        velocities: new Array(dimensions).fill(0),
      });
    }
  }

  function jitterPoints() {
    const jitterAmount = 0.05;
    points.forEach((p) => {
      // Add random jitter
      for (let i = 0; i < dimensions; i++) {
        p.coords[i] += (Math.random() - 0.5) * 2 * jitterAmount;
      }

      // Renormalize
      let sum = 0;
      for (let i = 0; i < dimensions; i++) {
        sum += p.coords[i] * p.coords[i];
      }
      const norm = Math.sqrt(sum);
      for (let i = 0; i < dimensions; i++) {
        p.coords[i] /= norm;
      }
    });
  }

  function changeDimensions(newDim) {
    const oldDim = dimensions;
    dimensions = newDim;

    points.forEach((p) => {
      if (newDim > oldDim) {
        // Add dimensions with small random jitter
        for (let i = oldDim; i < newDim; i++) {
          // Add small random values instead of 0
          p.coords.push((Math.random() - 0.5) * 0.2);
          p.velocities.push(0);
        }
      } else {
        // Remove dimensions
        p.coords = p.coords.slice(0, newDim);
        p.velocities = p.velocities.slice(0, newDim);
      }

      // Renormalize
      let sum = 0;
      for (let i = 0; i < dimensions; i++) {
        sum += p.coords[i] * p.coords[i];
      }
      const norm = Math.sqrt(sum);
      for (let i = 0; i < dimensions; i++) {
        p.coords[i] /= norm;
      }
    });
  }

  function projectTo2D(point) {
    // Start with original coordinates
    let coords = [...point.coords];

    // Apply 3D rotations
    // Rotate around Y axis (x1-x3 plane)
    if (dimensions >= 3) {
      const cosY = Math.cos(rotationY);
      const sinY = Math.sin(rotationY);
      const x1 = coords[0];
      const x3 = coords[2];
      coords[0] = x1 * cosY - x3 * sinY;
      coords[2] = x1 * sinY + x3 * cosY;
    }

    // Rotate around X axis (x2-x3 plane)
    if (dimensions >= 3) {
      const cosX = Math.cos(rotationX);
      const sinX = Math.sin(rotationX);
      const x2 = coords[1];
      const x3 = coords[2];
      coords[1] = x2 * cosX - x3 * sinX;
      coords[2] = x2 * sinX + x3 * cosX;
    }

    // Project to 2D using first two coordinates
    return {
      x: width / 2 + coords[0] * radius,
      y: height / 2 + coords[1] * radius,
      depth: dimensions >= 3 ? coords[2] : 0,
    };
  }

  function calculateForces() {
    const attractiveZoneWidth = parseInt(
      document.getElementById("attractiveZoneSlider").value,
    );
    const dt = 0.01;
    const damping = 0.95;

    // Reset forces
    const forces = points.map(() => new Array(dimensions).fill(0));

    // Calculate pairwise forces
    for (let i = 0; i < n; i++) {
      for (let j = i + 1; j < n; j++) {
        const pi = points[i];
        const pj = points[j];

        // Calculate distance in N-D
        let distSq = 0;
        const diff = [];
        for (let d = 0; d < dimensions; d++) {
          const delta = pj.coords[d] - pi.coords[d];
          diff.push(delta);
          distSq += delta * delta;
        }
        const dist = Math.sqrt(distSq);

        // Avoid division by zero
        if (dist < 0.01) continue;

        // Calculate neighbor distance based on topology
        let neighborDist;
        if (topology === "circle") {
          neighborDist = Math.min(
            Math.abs(j - i),
            Math.abs(j - i + n),
            Math.abs(j - i - n),
          );
        } else {
          // interval
          neighborDist = Math.abs(j - i);
        }

        // Determine force (using fixed weights of 1.0)
        let force;
        if (neighborDist <= attractiveZoneWidth) {
          const strength = 1 - (neighborDist - 1) / attractiveZoneWidth;
          force = (strength * attractiveWeight) / dist;
        } else {
          const strength = Math.min(5, 1 / dist);
          force = (-repulsiveWeight * strength) / dist;
        }

        // Apply forces
        for (let d = 0; d < dimensions; d++) {
          const f = force * diff[d];
          forces[i][d] += f;
          forces[j][d] -= f;
        }
      }
    }

    // Update velocities and positions
    points.forEach((p, i) => {
      // Update velocity and position
      for (let d = 0; d < dimensions; d++) {
        p.velocities[d] = (p.velocities[d] + forces[i][d] * dt) * damping;
        p.coords[d] += p.velocities[d] * dt;
      }

      // Renormalize
      let sum = 0;
      for (let d = 0; d < dimensions; d++) {
        sum += p.coords[d] * p.coords[d];
      }
      const norm = Math.sqrt(sum);
      for (let d = 0; d < dimensions; d++) {
        p.coords[d] /= norm;
      }
    });
  }

  // Create sphere SVG
  const sphereSvg = d3
    .select("#sphereVisualization")
    .html("")
    .append("svg")
    .attr("id", "sphereSvg")
    .attr("width", width)
    .attr("height", height);

  // Create heatmap SVG
  const heatmapSize = 400;
  const heatmapMargin = 0;
  let cellSize = heatmapSize / n; // Will be recalculated in drawHeatmap

  const heatmapSvg = d3
    .select("#heatmapVisualization")
    .html("")
    .append("svg")
    .attr("width", heatmapSize)
    .attr("height", heatmapSize);

  // Create color scale for heatmap (blue-white-red)
  const heatmapColorScale = d3.scaleSequential(d3.interpolateRdBu).domain([1, -1]); // Reversed so red is positive, blue is negative

  // Mouse interaction for rotation
  sphereSvg.on("mousedown", function (event) {
    if (event.target.tagName === "svg") {
      isDragging = true;
      dragStartX = event.clientX;
      dragStartY = event.clientY;
      startRotationX = rotationX;
      startRotationY = rotationY;
    }
  });

  d3.select(window).on("mousemove", function (event) {
    if (!isDragging) return;

    const dx = event.clientX - dragStartX;
    const dy = event.clientY - dragStartY;

    rotationY = startRotationY + dx * 0.01;
    rotationX = startRotationX + dy * 0.01;

    draw();
  });

  d3.select(window).on("mouseup", function () {
    isDragging = false;
  });

  // Initialize points
  initializePoints();

  function draw() {
    // Get current attractive zone width
    const attractiveZoneWidth = parseInt(
      document.getElementById("attractiveZoneSlider").value,
    );

    // Filter points based on topology
    let visiblePoints = points;
    if (topology === "interval") {
      visiblePoints = points.filter(
        (p) => p.index >= attractiveZoneWidth && p.index < n - attractiveZoneWidth,
      );
    }

    // Project all visible points and sort by depth
    const projectedPoints = visiblePoints.map((p) => ({
      ...p,
      projected: projectTo2D(p),
    }));

    const sortedPoints = projectedPoints.sort(
      (a, b) => a.projected.depth - b.projected.depth,
    );

    const circles = sphereSvg.selectAll("circle").data(sortedPoints, (d) => d.index);

    circles
      .enter()
      .append("circle")
      .attr("class", "point")
      .attr("r", 4)
      .merge(circles)
      .attr("cx", (d) => d.projected.x)
      .attr("cy", (d) => d.projected.y)
      .attr("fill", (d) => colorScale(d.index))
      .attr("opacity", (d) => 0.5 + 0.3 * (1 - Math.abs(d.projected.depth)));

    circles.exit().remove();

    // Only update heatmap every N frames or when not playing
    if (!isPlaying || frameCount % heatmapUpdateInterval === 0) {
      drawHeatmap();
    }
  }

  function drawHeatmap() {
    const attractiveZoneWidth = parseInt(
      document.getElementById("attractiveZoneSlider").value,
    );

    // Determine visible range based on topology
    let startIdx = 0;
    let endIdx = n;
    if (topology === "interval") {
      startIdx = attractiveZoneWidth;
      endIdx = n - attractiveZoneWidth;
    }

    // Calculate all pairwise inner products for visible points
    const innerProducts = [];
    for (let i = startIdx; i < endIdx; i++) {
      for (let j = startIdx; j < endIdx; j++) {
        let value = 0;
        for (let d = 0; d < dimensions; d++) {
          value += points[i].coords[d] * points[j].coords[d];
        }
        innerProducts.push({
          i: i - startIdx, // Adjust indices for display
          j: j - startIdx,
          value: value,
        });
      }
    }

    // Adjust cell size based on visible points
    const visibleCount = endIdx - startIdx;
    const cellSize = heatmapSize / visibleCount;

    // Draw heatmap cells
    const cells = heatmapSvg.selectAll("rect.heatmap-cell").data(innerProducts);

    cells
      .enter()
      .append("rect")
      .attr("class", "heatmap-cell")
      .merge(cells)
      .attr("x", (d) => d.j * cellSize)
      .attr("y", (d) => d.i * cellSize)
      .attr("width", cellSize)
      .attr("height", cellSize)
      .attr("fill", (d) => heatmapColorScale(d.value))
      .attr("stroke", "none");

    cells.exit().remove();
  }

  function animate() {
    if (!isPlaying) return;

    const speed = parseInt(document.getElementById("speedSlider").value);

    // Run multiple physics steps per frame for higher speed
    for (let i = 0; i < speed; i++) {
      calculateForces();
    }

    frameCount++;
    draw();

    animationId = requestAnimationFrame(animate);
  }

  // Event handlers - Dimension buttons
  d3.selectAll(".dimension-buttons button").on("click", function (event) {
    d3.selectAll(".dimension-buttons button").classed("active", false);
    d3.select(this).classed("active", true);
    changeDimensions(parseInt(this.dataset.value));
    draw();
  });

  // Topology buttons
  d3.selectAll(".topology-buttons button").on("click", function (event) {
    d3.selectAll(".topology-buttons button").classed("active", false);
    d3.select(this).classed("active", true);
    topology = this.dataset.value;
    draw();
  });

  // Play/Pause buttons
  d3.selectAll(".play-buttons button").on("click", function (event) {
    const action = this.dataset.action;

    if (action === "play" && !isPlaying) {
      isPlaying = true;
      d3.select("#playBtn").classed("active", true);
      d3.select("#pauseBtn").classed("active", false);
      animate();
    } else if (action === "pause" && isPlaying) {
      isPlaying = false;
      d3.select("#playBtn").classed("active", false);
      d3.select("#pauseBtn").classed("active", true);
      cancelAnimationFrame(animationId);
    }
  });

  // Shuffle/Reset button
  d3.select("#shuffleBtn").on("click", function () {
    // Add clicked class for animation
    const button = d3.select(this);
    button.classed("clicked", true);

    // Remove class after animation completes
    setTimeout(() => {
      button.classed("clicked", false);
    }, 500);

    // Reset the simulation
    initializePoints();
    draw();
  });

  // (Topology button listeners already set up above)

  document.getElementById("attractiveZoneSlider").addEventListener("input", function () {
    document.getElementById("attractiveZoneValue").textContent = this.value;
    if (topology === "interval") {
      draw(); // Redraw to update which points are visible
    }
  });

  document.getElementById("speedSlider").addEventListener("input", function () {
    const speed = parseInt(this.value);
    document.getElementById("speedValue").textContent = speed;
    // Adjust heatmap update frequency based on speed
    heatmapUpdateInterval = Math.max(5, Math.min(20, speed * 2));
  });

  // Initial draw
  draw();

  // Set up Intersection Observer for auto-play
  const observerOptions = {
    threshold: 0.5, // Trigger when 50% visible
    rootMargin: "0px",
  };

  let hasAutoPlayed = false;

  const intersectionObserver = new IntersectionObserver((entries) => {
    entries.forEach((entry) => {
      // Only auto-play once, when becoming visible for the first time
      if (entry.isIntersecting && !hasAutoPlayed && !isPlaying) {
        hasAutoPlayed = true;
        isPlaying = true;
        d3.select("#playBtn").classed("active", true);
        d3.select("#pauseBtn").classed("active", false);
        animate();
      }
    });
  }, observerOptions);

  // Observe the main container element
  const containerElement = document.querySelector(".n-sphere-dynamical-system");
  if (containerElement) {
    intersectionObserver.observe(containerElement);
  }
}

// Initialize the simulation
initDynamicSim();
</script>
        
    </body>
</html>

</figure>
<p>Of particular interest is the result from setting the ambient dimension to 3<d-footnote>Optimization in dimension 3, unlike in higher dimensions, admits bad local minima, because a generic curve on the surface of a sphere self-intersects. To avoid this, either increase the zone width until you get a great circle, then decrease it, or do the optimization in 4D, then select 3D.</d-footnote>: the result is a curve similar to the seams of a baseball (below left, circle), which matches the topology observed for three intrinsically one-dimensional phenomena observed in <d-cite key="modell2025origins,engels2025not"></d-cite>, of colors by hue, dates of the year, and years of the 20th century (which also exhibit dilation). Similar ripples were predicted to occur by Olah <d-cite key="olah2023manifold"></d-cite> and then observed by Gorton <d-cite key="gorton2024curve"></d-cite> in curve detector features in Inception v1. One of the earliest observations of ringing in a cosine similarity plot and rippled spiral/helix shape in a low-dimensional embedding was of the learned positional embeddings of tokens in GPT2 <d-cite key="yedidia2023gpt2,yedidia2023positional"></d-cite>. We also find similar structure in other representations, which we study in <a href='#appendix-sensory'>More Sensory and Counting Representations</a> in the appendix.</p>
<figure class="gdoc-image" style="--img-width: 1800px; "><img src='img_010.png' /><figcaption class='text-caption'>Left curve is a locally optimal high-curvature embedding of the circle onto the 2-sphere. Right figures, reproduced with permission from <span style='font-style: italic;'>Modell et al.</span> <d-cite key="modell2025origins"></d-cite>, show 3-dimensional PCA projections of data or features related to colours, years, and dates.</figcaption></figure>
<h3><a id='boundary' href='#boundary'>Sensing the Line Boundary</a></h3>
<p>We now study how the character counting representations are used to determine if the current line of text is approaching the<span style='font-style: italic;'> line boundary</span>. To detect the line boundary, the model needs to (1) determine the overall <span style='font-style: italic;'>line width</span> constraint and (2) compare the current character count with the line width to calculate the characters remaining.</p>
<h4>Twisting with QK</h4>
<p>We find that newline tokens have their own dedicated character <a href='#appendix-width-features'>counting features</a> that activate based on the width of the line, counting the number of characters between adjacent newlines.</p>
<p>To better understand how these representations are related, we train 150 probes for each possible value of “Line Width” like we did for “Character Count”. Using the attribution graph, we identify an attention head which activates boundary detection features. We visualize both sets of counting representations directly using the first 3 components of their joint PCA in the residual stream (left) and in the reduced QK space of this <span style='font-style: italic;'>boundary</span> <span style='font-style: italic;'>head</span> (right).<d-footnote>Specifically we multiply the line width probes through <d-math>W_K</d-math> and the character count probes through <d-math>W_Q</d-math>, and plot the points in the 3D PCA basis of their joint embedding.</d-footnote></p>
<figure key="resid_vs_head_twist" class="gdoc-image">
<script src="https://cdn.plot.ly/plotly-2.32.0.min.js" integrity="sha384-7TVmlZWH60iKX5Uk7lSvQhjtcgw2tkFjuwLcXoRSR4zXTyWFJRm9aPAguMh7CIra" crossorigin="anonymous"></script>
<div>                            <div id="a695f85b-e592-421e-9989-a94d97935f8c" class="plotly-graph-div" style="height:600px; width:1200px;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("a695f85b-e592-421e-9989-a94d97935f8c")) {                    Plotly.newPlot(                        "a695f85b-e592-421e-9989-a94d97935f8c",                        [{"hovertemplate":"%{text}","legend":"legend","legendgroup":"Character Count Probes_0","marker":{"color":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110],"colorscale":[[0.0,"#440154"],[0.1111111111111111,"#482878"],[0.2222222222222222,"#3e4989"],[0.3333333333333333,"#31688e"],[0.4444444444444444,"#26828e"],[0.5555555555555556,"#1f9e89"],[0.6666666666666666,"#35b779"],[0.7777777777777778,"#6ece58"],[0.8888888888888888,"#b5de2b"],[1.0,"#fde725"]],"opacity":1.0,"showscale":false,"size":6,"symbol":"circle"},"mode":"markers","name":"Character Count Probes","showlegend":false,"text":["Probe 40","Probe 41","Probe 42","Probe 43","Probe 44","Probe 45","Probe 46","Probe 47","Probe 48","Probe 49","Probe 50","Probe 51","Probe 52","Probe 53","Probe 54","Probe 55","Probe 56","Probe 57","Probe 58","Probe 59","Probe 60","Probe 61","Probe 62","Probe 63","Probe 64","Probe 65","Probe 66","Probe 67","Probe 68","Probe 69","Probe 70","Probe 71","Probe 72","Probe 73","Probe 74","Probe 75","Probe 76","Probe 77","Probe 78","Probe 79","Probe 80","Probe 81","Probe 82","Probe 83","Probe 84","Probe 85","Probe 86","Probe 87","Probe 88","Probe 89","Probe 90","Probe 91","Probe 92","Probe 93","Probe 94","Probe 95","Probe 96","Probe 97","Probe 98","Probe 99","Probe 100","Probe 101","Probe 102","Probe 103","Probe 104","Probe 105","Probe 106","Probe 107","Probe 108","Probe 109","Probe 110","Probe 111","Probe 112","Probe 113","Probe 114","Probe 115","Probe 116","Probe 117","Probe 118","Probe 119","Probe 120","Probe 121","Probe 122","Probe 123","Probe 124","Probe 125","Probe 126","Probe 127","Probe 128","Probe 129","Probe 130","Probe 131","Probe 132","Probe 133","Probe 134","Probe 135","Probe 136","Probe 137","Probe 138","Probe 139","Probe 140","Probe 141","Probe 142","Probe 143","Probe 144","Probe 145","Probe 146","Probe 147","Probe 148","Probe 149","Probe 150"],"x":[-0.09,-0.07,-0.03,-0.01,0.01,0.04,0.08,0.09,0.11,0.13,0.14,0.15,0.16,0.16,0.16,0.15,0.15,0.13,0.12,0.1,0.08,0.04,0.02,-0.02,-0.04,-0.09,-0.11,-0.15,-0.2,-0.23,-0.27,-0.31,-0.35,-0.39,-0.41,-0.43,-0.48,-0.5,-0.53,-0.56,-0.58,-0.6,-0.62,-0.63,-0.64,-0.64,-0.66,-0.66,-0.66,-0.67,-0.66,-0.66,-0.64,-0.63,-0.61,-0.58,-0.56,-0.52,-0.51,-0.47,-0.43,-0.41,-0.36,-0.31,-0.28,-0.24,-0.19,-0.13,-0.09,-0.05,0.01,0.05,0.09,0.14,0.2,0.23,0.28,0.31,0.35,0.4,0.43,0.45,0.49,0.53,0.56,0.57,0.58,0.6,0.62,0.65,0.65,0.64,0.66,0.67,0.68,0.68,0.67,0.66,0.68,0.66,0.66,0.64,0.62,0.62,0.63,0.6,0.59,0.56,0.56,0.55,0.54],"y":[-0.06,-0.06,-0.08,-0.1,-0.12,-0.15,-0.17,-0.19,-0.24,-0.26,-0.29,-0.32,-0.36,-0.39,-0.42,-0.46,-0.48,-0.51,-0.54,-0.57,-0.59,-0.61,-0.63,-0.63,-0.63,-0.65,-0.64,-0.64,-0.62,-0.6,-0.59,-0.56,-0.54,-0.52,-0.49,-0.45,-0.42,-0.39,-0.34,-0.32,-0.26,-0.21,-0.17,-0.13,-0.08,-0.03,0.01,0.07,0.11,0.16,0.2,0.25,0.31,0.33,0.38,0.43,0.47,0.51,0.54,0.58,0.61,0.63,0.65,0.69,0.72,0.72,0.72,0.75,0.75,0.75,0.76,0.76,0.76,0.75,0.74,0.73,0.7,0.69,0.67,0.65,0.63,0.59,0.58,0.54,0.51,0.48,0.43,0.43,0.38,0.35,0.31,0.28,0.25,0.21,0.19,0.15,0.13,0.09,0.08,0.04,0.02,0.01,-0.02,-0.04,-0.05,-0.07,-0.08,-0.09,-0.1,-0.1,-0.13],"z":[0.62,0.65,0.68,0.7,0.71,0.72,0.72,0.72,0.71,0.7,0.68,0.67,0.64,0.61,0.58,0.54,0.51,0.47,0.42,0.37,0.32,0.27,0.22,0.16,0.1,0.06,-0.0,-0.04,-0.1,-0.14,-0.18,-0.21,-0.25,-0.28,-0.31,-0.34,-0.37,-0.39,-0.42,-0.43,-0.44,-0.45,-0.45,-0.46,-0.45,-0.46,-0.46,-0.44,-0.44,-0.43,-0.43,-0.41,-0.39,-0.38,-0.36,-0.35,-0.32,-0.33,-0.29,-0.28,-0.26,-0.24,-0.23,-0.21,-0.19,-0.18,-0.18,-0.17,-0.16,-0.15,-0.15,-0.16,-0.13,-0.15,-0.17,-0.15,-0.16,-0.17,-0.18,-0.17,-0.18,-0.2,-0.2,-0.22,-0.23,-0.25,-0.27,-0.27,-0.28,-0.29,-0.3,-0.33,-0.35,-0.34,-0.35,-0.36,-0.37,-0.38,-0.38,-0.39,-0.4,-0.4,-0.41,-0.41,-0.41,-0.41,-0.4,-0.41,-0.42,-0.42,-0.4],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","legend":"legend","legendgroup":"Character Count Probes_0","marker":{"color":"rgba(255, 255, 255, 0.8)","line":{"color":"rgba(50, 50, 50, 1)","width":2},"showscale":false,"size":12,"symbol":"circle"},"mode":"markers","name":"Character Count Probes","showlegend":true,"x":[null],"y":[null],"z":[null],"type":"scatter3d","scene":"scene"},{"hovertemplate":"%{text}","legend":"legend","legendgroup":"Line Width Probes_0","marker":{"color":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110],"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"opacity":1.0,"showscale":false,"size":6,"symbol":"square"},"mode":"markers","name":"Line Width Probes","showlegend":false,"text":["Probe 40","Probe 41","Probe 42","Probe 43","Probe 44","Probe 45","Probe 46","Probe 47","Probe 48","Probe 49","Probe 50","Probe 51","Probe 52","Probe 53","Probe 54","Probe 55","Probe 56","Probe 57","Probe 58","Probe 59","Probe 60","Probe 61","Probe 62","Probe 63","Probe 64","Probe 65","Probe 66","Probe 67","Probe 68","Probe 69","Probe 70","Probe 71","Probe 72","Probe 73","Probe 74","Probe 75","Probe 76","Probe 77","Probe 78","Probe 79","Probe 80","Probe 81","Probe 82","Probe 83","Probe 84","Probe 85","Probe 86","Probe 87","Probe 88","Probe 89","Probe 90","Probe 91","Probe 92","Probe 93","Probe 94","Probe 95","Probe 96","Probe 97","Probe 98","Probe 99","Probe 100","Probe 101","Probe 102","Probe 103","Probe 104","Probe 105","Probe 106","Probe 107","Probe 108","Probe 109","Probe 110","Probe 111","Probe 112","Probe 113","Probe 114","Probe 115","Probe 116","Probe 117","Probe 118","Probe 119","Probe 120","Probe 121","Probe 122","Probe 123","Probe 124","Probe 125","Probe 126","Probe 127","Probe 128","Probe 129","Probe 130","Probe 131","Probe 132","Probe 133","Probe 134","Probe 135","Probe 136","Probe 137","Probe 138","Probe 139","Probe 140","Probe 141","Probe 142","Probe 143","Probe 144","Probe 145","Probe 146","Probe 147","Probe 148","Probe 149","Probe 150"],"x":[-0.1,-0.02,0.03,0.07,0.13,0.17,0.24,0.28,0.33,0.35,0.37,0.38,0.38,0.41,0.4,0.38,0.35,0.36,0.34,0.32,0.29,0.19,0.16,0.14,0.09,0.02,-0.02,-0.07,-0.13,-0.21,-0.25,-0.31,-0.37,-0.41,-0.43,-0.48,-0.55,-0.6,-0.62,-0.65,-0.66,-0.7,-0.72,-0.73,-0.72,-0.72,-0.73,-0.72,-0.73,-0.73,-0.68,-0.67,-0.67,-0.66,-0.62,-0.59,-0.55,-0.53,-0.49,-0.45,-0.4,-0.38,-0.32,-0.28,-0.23,-0.18,-0.14,-0.11,-0.04,-0.01,0.04,0.07,0.13,0.17,0.23,0.28,0.3,0.36,0.4,0.45,0.49,0.51,0.55,0.59,0.63,0.65,0.65,0.68,0.72,0.73,0.75,0.7,0.74,0.74,0.75,0.74,0.67,0.67,0.7,0.66,0.64,0.57,0.57,0.55,0.52,0.49,0.47,0.44,0.41,0.39,0.34],"y":[0.31,0.28,0.26,0.25,0.21,0.16,0.09,0.06,0.02,-0.03,-0.08,-0.19,-0.25,-0.28,-0.33,-0.38,-0.46,-0.5,-0.53,-0.57,-0.57,-0.63,-0.67,-0.69,-0.7,-0.69,-0.69,-0.68,-0.69,-0.68,-0.64,-0.6,-0.6,-0.57,-0.52,-0.47,-0.4,-0.37,-0.33,-0.27,-0.22,-0.15,-0.11,-0.06,-0.02,0.05,0.09,0.14,0.18,0.23,0.28,0.31,0.35,0.39,0.44,0.47,0.51,0.55,0.58,0.62,0.63,0.63,0.67,0.7,0.7,0.7,0.7,0.72,0.73,0.72,0.72,0.67,0.68,0.67,0.64,0.62,0.59,0.56,0.56,0.54,0.47,0.42,0.42,0.38,0.34,0.28,0.22,0.2,0.17,0.1,0.08,-0.0,-0.03,-0.07,-0.1,-0.15,-0.21,-0.24,-0.27,-0.31,-0.33,-0.34,-0.37,-0.38,-0.41,-0.43,-0.41,-0.43,-0.45,-0.45,-0.47],"z":[0.48,0.56,0.58,0.58,0.57,0.53,0.53,0.54,0.5,0.47,0.4,0.39,0.36,0.3,0.24,0.18,0.17,0.11,0.05,-0.0,-0.06,-0.07,-0.12,-0.17,-0.21,-0.22,-0.23,-0.26,-0.29,-0.29,-0.31,-0.28,-0.31,-0.33,-0.33,-0.33,-0.27,-0.28,-0.28,-0.28,-0.27,-0.21,-0.2,-0.21,-0.2,-0.17,-0.11,-0.1,-0.09,-0.07,-0.05,-0.01,0.01,-0.0,0.05,0.05,0.11,0.09,0.11,0.13,0.13,0.15,0.15,0.17,0.17,0.17,0.17,0.18,0.18,0.18,0.18,0.18,0.19,0.17,0.16,0.15,0.14,0.14,0.12,0.1,0.08,0.08,0.06,0.05,0.03,0.01,-0.0,-0.02,-0.04,-0.06,-0.11,-0.13,-0.14,-0.17,-0.19,-0.2,-0.2,-0.24,-0.26,-0.28,-0.29,-0.28,-0.3,-0.33,-0.34,-0.36,-0.34,-0.36,-0.36,-0.37,-0.35],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","legend":"legend","legendgroup":"Line Width Probes_0","marker":{"color":"rgba(255, 255, 255, 0.8)","line":{"color":"rgba(50, 50, 50, 1)","width":2},"showscale":false,"size":12,"symbol":"square"},"mode":"markers","name":"Line Width Probes","showlegend":true,"x":[null],"y":[null],"z":[null],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.09,-0.1],"y":[-0.06,0.31],"z":[0.62,0.48],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.07,-0.02],"y":[-0.06,0.28],"z":[0.65,0.56],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.03,0.03],"y":[-0.08,0.26],"z":[0.68,0.58],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.01,0.07],"y":[-0.1,0.25],"z":[0.7,0.58],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.01,0.13],"y":[-0.12,0.21],"z":[0.71,0.57],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.04,0.17],"y":[-0.15,0.16],"z":[0.72,0.53],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.08,0.24],"y":[-0.17,0.09],"z":[0.72,0.53],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.09,0.28],"y":[-0.19,0.06],"z":[0.72,0.54],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.11,0.33],"y":[-0.24,0.02],"z":[0.71,0.5],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.13,0.35],"y":[-0.26,-0.03],"z":[0.7,0.47],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.14,0.37],"y":[-0.29,-0.08],"z":[0.68,0.4],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.15,0.38],"y":[-0.32,-0.19],"z":[0.67,0.39],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.16,0.38],"y":[-0.36,-0.25],"z":[0.64,0.36],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.16,0.41],"y":[-0.39,-0.28],"z":[0.61,0.3],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.16,0.4],"y":[-0.42,-0.33],"z":[0.58,0.24],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.15,0.38],"y":[-0.46,-0.38],"z":[0.54,0.18],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.15,0.35],"y":[-0.48,-0.46],"z":[0.51,0.17],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.13,0.36],"y":[-0.51,-0.5],"z":[0.47,0.11],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.12,0.34],"y":[-0.54,-0.53],"z":[0.42,0.05],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.1,0.32],"y":[-0.57,-0.57],"z":[0.37,-0.0],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.08,0.29],"y":[-0.59,-0.57],"z":[0.32,-0.06],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.04,0.19],"y":[-0.61,-0.63],"z":[0.27,-0.07],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.02,0.16],"y":[-0.63,-0.67],"z":[0.22,-0.12],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.02,0.14],"y":[-0.63,-0.69],"z":[0.16,-0.17],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.04,0.09],"y":[-0.63,-0.7],"z":[0.1,-0.21],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.09,0.02],"y":[-0.65,-0.69],"z":[0.06,-0.22],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.11,-0.02],"y":[-0.64,-0.69],"z":[-0.0,-0.23],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.15,-0.07],"y":[-0.64,-0.68],"z":[-0.04,-0.26],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.2,-0.13],"y":[-0.62,-0.69],"z":[-0.1,-0.29],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.23,-0.21],"y":[-0.6,-0.68],"z":[-0.14,-0.29],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.27,-0.25],"y":[-0.59,-0.64],"z":[-0.18,-0.31],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.31,-0.31],"y":[-0.56,-0.6],"z":[-0.21,-0.28],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.35,-0.37],"y":[-0.54,-0.6],"z":[-0.25,-0.31],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.39,-0.41],"y":[-0.52,-0.57],"z":[-0.28,-0.33],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.41,-0.43],"y":[-0.49,-0.52],"z":[-0.31,-0.33],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.43,-0.48],"y":[-0.45,-0.47],"z":[-0.34,-0.33],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.48,-0.55],"y":[-0.42,-0.4],"z":[-0.37,-0.27],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.5,-0.6],"y":[-0.39,-0.37],"z":[-0.39,-0.28],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.53,-0.62],"y":[-0.34,-0.33],"z":[-0.42,-0.28],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.56,-0.65],"y":[-0.32,-0.27],"z":[-0.43,-0.28],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.58,-0.66],"y":[-0.26,-0.22],"z":[-0.44,-0.27],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.6,-0.7],"y":[-0.21,-0.15],"z":[-0.45,-0.21],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.62,-0.72],"y":[-0.17,-0.11],"z":[-0.45,-0.2],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.63,-0.73],"y":[-0.13,-0.06],"z":[-0.46,-0.21],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.64,-0.72],"y":[-0.08,-0.02],"z":[-0.45,-0.2],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.64,-0.72],"y":[-0.03,0.05],"z":[-0.46,-0.17],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.66,-0.73],"y":[0.01,0.09],"z":[-0.46,-0.11],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.66,-0.72],"y":[0.07,0.14],"z":[-0.44,-0.1],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.66,-0.73],"y":[0.11,0.18],"z":[-0.44,-0.09],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.67,-0.73],"y":[0.16,0.23],"z":[-0.43,-0.07],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.66,-0.68],"y":[0.2,0.28],"z":[-0.43,-0.05],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.66,-0.67],"y":[0.25,0.31],"z":[-0.41,-0.01],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.64,-0.67],"y":[0.31,0.35],"z":[-0.39,0.01],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.63,-0.66],"y":[0.33,0.39],"z":[-0.38,-0.0],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.61,-0.62],"y":[0.38,0.44],"z":[-0.36,0.05],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.58,-0.59],"y":[0.43,0.47],"z":[-0.35,0.05],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.56,-0.55],"y":[0.47,0.51],"z":[-0.32,0.11],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.52,-0.53],"y":[0.51,0.55],"z":[-0.33,0.09],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.51,-0.49],"y":[0.54,0.58],"z":[-0.29,0.11],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.47,-0.45],"y":[0.58,0.62],"z":[-0.28,0.13],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.43,-0.4],"y":[0.61,0.63],"z":[-0.26,0.13],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.41,-0.38],"y":[0.63,0.63],"z":[-0.24,0.15],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.36,-0.32],"y":[0.65,0.67],"z":[-0.23,0.15],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.31,-0.28],"y":[0.69,0.7],"z":[-0.21,0.17],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.28,-0.23],"y":[0.72,0.7],"z":[-0.19,0.17],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.24,-0.18],"y":[0.72,0.7],"z":[-0.18,0.17],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.19,-0.14],"y":[0.72,0.7],"z":[-0.18,0.17],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.13,-0.11],"y":[0.75,0.72],"z":[-0.17,0.18],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.09,-0.04],"y":[0.75,0.73],"z":[-0.16,0.18],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.05,-0.01],"y":[0.75,0.72],"z":[-0.15,0.18],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.01,0.04],"y":[0.76,0.72],"z":[-0.15,0.18],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.05,0.07],"y":[0.76,0.67],"z":[-0.16,0.18],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.09,0.13],"y":[0.76,0.68],"z":[-0.13,0.19],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.14,0.17],"y":[0.75,0.67],"z":[-0.15,0.17],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.2,0.23],"y":[0.74,0.64],"z":[-0.17,0.16],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.23,0.28],"y":[0.73,0.62],"z":[-0.15,0.15],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.28,0.3],"y":[0.7,0.59],"z":[-0.16,0.14],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.31,0.36],"y":[0.69,0.56],"z":[-0.17,0.14],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.35,0.4],"y":[0.67,0.56],"z":[-0.18,0.12],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.4,0.45],"y":[0.65,0.54],"z":[-0.17,0.1],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.43,0.49],"y":[0.63,0.47],"z":[-0.18,0.08],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.45,0.51],"y":[0.59,0.42],"z":[-0.2,0.08],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.49,0.55],"y":[0.58,0.42],"z":[-0.2,0.06],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.53,0.59],"y":[0.54,0.38],"z":[-0.22,0.05],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.56,0.63],"y":[0.51,0.34],"z":[-0.23,0.03],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.57,0.65],"y":[0.48,0.28],"z":[-0.25,0.01],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.58,0.65],"y":[0.43,0.22],"z":[-0.27,-0.0],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.6,0.68],"y":[0.43,0.2],"z":[-0.27,-0.02],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.62,0.72],"y":[0.38,0.17],"z":[-0.28,-0.04],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.65,0.73],"y":[0.35,0.1],"z":[-0.29,-0.06],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.65,0.75],"y":[0.31,0.08],"z":[-0.3,-0.11],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.64,0.7],"y":[0.28,-0.0],"z":[-0.33,-0.13],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.66,0.74],"y":[0.25,-0.03],"z":[-0.35,-0.14],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.67,0.74],"y":[0.21,-0.07],"z":[-0.34,-0.17],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.68,0.75],"y":[0.19,-0.1],"z":[-0.35,-0.19],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.68,0.74],"y":[0.15,-0.15],"z":[-0.36,-0.2],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.67,0.67],"y":[0.13,-0.21],"z":[-0.37,-0.2],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.66,0.67],"y":[0.09,-0.24],"z":[-0.38,-0.24],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.68,0.7],"y":[0.08,-0.27],"z":[-0.38,-0.26],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.66,0.66],"y":[0.04,-0.31],"z":[-0.39,-0.28],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.66,0.64],"y":[0.02,-0.33],"z":[-0.4,-0.29],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.64,0.57],"y":[0.01,-0.34],"z":[-0.4,-0.28],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.62,0.57],"y":[-0.02,-0.37],"z":[-0.41,-0.3],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.62,0.55],"y":[-0.04,-0.38],"z":[-0.41,-0.33],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.63,0.52],"y":[-0.05,-0.41],"z":[-0.41,-0.34],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.6,0.49],"y":[-0.07,-0.43],"z":[-0.41,-0.36],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.59,0.47],"y":[-0.08,-0.41],"z":[-0.4,-0.34],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.56,0.44],"y":[-0.09,-0.43],"z":[-0.41,-0.36],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.56,0.41],"y":[-0.1,-0.45],"z":[-0.42,-0.36],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.55,0.39],"y":[-0.1,-0.45],"z":[-0.42,-0.37],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.54,0.34],"y":[-0.13,-0.47],"z":[-0.4,-0.35],"type":"scatter3d","scene":"scene"},{"hovertemplate":"%{text}","legend":"legend2","legendgroup":"Character Count through Q_1","marker":{"color":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110],"colorscale":[[0.0,"#440154"],[0.1111111111111111,"#482878"],[0.2222222222222222,"#3e4989"],[0.3333333333333333,"#31688e"],[0.4444444444444444,"#26828e"],[0.5555555555555556,"#1f9e89"],[0.6666666666666666,"#35b779"],[0.7777777777777778,"#6ece58"],[0.8888888888888888,"#b5de2b"],[1.0,"#fde725"]],"opacity":1.0,"showscale":false,"size":6,"symbol":"circle"},"mode":"markers","name":"Character Count through Q","showlegend":false,"text":["Character 40","Character 41","Character 42","Character 43","Character 44","Character 45","Character 46","Character 47","Character 48","Character 49","Character 50","Character 51","Character 52","Character 53","Character 54","Character 55","Character 56","Character 57","Character 58","Character 59","Character 60","Character 61","Character 62","Character 63","Character 64","Character 65","Character 66","Character 67","Character 68","Character 69","Character 70","Character 71","Character 72","Character 73","Character 74","Character 75","Character 76","Character 77","Character 78","Character 79","Character 80","Character 81","Character 82","Character 83","Character 84","Character 85","Character 86","Character 87","Character 88","Character 89","Character 90","Character 91","Character 92","Character 93","Character 94","Character 95","Character 96","Character 97","Character 98","Character 99","Character 100","Character 101","Character 102","Character 103","Character 104","Character 105","Character 106","Character 107","Character 108","Character 109","Character 110","Character 111","Character 112","Character 113","Character 114","Character 115","Character 116","Character 117","Character 118","Character 119","Character 120","Character 121","Character 122","Character 123","Character 124","Character 125","Character 126","Character 127","Character 128","Character 129","Character 130","Character 131","Character 132","Character 133","Character 134","Character 135","Character 136","Character 137","Character 138","Character 139","Character 140","Character 141","Character 142","Character 143","Character 144","Character 145","Character 146","Character 147","Character 148","Character 149","Character 150"],"x":[-0.16,-0.27,-0.39,-0.46,-0.52,-0.59,-0.64,-0.65,-0.68,-0.7,-0.7,-0.7,-0.72,-0.7,-0.68,-0.67,-0.65,-0.61,-0.57,-0.53,-0.48,-0.41,-0.36,-0.29,-0.24,-0.15,-0.09,-0.02,0.08,0.15,0.22,0.3,0.38,0.45,0.51,0.57,0.63,0.68,0.73,0.78,0.82,0.86,0.89,0.92,0.94,0.96,0.97,0.99,0.99,1.0,1.0,0.99,0.98,0.98,0.96,0.94,0.91,0.88,0.85,0.8,0.77,0.73,0.67,0.61,0.55,0.49,0.43,0.34,0.28,0.2,0.14,0.06,-0.02,-0.09,-0.17,-0.22,-0.3,-0.36,-0.42,-0.48,-0.54,-0.58,-0.62,-0.67,-0.71,-0.76,-0.79,-0.8,-0.83,-0.87,-0.87,-0.89,-0.9,-0.91,-0.91,-0.93,-0.92,-0.91,-0.92,-0.91,-0.91,-0.9,-0.89,-0.88,-0.87,-0.86,-0.86,-0.83,-0.82,-0.82,-0.8],"y":[-0.81,-0.74,-0.63,-0.54,-0.44,-0.33,-0.24,-0.18,-0.07,0.0,0.09,0.15,0.23,0.31,0.36,0.44,0.5,0.57,0.63,0.69,0.74,0.8,0.84,0.88,0.92,0.94,0.96,0.97,0.98,0.98,0.97,0.95,0.92,0.89,0.86,0.82,0.78,0.73,0.68,0.62,0.56,0.51,0.45,0.4,0.33,0.28,0.21,0.15,0.09,0.02,-0.03,-0.1,-0.17,-0.22,-0.29,-0.35,-0.41,-0.47,-0.52,-0.6,-0.64,-0.69,-0.74,-0.79,-0.84,-0.87,-0.9,-0.94,-0.96,-0.98,-0.99,-1.0,-1.0,-1.0,-0.98,-0.97,-0.95,-0.93,-0.9,-0.87,-0.83,-0.8,-0.76,-0.71,-0.66,-0.61,-0.55,-0.53,-0.46,-0.4,-0.36,-0.31,-0.24,-0.19,-0.16,-0.1,-0.04,0.01,0.04,0.11,0.14,0.17,0.22,0.26,0.27,0.32,0.31,0.36,0.4,0.41,0.45],"z":[-0.56,-0.62,-0.68,-0.7,-0.73,-0.74,-0.73,-0.74,-0.73,-0.71,-0.71,-0.69,-0.66,-0.64,-0.63,-0.6,-0.57,-0.55,-0.53,-0.49,-0.47,-0.43,-0.39,-0.36,-0.32,-0.3,-0.26,-0.23,-0.2,-0.16,-0.13,-0.11,-0.09,-0.06,-0.03,-0.0,-0.0,0.02,0.03,0.05,0.06,0.06,0.07,0.07,0.08,0.08,0.07,0.07,0.07,0.06,0.06,0.06,0.05,0.04,0.03,0.03,-0.0,0.01,0.0,0.01,-0.02,-0.05,-0.03,-0.03,-0.02,-0.04,-0.05,-0.03,-0.02,-0.01,-0.01,0.01,0.01,0.03,0.06,0.07,0.09,0.11,0.12,0.12,0.15,0.16,0.2,0.22,0.23,0.23,0.28,0.28,0.31,0.3,0.33,0.34,0.36,0.36,0.38,0.36,0.39,0.41,0.4,0.4,0.4,0.4,0.41,0.4,0.41,0.41,0.4,0.42,0.4,0.4,0.39],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","legend":"legend2","legendgroup":"Character Count through Q_1","marker":{"color":"rgba(255, 255, 255, 0.8)","line":{"color":"rgba(50, 50, 50, 1)","width":2},"showscale":false,"size":12,"symbol":"circle"},"mode":"markers","name":"Character Count through Q","showlegend":true,"x":[null],"y":[null],"z":[null],"type":"scatter3d","scene":"scene2"},{"hovertemplate":"%{text}","legend":"legend2","legendgroup":"Line Width through K_1","marker":{"color":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110],"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"opacity":1.0,"showscale":false,"size":6,"symbol":"square"},"mode":"markers","name":"Line Width through K","showlegend":false,"text":["Width 40","Width 41","Width 42","Width 43","Width 44","Width 45","Width 46","Width 47","Width 48","Width 49","Width 50","Width 51","Width 52","Width 53","Width 54","Width 55","Width 56","Width 57","Width 58","Width 59","Width 60","Width 61","Width 62","Width 63","Width 64","Width 65","Width 66","Width 67","Width 68","Width 69","Width 70","Width 71","Width 72","Width 73","Width 74","Width 75","Width 76","Width 77","Width 78","Width 79","Width 80","Width 81","Width 82","Width 83","Width 84","Width 85","Width 86","Width 87","Width 88","Width 89","Width 90","Width 91","Width 92","Width 93","Width 94","Width 95","Width 96","Width 97","Width 98","Width 99","Width 100","Width 101","Width 102","Width 103","Width 104","Width 105","Width 106","Width 107","Width 108","Width 109","Width 110","Width 111","Width 112","Width 113","Width 114","Width 115","Width 116","Width 117","Width 118","Width 119","Width 120","Width 121","Width 122","Width 123","Width 124","Width 125","Width 126","Width 127","Width 128","Width 129","Width 130","Width 131","Width 132","Width 133","Width 134","Width 135","Width 136","Width 137","Width 138","Width 139","Width 140","Width 141","Width 142","Width 143","Width 144","Width 145","Width 146","Width 147","Width 148","Width 149","Width 150"],"x":[0.39,0.2,0.06,-0.04,-0.16,-0.26,-0.39,-0.48,-0.57,-0.65,-0.72,-0.77,-0.82,-0.85,-0.88,-0.9,-0.87,-0.88,-0.87,-0.85,-0.84,-0.75,-0.7,-0.68,-0.63,-0.54,-0.49,-0.43,-0.35,-0.27,-0.19,-0.11,-0.03,0.04,0.11,0.21,0.32,0.39,0.44,0.52,0.59,0.64,0.7,0.73,0.77,0.83,0.84,0.88,0.91,0.94,0.97,0.96,0.98,0.99,0.99,0.99,0.98,0.97,0.96,0.93,0.91,0.91,0.87,0.82,0.78,0.73,0.75,0.7,0.62,0.58,0.52,0.51,0.43,0.37,0.28,0.22,0.22,0.12,0.07,-0.02,-0.13,-0.18,-0.21,-0.3,-0.37,-0.46,-0.53,-0.55,-0.59,-0.65,-0.68,-0.74,-0.78,-0.81,-0.82,-0.83,-0.87,-0.85,-0.85,-0.85,-0.85,-0.84,-0.83,-0.82,-0.81,-0.79,-0.78,-0.76,-0.75,-0.74,-0.71],"y":[-0.83,-0.86,-0.86,-0.84,-0.82,-0.76,-0.68,-0.64,-0.57,-0.5,-0.41,-0.29,-0.17,-0.13,-0.02,0.09,0.19,0.25,0.34,0.42,0.48,0.58,0.65,0.7,0.76,0.83,0.85,0.89,0.93,0.96,0.98,0.99,1.0,1.0,0.99,0.98,0.95,0.92,0.9,0.86,0.81,0.77,0.71,0.68,0.64,0.56,0.53,0.46,0.4,0.34,0.24,0.23,0.17,0.12,0.02,-0.06,-0.1,-0.17,-0.23,-0.31,-0.39,-0.36,-0.46,-0.54,-0.6,-0.66,-0.63,-0.69,-0.76,-0.8,-0.83,-0.84,-0.88,-0.92,-0.95,-0.97,-0.97,-0.99,-0.99,-1.0,-0.99,-0.98,-0.98,-0.95,-0.93,-0.88,-0.84,-0.82,-0.78,-0.7,-0.66,-0.54,-0.51,-0.44,-0.41,-0.35,-0.21,-0.15,-0.14,-0.06,-0.02,0.07,0.11,0.13,0.19,0.25,0.24,0.29,0.33,0.35,0.41],"z":[-0.39,-0.48,-0.5,-0.54,-0.56,-0.6,-0.62,-0.6,-0.6,-0.57,-0.56,-0.57,-0.55,-0.51,-0.47,-0.43,-0.46,-0.4,-0.36,-0.31,-0.26,-0.32,-0.29,-0.21,-0.16,-0.13,-0.19,-0.18,-0.14,-0.11,-0.07,-0.1,-0.08,-0.05,-0.02,-0.01,-0.05,-0.05,-0.03,-0.03,-0.01,-0.06,-0.03,-0.05,-0.04,-0.04,-0.1,-0.09,-0.07,-0.07,-0.09,-0.13,-0.12,-0.11,-0.14,-0.13,-0.17,-0.16,-0.18,-0.17,-0.15,-0.21,-0.18,-0.18,-0.19,-0.18,-0.2,-0.2,-0.19,-0.18,-0.19,-0.19,-0.18,-0.16,-0.12,-0.1,-0.14,-0.1,-0.08,-0.05,0.0,0.01,0.03,0.05,0.09,0.11,0.14,0.16,0.2,0.29,0.31,0.4,0.36,0.39,0.41,0.43,0.45,0.5,0.5,0.52,0.53,0.54,0.54,0.56,0.56,0.56,0.57,0.58,0.58,0.58,0.57],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","legend":"legend2","legendgroup":"Line Width through K_1","marker":{"color":"rgba(255, 255, 255, 0.8)","line":{"color":"rgba(50, 50, 50, 1)","width":2},"showscale":false,"size":12,"symbol":"square"},"mode":"markers","name":"Line Width through K","showlegend":true,"x":[null],"y":[null],"z":[null],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.1599999964237213,0.38999998569488525],"y":[-0.8100000023841858,-0.8299999833106995],"z":[-0.5600000023841858,-0.38999998569488525],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.27000001072883606,0.20000000298023224],"y":[-0.7400000095367432,-0.8600000143051147],"z":[-0.6200000047683716,-0.47999998927116394],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.38999998569488525,0.05999999865889549],"y":[-0.6299999952316284,-0.8600000143051147],"z":[-0.6800000071525574,-0.5],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.46000000834465027,-0.03999999910593033],"y":[-0.5400000214576721,-0.8399999737739563],"z":[-0.699999988079071,-0.5400000214576721],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.5199999809265137,-0.1599999964237213],"y":[-0.4399999976158142,-0.8199999928474426],"z":[-0.7300000190734863,-0.5600000023841858],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.5899999737739563,-0.25999999046325684],"y":[-0.33000001311302185,-0.7599999904632568],"z":[-0.7400000095367432,-0.6000000238418579],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.6399999856948853,-0.38999998569488525],"y":[-0.23999999463558197,-0.6800000071525574],"z":[-0.7300000190734863,-0.6200000047683716],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.6499999761581421,-0.47999998927116394],"y":[-0.18000000715255737,-0.6399999856948853],"z":[-0.7400000095367432,-0.6000000238418579],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.6800000071525574,-0.5699999928474426],"y":[-0.07000000029802322,-0.5699999928474426],"z":[-0.7300000190734863,-0.6000000238418579],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.699999988079071,-0.6499999761581421],"y":[0.0,-0.5],"z":[-0.7099999785423279,-0.5699999928474426],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.699999988079071,-0.7200000286102295],"y":[0.09000000357627869,-0.4099999964237213],"z":[-0.7099999785423279,-0.5600000023841858],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.699999988079071,-0.7699999809265137],"y":[0.15000000596046448,-0.28999999165534973],"z":[-0.6899999976158142,-0.5699999928474426],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.7200000286102295,-0.8199999928474426],"y":[0.23000000417232513,-0.17000000178813934],"z":[-0.6600000262260437,-0.550000011920929],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.699999988079071,-0.8500000238418579],"y":[0.3100000023841858,-0.12999999523162842],"z":[-0.6399999856948853,-0.5099999904632568],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.6800000071525574,-0.8799999952316284],"y":[0.36000001430511475,-0.019999999552965164],"z":[-0.6299999952316284,-0.4699999988079071],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.6700000166893005,-0.8999999761581421],"y":[0.4399999976158142,0.09000000357627869],"z":[-0.6000000238418579,-0.4300000071525574],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.6499999761581421,-0.8700000047683716],"y":[0.5,0.1899999976158142],"z":[-0.5699999928474426,-0.46000000834465027],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.6100000143051147,-0.8799999952316284],"y":[0.5699999928474426,0.25],"z":[-0.550000011920929,-0.4000000059604645],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.5699999928474426,-0.8700000047683716],"y":[0.6299999952316284,0.3400000035762787],"z":[-0.5299999713897705,-0.36000001430511475],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.5299999713897705,-0.8500000238418579],"y":[0.6899999976158142,0.41999998688697815],"z":[-0.49000000953674316,-0.3100000023841858],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.47999998927116394,-0.8399999737739563],"y":[0.7400000095367432,0.47999998927116394],"z":[-0.4699999988079071,-0.25999999046325684],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.4099999964237213,-0.75],"y":[0.800000011920929,0.5799999833106995],"z":[-0.4300000071525574,-0.3199999928474426],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.36000001430511475,-0.699999988079071],"y":[0.8399999737739563,0.6499999761581421],"z":[-0.38999998569488525,-0.28999999165534973],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.28999999165534973,-0.6800000071525574],"y":[0.8799999952316284,0.699999988079071],"z":[-0.36000001430511475,-0.20999999344348907],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.23999999463558197,-0.6299999952316284],"y":[0.9200000166893005,0.7599999904632568],"z":[-0.3199999928474426,-0.1599999964237213],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.15000000596046448,-0.5400000214576721],"y":[0.9399999976158142,0.8299999833106995],"z":[-0.30000001192092896,-0.12999999523162842],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.09000000357627869,-0.49000000953674316],"y":[0.9599999785423279,0.8500000238418579],"z":[-0.25999999046325684,-0.1899999976158142],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.019999999552965164,-0.4300000071525574],"y":[0.9700000286102295,0.8899999856948853],"z":[-0.23000000417232513,-0.18000000715255737],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.07999999821186066,-0.3499999940395355],"y":[0.9800000190734863,0.9300000071525574],"z":[-0.20000000298023224,-0.14000000059604645],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.15000000596046448,-0.27000001072883606],"y":[0.9800000190734863,0.9599999785423279],"z":[-0.1599999964237213,-0.10999999940395355],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.2199999988079071,-0.1899999976158142],"y":[0.9700000286102295,0.9800000190734863],"z":[-0.12999999523162842,-0.07000000029802322],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.30000001192092896,-0.10999999940395355],"y":[0.949999988079071,0.9900000095367432],"z":[-0.10999999940395355,-0.10000000149011612],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.3799999952316284,-0.029999999329447746],"y":[0.9200000166893005,1.0],"z":[-0.09000000357627869,-0.07999999821186066],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.44999998807907104,0.03999999910593033],"y":[0.8899999856948853,1.0],"z":[-0.05999999865889549,-0.05000000074505806],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.5099999904632568,0.10999999940395355],"y":[0.8600000143051147,0.9900000095367432],"z":[-0.029999999329447746,-0.019999999552965164],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.5699999928474426,0.20999999344348907],"y":[0.8199999928474426,0.9800000190734863],"z":[-0.0,-0.009999999776482582],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.6299999952316284,0.3199999928474426],"y":[0.7799999713897705,0.949999988079071],"z":[-0.0,-0.05000000074505806],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.6800000071525574,0.38999998569488525],"y":[0.7300000190734863,0.9200000166893005],"z":[0.019999999552965164,-0.05000000074505806],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.7300000190734863,0.4399999976158142],"y":[0.6800000071525574,0.8999999761581421],"z":[0.029999999329447746,-0.029999999329447746],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.7799999713897705,0.5199999809265137],"y":[0.6200000047683716,0.8600000143051147],"z":[0.05000000074505806,-0.029999999329447746],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.8199999928474426,0.5899999737739563],"y":[0.5600000023841858,0.8100000023841858],"z":[0.05999999865889549,-0.009999999776482582],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.8600000143051147,0.6399999856948853],"y":[0.5099999904632568,0.7699999809265137],"z":[0.05999999865889549,-0.05999999865889549],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.8899999856948853,0.699999988079071],"y":[0.44999998807907104,0.7099999785423279],"z":[0.07000000029802322,-0.029999999329447746],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.9200000166893005,0.7300000190734863],"y":[0.4000000059604645,0.6800000071525574],"z":[0.07000000029802322,-0.05000000074505806],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.9399999976158142,0.7699999809265137],"y":[0.33000001311302185,0.6399999856948853],"z":[0.07999999821186066,-0.03999999910593033],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.9599999785423279,0.8299999833106995],"y":[0.2800000011920929,0.5600000023841858],"z":[0.07999999821186066,-0.03999999910593033],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.9700000286102295,0.8399999737739563],"y":[0.20999999344348907,0.5299999713897705],"z":[0.07000000029802322,-0.10000000149011612],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.9900000095367432,0.8799999952316284],"y":[0.15000000596046448,0.46000000834465027],"z":[0.07000000029802322,-0.09000000357627869],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.9900000095367432,0.9100000262260437],"y":[0.09000000357627869,0.4000000059604645],"z":[0.07000000029802322,-0.07000000029802322],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[1.0,0.9399999976158142],"y":[0.019999999552965164,0.3400000035762787],"z":[0.05999999865889549,-0.07000000029802322],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[1.0,0.9700000286102295],"y":[-0.029999999329447746,0.23999999463558197],"z":[0.05999999865889549,-0.09000000357627869],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.9900000095367432,0.9599999785423279],"y":[-0.10000000149011612,0.23000000417232513],"z":[0.05999999865889549,-0.12999999523162842],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.9800000190734863,0.9800000190734863],"y":[-0.17000000178813934,0.17000000178813934],"z":[0.05000000074505806,-0.11999999731779099],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.9800000190734863,0.9900000095367432],"y":[-0.2199999988079071,0.11999999731779099],"z":[0.03999999910593033,-0.10999999940395355],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.9599999785423279,0.9900000095367432],"y":[-0.28999999165534973,0.019999999552965164],"z":[0.029999999329447746,-0.14000000059604645],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.9399999976158142,0.9900000095367432],"y":[-0.3499999940395355,-0.05999999865889549],"z":[0.029999999329447746,-0.12999999523162842],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.9100000262260437,0.9800000190734863],"y":[-0.4099999964237213,-0.10000000149011612],"z":[-0.0,-0.17000000178813934],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.8799999952316284,0.9700000286102295],"y":[-0.4699999988079071,-0.17000000178813934],"z":[0.009999999776482582,-0.1599999964237213],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.8500000238418579,0.9599999785423279],"y":[-0.5199999809265137,-0.23000000417232513],"z":[0.0,-0.18000000715255737],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.800000011920929,0.9300000071525574],"y":[-0.6000000238418579,-0.3100000023841858],"z":[0.009999999776482582,-0.17000000178813934],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.7699999809265137,0.9100000262260437],"y":[-0.6399999856948853,-0.38999998569488525],"z":[-0.019999999552965164,-0.15000000596046448],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.7300000190734863,0.9100000262260437],"y":[-0.6899999976158142,-0.36000001430511475],"z":[-0.05000000074505806,-0.20999999344348907],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.6700000166893005,0.8700000047683716],"y":[-0.7400000095367432,-0.46000000834465027],"z":[-0.029999999329447746,-0.18000000715255737],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.6100000143051147,0.8199999928474426],"y":[-0.7900000214576721,-0.5400000214576721],"z":[-0.029999999329447746,-0.18000000715255737],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.550000011920929,0.7799999713897705],"y":[-0.8399999737739563,-0.6000000238418579],"z":[-0.019999999552965164,-0.1899999976158142],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.49000000953674316,0.7300000190734863],"y":[-0.8700000047683716,-0.6600000262260437],"z":[-0.03999999910593033,-0.18000000715255737],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.4300000071525574,0.75],"y":[-0.8999999761581421,-0.6299999952316284],"z":[-0.05000000074505806,-0.20000000298023224],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.3400000035762787,0.699999988079071],"y":[-0.9399999976158142,-0.6899999976158142],"z":[-0.029999999329447746,-0.20000000298023224],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.2800000011920929,0.6200000047683716],"y":[-0.9599999785423279,-0.7599999904632568],"z":[-0.019999999552965164,-0.1899999976158142],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.20000000298023224,0.5799999833106995],"y":[-0.9800000190734863,-0.800000011920929],"z":[-0.009999999776482582,-0.18000000715255737],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.14000000059604645,0.5199999809265137],"y":[-0.9900000095367432,-0.8299999833106995],"z":[-0.009999999776482582,-0.1899999976158142],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[0.05999999865889549,0.5099999904632568],"y":[-1.0,-0.8399999737739563],"z":[0.009999999776482582,-0.1899999976158142],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.019999999552965164,0.4300000071525574],"y":[-1.0,-0.8799999952316284],"z":[0.009999999776482582,-0.18000000715255737],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.09000000357627869,0.3700000047683716],"y":[-1.0,-0.9200000166893005],"z":[0.029999999329447746,-0.1599999964237213],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.17000000178813934,0.2800000011920929],"y":[-0.9800000190734863,-0.949999988079071],"z":[0.05999999865889549,-0.11999999731779099],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.2199999988079071,0.2199999988079071],"y":[-0.9700000286102295,-0.9700000286102295],"z":[0.07000000029802322,-0.10000000149011612],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.30000001192092896,0.2199999988079071],"y":[-0.949999988079071,-0.9700000286102295],"z":[0.09000000357627869,-0.14000000059604645],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.36000001430511475,0.11999999731779099],"y":[-0.9300000071525574,-0.9900000095367432],"z":[0.10999999940395355,-0.10000000149011612],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.41999998688697815,0.07000000029802322],"y":[-0.8999999761581421,-0.9900000095367432],"z":[0.11999999731779099,-0.07999999821186066],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.47999998927116394,-0.019999999552965164],"y":[-0.8700000047683716,-1.0],"z":[0.11999999731779099,-0.05000000074505806],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.5400000214576721,-0.12999999523162842],"y":[-0.8299999833106995,-0.9900000095367432],"z":[0.15000000596046448,0.0],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.5799999833106995,-0.18000000715255737],"y":[-0.800000011920929,-0.9800000190734863],"z":[0.1599999964237213,0.009999999776482582],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.6200000047683716,-0.20999999344348907],"y":[-0.7599999904632568,-0.9800000190734863],"z":[0.20000000298023224,0.029999999329447746],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.6700000166893005,-0.30000001192092896],"y":[-0.7099999785423279,-0.949999988079071],"z":[0.2199999988079071,0.05000000074505806],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.7099999785423279,-0.3700000047683716],"y":[-0.6600000262260437,-0.9300000071525574],"z":[0.23000000417232513,0.09000000357627869],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.7599999904632568,-0.46000000834465027],"y":[-0.6100000143051147,-0.8799999952316284],"z":[0.23000000417232513,0.10999999940395355],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.7900000214576721,-0.5299999713897705],"y":[-0.550000011920929,-0.8399999737739563],"z":[0.2800000011920929,0.14000000059604645],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.800000011920929,-0.550000011920929],"y":[-0.5299999713897705,-0.8199999928474426],"z":[0.2800000011920929,0.1599999964237213],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.8299999833106995,-0.5899999737739563],"y":[-0.46000000834465027,-0.7799999713897705],"z":[0.3100000023841858,0.20000000298023224],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.8700000047683716,-0.6499999761581421],"y":[-0.4000000059604645,-0.699999988079071],"z":[0.30000001192092896,0.28999999165534973],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.8700000047683716,-0.6800000071525574],"y":[-0.36000001430511475,-0.6600000262260437],"z":[0.33000001311302185,0.3100000023841858],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.8899999856948853,-0.7400000095367432],"y":[-0.3100000023841858,-0.5400000214576721],"z":[0.3400000035762787,0.4000000059604645],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.8999999761581421,-0.7799999713897705],"y":[-0.23999999463558197,-0.5099999904632568],"z":[0.36000001430511475,0.36000001430511475],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.9100000262260437,-0.8100000023841858],"y":[-0.1899999976158142,-0.4399999976158142],"z":[0.36000001430511475,0.38999998569488525],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.9100000262260437,-0.8199999928474426],"y":[-0.1599999964237213,-0.4099999964237213],"z":[0.3799999952316284,0.4099999964237213],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.9300000071525574,-0.8299999833106995],"y":[-0.10000000149011612,-0.3499999940395355],"z":[0.36000001430511475,0.4300000071525574],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.9200000166893005,-0.8700000047683716],"y":[-0.03999999910593033,-0.20999999344348907],"z":[0.38999998569488525,0.44999998807907104],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.9100000262260437,-0.8500000238418579],"y":[0.009999999776482582,-0.15000000596046448],"z":[0.4099999964237213,0.5],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.9200000166893005,-0.8500000238418579],"y":[0.03999999910593033,-0.14000000059604645],"z":[0.4000000059604645,0.5],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.9100000262260437,-0.8500000238418579],"y":[0.10999999940395355,-0.05999999865889549],"z":[0.4000000059604645,0.5199999809265137],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.9100000262260437,-0.8500000238418579],"y":[0.14000000059604645,-0.019999999552965164],"z":[0.4000000059604645,0.5299999713897705],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.8999999761581421,-0.8399999737739563],"y":[0.17000000178813934,0.07000000029802322],"z":[0.4000000059604645,0.5400000214576721],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.8899999856948853,-0.8299999833106995],"y":[0.2199999988079071,0.10999999940395355],"z":[0.4099999964237213,0.5400000214576721],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.8799999952316284,-0.8199999928474426],"y":[0.25999999046325684,0.12999999523162842],"z":[0.4000000059604645,0.5600000023841858],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.8700000047683716,-0.8100000023841858],"y":[0.27000001072883606,0.1899999976158142],"z":[0.4099999964237213,0.5600000023841858],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.8600000143051147,-0.7900000214576721],"y":[0.3199999928474426,0.25],"z":[0.4099999964237213,0.5600000023841858],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.8600000143051147,-0.7799999713897705],"y":[0.3100000023841858,0.23999999463558197],"z":[0.4000000059604645,0.5699999928474426],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.8299999833106995,-0.7599999904632568],"y":[0.36000001430511475,0.28999999165534973],"z":[0.41999998688697815,0.5799999833106995],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.8199999928474426,-0.75],"y":[0.4000000059604645,0.33000001311302185],"z":[0.4000000059604645,0.5799999833106995],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.8199999928474426,-0.7400000095367432],"y":[0.4099999964237213,0.3499999940395355],"z":[0.4000000059604645,0.5799999833106995],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","line":{"color":"rgba(0, 0, 0, 1)","width":1},"mode":"lines","showlegend":false,"x":[-0.800000011920929,-0.7099999785423279],"y":[0.44999998807907104,0.4099999964237213],"z":[0.38999998569488525,0.5699999928474426],"type":"scatter3d","scene":"scene2"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"scene":{"domain":{"x":[0.0,0.475],"y":[0.0,1.0]},"xaxis":{"title":{"text":""},"range":[-0.83,0.85],"showticklabels":false,"showgrid":false,"backgroundcolor":"rgba(255, 255, 255, 0)","gridcolor":"rgba(255, 255, 255, 0)","showbackground":false,"showspikes":false},"yaxis":{"title":{"text":""},"range":[-0.7999999999999999,0.86],"showticklabels":false,"showgrid":false,"backgroundcolor":"rgba(255, 255, 255, 0)","gridcolor":"rgba(255, 255, 255, 0)","showbackground":false,"showspikes":false},"zaxis":{"title":{"text":""},"range":[-0.56,0.82],"showticklabels":false,"showgrid":false,"backgroundcolor":"rgba(255, 255, 255, 0)","gridcolor":"rgba(255, 255, 255, 0)","showbackground":false,"showspikes":false},"camera":{"eye":{"x":1,"y":1,"z":1},"center":{"x":0,"y":0,"z":0}},"dragmode":"orbit","aspectmode":"cube"},"scene2":{"domain":{"x":[0.525,1.0],"y":[0.0,1.0]},"xaxis":{"title":{"text":""},"range":[-1.0300000071525575,1.1],"showticklabels":false,"showgrid":false,"backgroundcolor":"rgba(255, 255, 255, 0)","gridcolor":"rgba(255, 255, 255, 0)","showbackground":false,"showspikes":false},"yaxis":{"title":{"text":""},"range":[-1.1,1.1],"showticklabels":false,"showgrid":false,"backgroundcolor":"rgba(255, 255, 255, 0)","gridcolor":"rgba(255, 255, 255, 0)","showbackground":false,"showspikes":false},"zaxis":{"title":{"text":""},"range":[-0.8400000095367431,0.6799999833106994],"showticklabels":false,"showgrid":false,"backgroundcolor":"rgba(255, 255, 255, 0)","gridcolor":"rgba(255, 255, 255, 0)","showbackground":false,"showspikes":false},"camera":{"eye":{"x":1,"y":1,"z":1},"center":{"x":0,"y":0,"z":0}},"dragmode":"orbit","aspectmode":"cube"},"annotations":[{"font":{"size":16},"showarrow":false,"text":"In the residual stream","x":0.2375,"xanchor":"center","xref":"paper","y":0.95,"yanchor":"bottom","yref":"paper"},{"font":{"size":16},"showarrow":false,"text":"In boundary head QK space","x":0.7625,"xanchor":"center","xref":"paper","y":0.95,"yanchor":"bottom","yref":"paper"}],"title":{"text":"\u003cb\u003eAlignment Between Character Count Probes and Line Width Probes\u003c\u002fb\u003e","x":0.5,"xanchor":"center","y":0.98},"margin":{"l":0,"r":0,"t":30,"b":0},"legend":{"itemsizing":"constant","orientation":"h","x":0.25,"xanchor":"center","y":0.02,"yanchor":"bottom"},"legend2":{"itemsizing":"constant","orientation":"h","x":0.75,"xanchor":"center","y":0.02,"yanchor":"bottom"},"width":1200,"height":600,"showlegend":true,"font":{"family":"-apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, Oxygen, Ubuntu, Cantarell, \"Fira Sans\", \"Droid Sans\", \"Helvetica Neue\", Arial, sans-serif","color":"rgba(0, 0, 0, 0.8)"}},                        {"displayModeBar": false, "responsive": true}                    )                };                            </script>        </div>
<figcaption class='text-caption'>Boundary heads twist the representation of line width and character count to detect the line boundary.<br>Left: Joint PCA of character count and line width probes.<br>Right: Same after multiplying them through the corresponding QK weights of the boundary head. Range is from 40 (dark) to 150 (light).</figcaption></figure>
<p>We find that this attention head “twists” the character count manifold such that character count <d-math>i</d-math> is aligned with line width <d-math>k=i+\epsilon</d-math>. This causes the head to attend to the newline when the character count is just a bit <span style='font-style: italic;'>less</span> than the line width, thereby indicating that the boundary is approaching. This algorithm is quite general, and enables this head to detect approaching line boundaries for arbitrary line widths!<d-footnote>This algorithm also generalizes to arbitrary kinds of separators (e.g., double newlines or pipes), as the QK circuit can handle the positional offset independently of the OV circuit copying the separator type.</d-footnote></p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_011.png' /><figcaption class='text-caption'>Cosine similarity of previous line width and character count probes through different transforms. (Left) the identity map, (Center) QK of boundary head, (Right) QK of random head in the same layer. Boundary heads align the probes but with a small offset.</figcaption></figure>
<p>This plot shows that</p>
<ul><li style='margin-left: 36pt;'>In the residual stream, probes for character count <d-math>i</d-math> are maximally aligned with probes line width probes <d-math>k</d-math> when <d-math>i=k</d-math>, but are not highly aligned in absolute terms – the maximum cosine sim is ~0.25. </li><li style='margin-left: 36pt;'>In the QK space of the boundary head, the probes are maximally aligned on the offdiagonal <d-math>i < k</d-math>, and are almost perfectly aligned in absolute terms – the maximum cosine sim is <d-math>\approx 1</d-math>.</li><li style='margin-left: 36pt;'>In the QK space of a random head, there is almost no structure between the probes.</li></ul>
<p>As a consequence of the ringing in the character count representations, we also observe ringing in the inner products (see <a href='#rippled-representations'>Rippled Representations are Optimal</a> above). The model is robust to these off-diagonal interference terms via the softmax applied to attention scores.</p>
<h4>Leveraging Multiple Boundary Heads</h4>
<p>We find that the model actually uses multiple boundary heads, each twisting the manifolds by a different offset to implement a kind of “stereoscopic” algorithm for computing the number of characters remaining.<d-footnote>There are also multiple sets of boundary heads at multiple layers that usually come in sets of ~3 with similar relative offsets (so not actually “stereo”).</d-footnote> We attach more visualizations of boundary heads in the <a href='#appendix-twisting'>Appendix</a>.</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_012.png' /><figcaption class='text-caption'>Cosine similarity of line width and character count probes through three different boundary heads in the same layer with different amounts of twisting. Green line indicates the argmax for each row, and is used to calculate the average offset reported in the subtitles.</figcaption></figure>
<p>To better understand each boundary head’s output, we train a set of probes for each value of characters remaining in the line (i.e., the line width <d-math>k</d-math> minus the character count <d-math>i</d-math>, restricted to <d-math>k - i < 40</d-math>). For each boundary head, we show the proportion of attention on the newline, as well as the norm of each head’s output projected onto the probe space as a function of characters remaining. </p>
<p>As predicted by our weights based analysis, we observe that boundary heads have distinct but overlapping response curves that “tile” the possible values of characters remaining.</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_013.png' /><figcaption class='text-caption'>Each boundary head’s response curve peaks at a different distance from the end of the line.</figcaption></figure>
<p>It's worth understanding why the model needs multiple boundary heads rather than just one. If the model relied only on boundary head 0, it couldn't distinguish between 5 characters remaining and 17 characters remaining—both would produce similar outputs. By having each head's output vary most significantly in different ranges, their sum achieves high resolution across the entire relevant range of “Characters Remaining” values.</p>
<p>We can see this more clearly by plotting each head's output in the first two principal components of the characters remaining space (which captures 92% of the variance). Head 0 shows large variance in the [0, 10] and [15, 20] ranges, Head 1 varies most in the [10, 20] range, and Head 2 varies most in the [5, 15] range. While no single head provides high resolution across the entire curve, their sum produces an evenly spaced representation that covers all values effectively.</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_014.png' /><figcaption class='text-caption'>Each head’s output as a function of characters remaining, and their sum in the PCA basis. Individual head outputs are almost one-dimensional, while the sum is a two-dimensional curve.</figcaption></figure>
<p>We validate the causal importance of this two-dimensional subspace by performing an ablation and intervention experiment. Specifically, we conduct the same experiments as before: ablate the subspace and measure its effect on loss by token (left) and precisely modulate the characters remaining estimate on the last token in the aluminum prompt by substituting mean activation vectors.</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_015.png' /><figcaption class='text-caption'>Characters remaining subspace can be causally intervened upon. (Left) Ablating the subspace has a large effect only when the next token is a newline. (Right) We surgically intervene on the characters remaining space to modulate the prediction of the newline by subtracting the true characters remaining mean activation and adding in a patched characters remaining activations. Note that the completion “ aluminum.” requires ten characters to fit.</figcaption></figure>
<h4><a id='one-dim' href='#one-dim'>The Role of the Extra Dimensions</a></h4>
<p>We are now in a position to understand two distinct but related questions: (1) why these counting representations are multidimensional and (2) why multiple attention heads are required to compute these multidimensional representations.</p>
<p><span style='font-weight: 700;'>Geometric Computations</span> – A multi-dimensional representation enables the model to rotate position encodings using linear transformations—something impossible with one-dimensional representations. For instance, to detect an approaching line boundary, the model can rotate the position manifold to align with line width, then use a dot product to identify when only a few characters remain. With a 1D encoding, linear operations reduce to scaling and translation, so comparing position against line width would just multiply the two values, producing a monotonically increasing result with no natural threshold. Higher dimensions beyond 2D allow the manifold to pack more information through additional curvature.</p>
<p><span style='font-weight: 700;'>Resolution</span> – For character counting, the model must distinguish between adjacent counts for a large range of character positions, as this determines whether the next word fits. In a one-dimensional representation, positions would be arranged along a ray, with each position separated by some constant <d-math>\delta</d-math>. To reliably distinguish adjacent positions above noise, we need <d-math>||v_{42} - v_{41}|| = \delta</d-math> to exceed some threshold. But with 150+ positions to represent, this creates an untenable choice: either use enormous dynamic range (<d-math>||v_{150}|| \gg ||v_1||</d-math>), which is problematic for transformer computations, or sacrifice resolution between adjacent positions. (Normalization blocks only exacerbate this effect: while points can be spaced far away on a ray if their norms get large enough, there is at most <d-math>\pi</d-math> worth of angular distance along the projection of that ray onto the unit hypersphere.) Embedding the curve into higher dimensions solves this: positions maintain similar norms while being well-separated in the ambient space, achieving fine resolution without norm explosion (See <a href='#rippled-representations'>Rippled Representations are Optimal</a> above.) For counting the characters remaining, the dynamic range is smaller, and so the model is able to embed the representation in a smaller subspace as a result.</p>
<p>To achieve the curvature for necessary high resolution, multiple attention heads are needed to cooperatively construct the curved geometry of the counting manifold. An individual attention head's output is a linear combination of its inputs (weighted by attention and transformed by the OV circuit), and thus is fundamentally constrained by the curvature already present in those inputs. In the absence of MLP contributions to the counting representation, if the output manifold needs to exhibit substantial curvature, multiple attention heads need to coordinate—each contributing a piece of the overall geometric structure. We will see another example of distributed head computation in the section on the <a href='#count-algo'>Distributed Character Counting Algorithm</a>.</p>
<h4><a id='discovery' href='#discovery'>A Discovery Story</a></h4>
<p>How did we originally find this boundary detection mechanism? When we first computed an attribution graph, we saw several edges from the previous newline features and embedding to predict-newline features. QK attributions showed that the top key feature was a “the previous line was 40–60 characters long” feature and the top query feature was “the current character count is 35–50” feature. At any one time there were often multiple counting features active at different strengths, suggesting that these features might be discretizing a manifold.</p>
<figure class="gdoc-image" style="--img-width: 1360px; "><img src='img_016.png' /></figure>
<p>The boundary heads cause a family of boundary detecting features to activate in response to how close the current line is to the global line width. That is, they sense the approaching line boundary or the reverse index of the line count. Investigating these three sets of feature families led us to the count manifolds which they sparsely parametrize, and investigating the relevant attention heads let us find the boundary heads.</p>
<p>Finally, we note that these boundary-sensing representations parallels a well-studied phenomenon in neuroscience: boundary cells <d-cite key="solstad2008representation"></d-cite>, which activate at specific distances from environmental boundaries (e.g., walls). Both the artificial features and biological cells come in families with varied receptive fields and offsets.</p>
<h3><a id='prediction' href='#prediction'>Predicting the Newline</a></h3>
<p>The final step of the linebreak task is to combine the estimate of the line boundary with the prediction of the next word to determine whether the next word will fit on the line, or if the line should be broken.</p>
<p>In the attribution graph for the aluminum prompt, we see exactly this merging of paths. The most influential feature<d-footnote>Influence in the sense of influence on the logit node, as defined in Ameisen et al. <d-cite key="ameisen2025circuit"></d-cite></d-footnote> in the entire graph is a late feature that activates in contexts where the next word would cause the current line to exceed the overall line width. For our prompt, this feature upweights the probability of newline and downweights the probability of “aluminum.” The top two inputs to this break predictor feature are a “say aluminum” feature and “boundary detecting” feature that gets activated by the aforementioned boundary head.</p>
<figure class="gdoc-image" style="--img-width: 1400px; "><img src='img_017.png' /></figure>
<p>While the boundary detector activates regardless of the next token length, break<span style='font-style: italic;'> </span>predictor<span style='font-style: italic;'> </span>features activate only if the next token will exceed the length of the current line (as in the Aluminum prompt), and hence upweight the prediction of a newline.<d-footnote>These features also sometimes activate on zero-width <span style='font-style: italic;'>modifier</span> tokens (e.g., a token which indicates the first letter of the following token should be capitalized) that need to be adjacent to the modified token, and the modified token is sufficiently long to go over the line limit (e.g. for “Aluminum” instead of “aluminum”).</d-footnote> We also see break <span style='font-style: italic;'>suppressor</span> features, which only activate if the next token would just barely fit on the line, and hence downweight the prediction of a newline. Both break predictors and suppressors come in larger feature families, which we display in the <a href='#appendix-break-predictors'>Appendix</a>.</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_018.png' /><figcaption class='text-caption'>Average activations of three features based on true next token character length and the characters remaining in a line (line width − character count).</figcaption></figure>
<h4>Joint Geometry Enables Easy Computation</h4>
<p>What is the geometry underlying the model’s ability to determine if the next token will fit on the line? Put another way, how is the break predictor feature above constructed from the boundary detector and next-word features?</p>
<p>To study this, we compute the average activations at the end of the model (~90% depth) across all tokens for all values of characters remaining <d-math>i</d-math> and next token lengths <d-math>j</d-math>.<d-footnote>We use the true next non-newline token as the label. This is an approximation because it assumes that the model perfectly predicts the next token. </d-footnote> By performing a PCA on the combination of mean vectors, we see that the two counts are arranged in orthogonal subspaces with only moderate curvature. Note, this lower dimensional geometry may suffice here because the dynamic range of the count is much smaller. </p>
<figure key="linear_classifier" class="gdoc-image">
<script src="https://cdn.plot.ly/plotly-2.32.0.min.js" integrity="sha384-7TVmlZWH60iKX5Uk7lSvQhjtcgw2tkFjuwLcXoRSR4zXTyWFJRm9aPAguMh7CIra" crossorigin="anonymous"></script>
<div>                            <div id="e0c662f8-1496-4040-8148-9983e106c288" class="plotly-graph-div" style="height:600px; width:1200px;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("e0c662f8-1496-4040-8148-9983e106c288")) {                    Plotly.newPlot(                        "e0c662f8-1496-4040-8148-9983e106c288",                        [{"hovertemplate":"%{text}","legend":"legend","legendgroup":"Next Token Length_0","line":{"color":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14],"colorscale":[[0.0,"#440154"],[0.1111111111111111,"#482878"],[0.2222222222222222,"#3e4989"],[0.3333333333333333,"#31688e"],[0.4444444444444444,"#26828e"],[0.5555555555555556,"#1f9e89"],[0.6666666666666666,"#35b779"],[0.7777777777777778,"#6ece58"],[0.8888888888888888,"#b5de2b"],[1.0,"#fde725"]],"width":2},"marker":{"color":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15],"colorscale":[[0.0,"#440154"],[0.1111111111111111,"#482878"],[0.2222222222222222,"#3e4989"],[0.3333333333333333,"#31688e"],[0.4444444444444444,"#26828e"],[0.5555555555555556,"#1f9e89"],[0.6666666666666666,"#35b779"],[0.7777777777777778,"#6ece58"],[0.8888888888888888,"#b5de2b"],[1.0,"#fde725"]],"opacity":1,"showscale":false,"size":4,"symbol":"diamond"},"mode":"lines+markers","name":"Next Token Length","showlegend":false,"text":["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15"],"x":[9.55,4.0,4.38,2.87,1.63,0.46,0.2,-0.52,-1.48,-1.73,-2.89,-3.44,-2.99,-4.37,-5.67],"y":[-10.13,-7.81,-8.07,-5.73,-4.48,-1.28,0.73,1.56,2.97,3.08,4.43,5.43,5.5,6.42,7.45],"z":[5.65,1.79,0.32,-0.34,-0.45,-1.16,-1.31,-1.13,-0.99,-0.78,-0.61,-0.53,-0.3,-0.18,0.03],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","legend":"legend","legendgroup":"Next Token Length_0","marker":{"color":"rgba(255, 255, 255, 0.8)","line":{"color":"rgba(50, 50, 50, 1)","width":2},"showscale":false,"size":12,"symbol":"diamond"},"mode":"markers","name":"Next Token Length","showlegend":true,"x":[null],"y":[null],"z":[null],"type":"scatter3d","scene":"scene"},{"hovertemplate":"%{text}","legend":"legend","legendgroup":"Characters Remaining_0","line":{"color":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14],"colorscale":[[0.0,"#000004"],[0.1111111111111111,"#1b0c41"],[0.2222222222222222,"#4a0c6b"],[0.3333333333333333,"#781c6d"],[0.4444444444444444,"#a52c60"],[0.5555555555555556,"#cf4446"],[0.6666666666666666,"#ed6925"],[0.7777777777777778,"#fb9b06"],[0.8888888888888888,"#f7d13d"],[1.0,"#fcffa4"]],"width":2},"marker":{"color":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15],"colorscale":[[0.0,"#000004"],[0.1111111111111111,"#1b0c41"],[0.2222222222222222,"#4a0c6b"],[0.3333333333333333,"#781c6d"],[0.4444444444444444,"#a52c60"],[0.5555555555555556,"#cf4446"],[0.6666666666666666,"#ed6925"],[0.7777777777777778,"#fb9b06"],[0.8888888888888888,"#f7d13d"],[1.0,"#fcffa4"]],"opacity":1,"showscale":false,"size":4,"symbol":"square"},"mode":"lines+markers","name":"Characters Remaining","showlegend":false,"text":["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15"],"x":[25.96,24.41,20.5,14.99,8.97,3.64,-1.19,-5.64,-8.83,-11.21,-12.87,-13.87,-14.59,-15.02,-15.27],"y":[7.64,5.5,2.76,0.09,-1.65,-2.5,-2.67,-2.32,-1.9,-1.46,-1.05,-0.79,-0.59,-0.49,-0.51],"z":[7.13,3.6,-1.26,-5.51,-7.68,-7.87,-6.43,-3.83,-1.05,1.23,2.92,4.03,4.61,4.97,5.15],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","legend":"legend","legendgroup":"Characters Remaining_0","marker":{"color":"rgba(255, 255, 255, 0.8)","line":{"color":"rgba(50, 50, 50, 1)","width":2},"showscale":false,"size":12,"symbol":"square"},"mode":"markers","name":"Characters Remaining","showlegend":true,"x":[null],"y":[null],"z":[null],"type":"scatter3d","scene":"scene"},{"hovertemplate":"%{text}","legend":"legend2","legendgroup":"Next Token Length_1","line":{"color":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14],"colorscale":[[0.0,"#440154"],[0.1111111111111111,"#482878"],[0.2222222222222222,"#3e4989"],[0.3333333333333333,"#31688e"],[0.4444444444444444,"#26828e"],[0.5555555555555556,"#1f9e89"],[0.6666666666666666,"#35b779"],[0.7777777777777778,"#6ece58"],[0.8888888888888888,"#b5de2b"],[1.0,"#fde725"]],"width":2},"marker":{"color":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15],"colorscale":[[0.0,"#440154"],[0.1111111111111111,"#482878"],[0.2222222222222222,"#3e4989"],[0.3333333333333333,"#31688e"],[0.4444444444444444,"#26828e"],[0.5555555555555556,"#1f9e89"],[0.6666666666666666,"#35b779"],[0.7777777777777778,"#6ece58"],[0.8888888888888888,"#b5de2b"],[1.0,"#fde725"]],"opacity":1,"showscale":false,"size":4,"symbol":"diamond"},"mode":"lines+markers","name":"Next Token Length","showlegend":false,"text":["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15"],"x":[9.55,4.0,4.38,2.87,1.63,0.46,0.2,-0.52,-1.48,-1.73,-2.89,-3.44,-2.99,-4.37,-5.67],"y":[-10.13,-7.81,-8.07,-5.73,-4.48,-1.28,0.73,1.56,2.97,3.08,4.43,5.43,5.5,6.42,7.45],"z":[5.65,1.79,0.32,-0.34,-0.45,-1.16,-1.31,-1.13,-0.99,-0.78,-0.61,-0.53,-0.3,-0.18,0.03],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","legend":"legend2","legendgroup":"Next Token Length_1","marker":{"color":"rgba(255, 255, 255, 0.8)","line":{"color":"rgba(50, 50, 50, 1)","width":2},"showscale":false,"size":12,"symbol":"diamond"},"mode":"markers","name":"Next Token Length","showlegend":true,"x":[null],"y":[null],"z":[null],"type":"scatter3d","scene":"scene2"},{"hovertemplate":"%{text}","legend":"legend2","legendgroup":"Characters Remaining_1","line":{"color":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14],"colorscale":[[0.0,"#000004"],[0.1111111111111111,"#1b0c41"],[0.2222222222222222,"#4a0c6b"],[0.3333333333333333,"#781c6d"],[0.4444444444444444,"#a52c60"],[0.5555555555555556,"#cf4446"],[0.6666666666666666,"#ed6925"],[0.7777777777777778,"#fb9b06"],[0.8888888888888888,"#f7d13d"],[1.0,"#fcffa4"]],"width":2},"marker":{"color":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15],"colorscale":[[0.0,"#000004"],[0.1111111111111111,"#1b0c41"],[0.2222222222222222,"#4a0c6b"],[0.3333333333333333,"#781c6d"],[0.4444444444444444,"#a52c60"],[0.5555555555555556,"#cf4446"],[0.6666666666666666,"#ed6925"],[0.7777777777777778,"#fb9b06"],[0.8888888888888888,"#f7d13d"],[1.0,"#fcffa4"]],"opacity":1,"showscale":false,"size":4,"symbol":"square"},"mode":"lines+markers","name":"Characters Remaining","showlegend":false,"text":["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15"],"x":[25.96,24.41,20.5,14.99,8.97,3.64,-1.19,-5.64,-8.83,-11.21,-12.87,-13.87,-14.59,-15.02,-15.27],"y":[7.64,5.5,2.76,0.09,-1.65,-2.5,-2.67,-2.32,-1.9,-1.46,-1.05,-0.79,-0.59,-0.49,-0.51],"z":[7.13,3.6,-1.26,-5.51,-7.68,-7.87,-6.43,-3.83,-1.05,1.23,2.92,4.03,4.61,4.97,5.15],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","legend":"legend2","legendgroup":"Characters Remaining_1","marker":{"color":"rgba(255, 255, 255, 0.8)","line":{"color":"rgba(50, 50, 50, 1)","width":2},"showscale":false,"size":12,"symbol":"square"},"mode":"markers","name":"Characters Remaining","showlegend":true,"x":[null],"y":[null],"z":[null],"type":"scatter3d","scene":"scene2"},{"hovertemplate":"%{text}","legend":"legend2","legendgroup":"Margin After Next Word_1","marker":{"color":[0,-1,-2,-3,-4,-5,-6,-7,-8,-9,-10,-11,-12,-13,-14,1,0,-1,-2,-3,-4,-5,-6,-7,-8,-9,-10,-11,-12,-13,2,1,0,-1,-2,-3,-4,-5,-6,-7,-8,-9,-10,-11,-12,3,2,1,0,-1,-2,-3,-4,-5,-6,-7,-8,-9,-10,-11,4,3,2,1,0,-1,-2,-3,-4,-5,-6,-7,-8,-9,-10,5,4,3,2,1,0,-1,-2,-3,-4,-5,-6,-7,-8,-9,6,5,4,3,2,1,0,-1,-2,-3,-4,-5,-6,-7,-8,7,6,5,4,3,2,1,0,-1,-2,-3,-4,-5,-6,-7,8,7,6,5,4,3,2,1,0,-1,-2,-3,-4,-5,-6,9,8,7,6,5,4,3,2,1,0,-1,-2,-3,-4,-5,10,9,8,7,6,5,4,3,2,1,0,-1,-2,-3,-4,11,10,9,8,7,6,5,4,3,2,1,0,-1,-2,-3,12,11,10,9,8,7,6,5,4,3,2,1,0,-1,-2,13,12,11,10,9,8,7,6,5,4,3,2,1,0,-1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,0],"colorscale":[[0.0,"rgb(103,0,31)"],[0.1,"rgb(178,24,43)"],[0.2,"rgb(214,96,77)"],[0.3,"rgb(244,165,130)"],[0.4,"rgb(253,219,199)"],[0.5,"rgb(247,247,247)"],[0.6,"rgb(209,229,240)"],[0.7,"rgb(146,197,222)"],[0.8,"rgb(67,147,195)"],[0.9,"rgb(33,102,172)"],[1.0,"rgb(5,48,97)"]],"opacity":0.4,"showscale":false,"size":10,"symbol":"circle"},"mode":"markers","name":"Margin After Next Word","showlegend":false,"text":["0","-1","-2","-3","-4","-5","-6","-7","-8","-9","-10","-11","-12","-13","-14","1","0","-1","-2","-3","-4","-5","-6","-7","-8","-9","-10","-11","-12","-13","2","1","0","-1","-2","-3","-4","-5","-6","-7","-8","-9","-10","-11","-12","3","2","1","0","-1","-2","-3","-4","-5","-6","-7","-8","-9","-10","-11","4","3","2","1","0","-1","-2","-3","-4","-5","-6","-7","-8","-9","-10","5","4","3","2","1","0","-1","-2","-3","-4","-5","-6","-7","-8","-9","6","5","4","3","2","1","0","-1","-2","-3","-4","-5","-6","-7","-8","7","6","5","4","3","2","1","0","-1","-2","-3","-4","-5","-6","-7","8","7","6","5","4","3","2","1","0","-1","-2","-3","-4","-5","-6","9","8","7","6","5","4","3","2","1","0","-1","-2","-3","-4","-5","10","9","8","7","6","5","4","3","2","1","0","-1","-2","-3","-4","11","10","9","8","7","6","5","4","3","2","1","0","-1","-2","-3","12","11","10","9","8","7","6","5","4","3","2","1","0","-1","-2","13","12","11","10","9","8","7","6","5","4","3","2","1","0","-1","14","13","12","11","10","9","8","7","6","5","4","3","2","1","0"],"x":[31.79,26.9,26.81,24.93,23.64,22.15,21.71,20.87,19.87,19.6,18.5,17.93,18.31,17.05,15.87,31.39,26.5,26.41,24.53,23.24,21.75,21.31,20.47,19.47,19.21,18.1,17.53,17.92,16.66,15.47,28.6,23.72,23.63,21.75,20.46,18.97,18.53,17.69,16.69,16.42,15.32,14.75,15.14,13.88,12.69,23.84,18.95,18.87,16.98,15.69,14.2,13.76,12.93,11.92,11.66,10.55,9.98,10.37,9.11,7.93,18.4,13.51,13.42,11.54,10.25,8.76,8.32,7.49,6.48,6.22,5.11,4.54,4.93,3.67,2.49,13.53,8.64,8.55,6.67,5.38,3.89,3.45,2.62,1.61,1.34,0.24,-0.33,0.06,-1.2,-2.39,9.11,4.23,4.14,2.26,0.96,-0.53,-0.97,-1.8,-2.81,-3.07,-4.18,-4.75,-4.36,-5.62,-6.8,5.13,0.24,0.15,-1.73,-3.02,-4.51,-4.95,-5.78,-6.79,-7.05,-8.16,-8.73,-8.34,-9.6,-10.78,2.31,-2.57,-2.66,-4.54,-5.83,-7.33,-7.77,-8.6,-9.61,-9.87,-10.97,-11.55,-11.16,-12.42,-13.6,0.18,-4.71,-4.8,-6.68,-7.97,-9.46,-9.9,-10.74,-11.74,-12.01,-13.11,-13.68,-13.29,-14.55,-15.74,-1.35,-6.24,-6.33,-8.21,-9.5,-10.99,-11.43,-12.27,-13.27,-13.54,-14.64,-15.21,-14.82,-16.08,-17.27,-2.28,-7.16,-7.25,-9.13,-10.42,-11.92,-12.36,-13.19,-14.2,-14.46,-15.57,-16.14,-15.75,-17.01,-18.19,-2.95,-7.83,-7.92,-9.8,-11.09,-12.58,-13.02,-13.86,-14.87,-15.13,-16.23,-16.81,-16.42,-17.68,-18.86,-3.32,-8.21,-8.3,-10.18,-11.47,-12.96,-13.4,-14.23,-15.24,-15.5,-16.61,-17.18,-16.79,-18.05,-19.23,-3.52,-8.41,-8.5,-10.38,-11.67,-13.16,-13.6,-14.43,-15.44,-15.7,-16.81,-17.38,-16.99,-18.25,-19.43],"y":[-4.63,-2.73,-2.06,1.01,2.24,5.97,8.05,9.15,10.66,10.86,12.21,13.3,13.34,14.29,15.24,-6.36,-4.46,-3.79,-0.72,0.51,4.24,6.32,7.41,8.93,9.13,10.47,11.57,11.61,12.56,13.51,-8.89,-6.99,-6.32,-3.24,-2.02,1.71,3.79,4.89,6.4,6.6,7.95,9.04,9.08,10.03,10.98,-11.32,-9.42,-8.75,-5.67,-4.45,-0.72,1.36,2.46,3.97,4.17,5.52,6.61,6.65,7.6,8.55,-13.04,-11.14,-10.47,-7.39,-6.17,-2.44,-0.36,0.74,2.26,2.45,3.8,4.89,4.93,5.88,6.84,-14.03,-12.13,-11.46,-8.38,-7.16,-3.43,-1.34,-0.25,1.27,1.47,2.81,3.9,3.94,4.89,5.85,-14.28,-12.37,-11.71,-8.63,-7.41,-3.68,-1.59,-0.5,1.02,1.22,2.56,3.65,3.69,4.64,5.6,-14.05,-12.15,-11.48,-8.41,-7.18,-3.45,-1.37,-0.28,1.24,1.44,2.78,3.88,3.91,4.87,5.82,-13.77,-11.86,-11.2,-8.12,-6.9,-3.17,-1.08,0.01,1.53,1.73,3.07,4.16,4.2,5.15,6.11,-13.38,-11.48,-10.81,-7.74,-6.51,-2.78,-0.7,0.39,1.91,2.11,3.45,4.55,4.58,5.54,6.49,-12.91,-11.01,-10.34,-7.26,-6.04,-2.31,-0.23,0.87,2.39,2.59,3.93,5.02,5.06,6.01,6.97,-12.66,-10.76,-10.09,-7.01,-5.79,-2.06,0.02,1.12,2.63,2.83,4.18,5.27,5.31,6.26,7.21,-12.42,-10.52,-9.85,-6.77,-5.55,-1.82,0.26,1.36,2.88,3.07,4.42,5.51,5.55,6.5,7.46,-12.3,-10.4,-9.74,-6.66,-5.44,-1.7,0.38,1.47,2.99,3.19,4.53,5.62,5.66,6.61,7.57,-12.32,-10.42,-9.75,-6.67,-5.45,-1.72,0.36,1.46,2.97,3.17,4.52,5.61,5.65,6.6,7.55],"z":[13.76,10.53,8.76,7.86,7.6,6.84,6.68,6.75,6.85,7.04,7.21,7.24,7.48,7.59,7.89,10.49,7.25,5.49,4.59,4.33,3.57,3.41,3.48,3.58,3.77,3.93,3.97,4.21,4.32,4.61,5.55,2.32,0.55,-0.35,-0.61,-1.37,-1.53,-1.46,-1.36,-1.17,-1.0,-0.97,-0.73,-0.62,-0.32,1.28,-1.96,-3.72,-4.62,-4.88,-5.64,-5.8,-5.73,-5.63,-5.44,-5.28,-5.24,-5.0,-4.89,-4.6,-1.41,-4.64,-6.4,-7.31,-7.56,-8.32,-8.48,-8.42,-8.31,-8.12,-7.96,-7.92,-7.68,-7.57,-7.28,-2.05,-5.29,-7.05,-7.95,-8.21,-8.97,-9.13,-9.06,-8.96,-8.77,-8.61,-8.57,-8.33,-8.22,-7.93,-0.93,-4.17,-5.93,-6.83,-7.09,-7.85,-8.01,-7.94,-7.84,-7.65,-7.48,-7.45,-7.21,-7.1,-6.8,1.41,-1.83,-3.59,-4.49,-4.75,-5.51,-5.67,-5.6,-5.5,-5.31,-5.15,-5.11,-4.87,-4.76,-4.46,4.08,0.84,-0.92,-1.82,-2.08,-2.84,-3.0,-2.93,-2.83,-2.64,-2.47,-2.44,-2.2,-2.09,-1.79,6.35,3.11,1.35,0.45,0.19,-0.57,-0.73,-0.66,-0.56,-0.37,-0.21,-0.17,0.07,0.18,0.47,8.07,4.84,3.07,2.17,1.91,1.15,0.99,1.06,1.17,1.36,1.52,1.55,1.79,1.9,2.2,9.22,5.99,4.22,3.32,3.06,2.3,2.14,2.21,2.31,2.51,2.67,2.7,2.94,3.05,3.35,9.84,6.61,4.85,3.95,3.69,2.93,2.77,2.83,2.94,3.13,3.29,3.33,3.57,3.68,3.97,10.22,6.99,5.22,4.32,4.06,3.3,3.14,3.21,3.32,3.51,3.67,3.7,3.94,4.05,4.35,10.43,7.2,5.43,4.53,4.27,3.51,3.35,3.42,3.52,3.71,3.88,3.91,4.15,4.26,4.56],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","legend":"legend2","legendgroup":"Margin After Next Word_1","marker":{"color":"rgba(255, 255, 255, 0.8)","line":{"color":"rgba(50, 50, 50, 1)","width":2},"showscale":false,"size":12,"symbol":"circle"},"mode":"markers","name":"Margin After Next Word","showlegend":true,"x":[null],"y":[null],"z":[null],"type":"scatter3d","scene":"scene2"},{"colorscale":[[0,"rgba(0, 150, 200, 0.3)"],[1,"rgba(0, 150, 200, 0.3)"]],"contours":{"x":{"highlight":false},"y":{"highlight":false},"z":{"highlight":false}},"hoverinfo":"skip","name":"Decision Boundary","showlegend":false,"showscale":false,"x":[[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29],[-19.93,-18.87,-17.8,-16.74,-15.67,-14.61,-13.54,-12.47,-11.41,-10.34,-9.28,-8.21,-7.15,-6.08,-5.01,-3.95,-2.88,-1.82,-0.75,0.31,1.38,2.45,3.51,4.58,5.64,6.71,7.77,8.84,9.91,10.97,12.04,13.1,14.17,15.23,16.3,17.37,18.43,19.5,20.56,21.63,22.69,23.76,24.83,25.89,26.96,28.02,29.09,30.15,31.22,32.29]],"y":[[-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78,-14.78],[-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15,-14.15],[-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53,-13.53],[-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91,-12.91],[-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28,-12.28],[-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66,-11.66],[-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04,-11.04],[-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42,-10.42],[-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79,-9.79],[-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17,-9.17],[-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55,-8.55],[-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92,-7.92],[-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3,-7.3],[-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68,-6.68],[-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06,-6.06],[-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43,-5.43],[-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81,-4.81],[-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19,-4.19],[-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57,-3.57],[-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94,-2.94],[-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32,-2.32],[-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7,-1.7],[-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07,-1.07],[-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45,-0.45],[0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17],[0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79,0.79],[1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42,1.42],[2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04,2.04],[2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66,2.66],[3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29,3.29],[3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91,3.91],[4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53,4.53],[5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15,5.15],[5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78,5.78],[6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4,6.4],[7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02,7.02],[7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65,7.65],[8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27,8.27],[8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89,8.89],[9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51,9.51],[10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14,10.14],[10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76,10.76],[11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38,11.38],[12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0],[12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63,12.63],[13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25,13.25],[13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87,13.87],[14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5,14.5],[15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12,15.12],[15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74,15.74]],"z":[[-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63],[-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63],[-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.21,-9.37],[-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.2,-9.37,-8.53,-7.69],[-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.2,-9.36,-8.52,-7.68,-6.85,-6.01],[-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.19,-9.35,-8.51,-7.68,-6.84,-6.0,-5.16,-4.33],[-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.18,-9.34,-8.51,-7.67,-6.83,-5.99,-5.16,-4.32,-3.48,-2.64],[-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.17,-9.34,-8.5,-7.66,-6.82,-5.99,-5.15,-4.31,-3.47,-2.64,-1.8,-0.96],[-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.17,-9.33,-8.49,-7.65,-6.82,-5.98,-5.14,-4.3,-3.47,-2.63,-1.79,-0.95,-0.12,0.72],[-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.16,-9.32,-8.48,-7.65,-6.81,-5.97,-5.13,-4.3,-3.46,-2.62,-1.78,-0.95,-0.11,0.73,1.57,2.4],[-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.15,-9.31,-8.48,-7.64,-6.8,-5.96,-5.13,-4.29,-3.45,-2.61,-1.78,-0.94,-0.1,0.74,1.57,2.41,3.25,4.09],[-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.14,-9.31,-8.47,-7.63,-6.79,-5.96,-5.12,-4.28,-3.44,-2.61,-1.77,-0.93,-0.09,0.74,1.58,2.42,3.26,4.09,4.93,5.77],[-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.14,-9.3,-8.46,-7.62,-6.79,-5.95,-5.11,-4.27,-3.44,-2.6,-1.76,-0.92,-0.09,0.75,1.59,2.43,3.26,4.1,4.94,5.78,6.61,7.45],[-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.13,-9.29,-8.45,-7.62,-6.78,-5.94,-5.1,-4.27,-3.43,-2.59,-1.75,-0.92,-0.08,0.76,1.6,2.43,3.27,4.11,4.95,5.78,6.62,7.46,8.3,9.13],[-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.12,-9.28,-8.45,-7.61,-6.77,-5.93,-5.1,-4.26,-3.42,-2.58,-1.75,-0.91,-0.07,0.77,1.6,2.44,3.28,4.12,4.95,5.79,6.63,7.47,8.3,9.14,9.98,10.82],[-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.11,-9.28,-8.44,-7.6,-6.76,-5.93,-5.09,-4.25,-3.41,-2.58,-1.74,-0.9,-0.06,0.77,1.61,2.45,3.29,4.12,4.96,5.8,6.64,7.47,8.31,9.15,9.99,10.82,11.66,12.5],[-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.11,-9.27,-8.43,-7.59,-6.76,-5.92,-5.08,-4.24,-3.41,-2.57,-1.73,-0.89,-0.06,0.78,1.62,2.46,3.29,4.13,4.97,5.81,6.64,7.48,8.32,9.16,9.99,10.83,11.67,12.51,13.34,14.18],[-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.1,-9.26,-8.42,-7.58,-6.75,-5.91,-5.07,-4.23,-3.4,-2.56,-1.72,-0.89,-0.05,0.79,1.63,2.46,3.3,4.14,4.98,5.81,6.65,7.49,8.33,9.16,10.0,10.84,11.68,12.51,13.35,14.19,15.03,15.26],[-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.09,-9.25,-8.41,-7.58,-6.74,-5.9,-5.06,-4.23,-3.39,-2.55,-1.71,-0.88,-0.04,0.8,1.64,2.47,3.31,4.15,4.99,5.82,6.66,7.5,8.34,9.17,10.01,10.85,11.68,12.52,13.36,14.2,15.03,15.26,15.26,15.26],[-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.08,-9.24,-8.41,-7.57,-6.73,-5.89,-5.06,-4.22,-3.38,-2.54,-1.71,-0.87,-0.03,0.81,1.64,2.48,3.32,4.16,4.99,5.83,6.67,7.51,8.34,9.18,10.02,10.86,11.69,12.53,13.37,14.21,15.04,15.26,15.26,15.26,15.26,15.26],[-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.07,-9.24,-8.4,-7.56,-6.72,-5.89,-5.05,-4.21,-3.37,-2.54,-1.7,-0.86,-0.02,0.81,1.65,2.49,3.33,4.16,5.0,5.84,6.68,7.51,8.35,9.19,10.03,10.86,11.7,12.54,13.38,14.21,15.05,15.26,15.26,15.26,15.26,15.26,15.26,15.26],[-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.07,-9.23,-8.39,-7.55,-6.72,-5.88,-5.04,-4.2,-3.37,-2.53,-1.69,-0.85,-0.02,0.82,1.66,2.5,3.33,4.17,5.01,5.85,6.68,7.52,8.36,9.2,10.03,10.87,11.71,12.55,13.38,14.22,15.06,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26],[-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.06,-9.22,-8.38,-7.55,-6.71,-5.87,-5.03,-4.2,-3.36,-2.52,-1.68,-0.85,-0.01,0.83,1.67,2.5,3.34,4.18,5.02,5.85,6.69,7.53,8.37,9.2,10.04,10.88,11.72,12.55,13.39,14.23,15.07,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26],[-10.63,-10.63,-10.63,-10.63,-10.63,-10.63,-10.05,-9.21,-8.38,-7.54,-6.7,-5.86,-5.03,-4.19,-3.35,-2.51,-1.68,-0.84,-0.0,0.84,1.67,2.51,3.35,4.19,5.02,5.86,6.7,7.54,8.37,9.21,10.05,10.89,11.72,12.56,13.4,14.24,15.07,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26],[-10.63,-10.63,-10.63,-10.63,-10.04,-9.21,-8.37,-7.53,-6.69,-5.86,-5.02,-4.18,-3.34,-2.51,-1.67,-0.83,0.01,0.84,1.68,2.52,3.36,4.19,5.03,5.87,6.71,7.54,8.38,9.22,10.06,10.89,11.73,12.57,13.41,14.24,15.08,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26],[-10.63,-10.63,-10.04,-9.2,-8.36,-7.52,-6.69,-5.85,-5.01,-4.17,-3.34,-2.5,-1.66,-0.82,0.01,0.85,1.69,2.53,3.36,4.2,5.04,5.88,6.71,7.55,8.39,9.23,10.06,10.9,11.74,12.58,13.41,14.25,15.09,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26],[-10.03,-9.19,-8.35,-7.52,-6.68,-5.84,-5.0,-4.17,-3.33,-2.49,-1.65,-0.82,0.02,0.86,1.7,2.53,3.37,4.21,5.05,5.88,6.72,7.56,8.4,9.23,10.07,10.91,11.75,12.58,13.42,14.26,15.1,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26],[-8.35,-7.51,-6.67,-5.83,-5.0,-4.16,-3.32,-2.48,-1.65,-0.81,0.03,0.87,1.7,2.54,3.38,4.22,5.05,5.89,6.73,7.57,8.4,9.24,10.08,10.92,11.75,12.59,13.43,14.27,15.1,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26],[-6.66,-5.83,-4.99,-4.15,-3.31,-2.48,-1.64,-0.8,0.04,0.87,1.71,2.55,3.39,4.22,5.06,5.9,6.74,7.57,8.41,9.25,10.09,10.92,11.76,12.6,13.44,14.27,15.11,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26],[-4.98,-4.14,-3.31,-2.47,-1.63,-0.79,0.04,0.88,1.72,2.56,3.39,4.23,5.07,5.91,6.74,7.58,8.42,9.26,10.09,10.93,11.77,12.61,13.44,14.28,15.12,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26],[-3.3,-2.46,-1.62,-0.79,0.05,0.89,1.73,2.56,3.4,4.24,5.08,5.91,6.75,7.59,8.43,9.26,10.1,10.94,11.78,12.61,13.45,14.29,15.13,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26],[-1.62,-0.78,0.06,0.9,1.73,2.57,3.41,4.25,5.08,5.92,6.76,7.6,8.43,9.27,10.11,10.95,11.78,12.62,13.46,14.3,15.13,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26],[0.07,0.9,1.74,2.58,3.42,4.25,5.09,5.93,6.77,7.6,8.44,9.28,10.12,10.95,11.79,12.63,13.47,14.3,15.14,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26],[1.75,2.59,3.42,4.26,5.1,5.94,6.77,7.61,8.45,9.29,10.12,10.96,11.8,12.64,13.47,14.31,15.15,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26],[3.43,4.27,5.11,5.94,6.78,7.62,8.46,9.29,10.13,10.97,11.81,12.64,13.48,14.32,15.16,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26],[5.12,5.95,6.79,7.63,8.47,9.3,10.14,10.98,11.82,12.65,13.49,14.33,15.17,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26],[6.8,7.64,8.47,9.31,10.15,10.99,11.82,12.66,13.5,14.34,15.17,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26],[8.48,9.32,10.16,10.99,11.83,12.67,13.51,14.34,15.18,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26],[10.16,11.0,11.84,12.68,13.51,14.35,15.19,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26],[11.85,12.68,13.52,14.36,15.2,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26],[13.53,14.37,15.2,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26],[15.21,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26],[15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26],[15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26],[15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26],[15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26],[15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26],[15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26],[15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26],[15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26,15.26]],"type":"surface","scene":"scene2"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"scene":{"domain":{"x":[0.0,0.5],"y":[0.0,1.0]},"xaxis":{"title":{"text":""},"range":[-15.37,26.060000000000002],"showticklabels":false,"showgrid":false,"backgroundcolor":"rgba(255, 255, 255, 0)","gridcolor":"rgba(255, 255, 255, 0)","showbackground":false,"showspikes":false},"yaxis":{"title":{"text":""},"range":[-10.23,7.739999999999999],"showticklabels":false,"showgrid":false,"backgroundcolor":"rgba(255, 255, 255, 0)","gridcolor":"rgba(255, 255, 255, 0)","showbackground":false,"showspikes":false},"zaxis":{"title":{"text":""},"range":[-7.97,7.2299999999999995],"showticklabels":false,"showgrid":false,"backgroundcolor":"rgba(255, 255, 255, 0)","gridcolor":"rgba(255, 255, 255, 0)","showbackground":false,"showspikes":false},"camera":{"eye":{"x":-0.3,"y":0.8,"z":2}},"dragmode":"orbit","aspectmode":"cube"},"scene2":{"domain":{"x":[0.5,1.0],"y":[0.0,1.0]},"xaxis":{"title":{"text":""},"range":[-19.53,31.89],"showticklabels":false,"showgrid":false,"backgroundcolor":"rgba(255, 255, 255, 0)","gridcolor":"rgba(255, 255, 255, 0)","showbackground":false,"showspikes":false},"yaxis":{"title":{"text":""},"range":[-14.379999999999999,15.34],"showticklabels":false,"showgrid":false,"backgroundcolor":"rgba(255, 255, 255, 0)","gridcolor":"rgba(255, 255, 255, 0)","showbackground":false,"showspikes":false},"zaxis":{"title":{"text":""},"range":[-9.23,13.86],"showticklabels":false,"showgrid":false,"backgroundcolor":"rgba(255, 255, 255, 0)","gridcolor":"rgba(255, 255, 255, 0)","showbackground":false,"showspikes":false},"camera":{"eye":{"x":-0.2,"y":0.6,"z":1.5}},"dragmode":"orbit","aspectmode":"cube"},"annotations":[{"font":{"size":16},"showarrow":false,"text":"Next Word Length vs Characters Remaining","x":0.25,"xanchor":"center","xref":"paper","y":0.95,"yanchor":"bottom","yref":"paper"},{"font":{"size":16},"showarrow":false,"text":"The Sum Makes Linebreaking Linearly Separable","x":0.75,"xanchor":"center","xref":"paper","y":0.95,"yanchor":"bottom","yref":"paper"}],"title":{"text":"\u003cb\u003eOrthogonal Representations Create a Linear Decision Boundary for Linebreaking\u003c\u002fb\u003e","x":0.5,"xanchor":"center","y":0.98},"margin":{"l":0,"r":0,"t":30,"b":0},"legend":{"itemsizing":"constant","orientation":"h","x":0.25,"xanchor":"center","y":0.02,"yanchor":"bottom"},"legend2":{"itemsizing":"constant","orientation":"h","x":0.75,"xanchor":"center","y":0.02,"yanchor":"bottom"},"width":1200,"height":600,"showlegend":true,"font":{"family":"-apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, Oxygen, Ubuntu, Cantarell, \"Fira Sans\", \"Droid Sans\", \"Helvetica Neue\", Arial, sans-serif","color":"rgba(0, 0, 0, 0.8)"}},                        {"displayModeBar": false, "responsive": true}                    )                };                            </script>        </div>
<figcaption class='text-caption'>Low dimensional projections of next token character length and characters remaining counting manifolds for 1 (dark) to 15 (light) characters.<br>(Left) The PCA of their union. (Right) The PCA of all their pairwise combinations.<br>The orthogonal representations make the correct newline decision linearly separable.</figcaption></figure>
<p>Now consider the pairwise sum of each possible character-remaining vector <d-math>i</d-math> and next-token-length vector <d-math>j</d-math>.<d-footnote>This sum is principled because both sets of vectors are marginalized data means, so collectively have the mean of the data, which we center to be 0.</d-footnote> Since these counts are arranged orthogonally, the decision to break the line <d-math>i-j \geq 0</d-math> corresponds to a simple separating hyperplane. In other words, the prediction to break the line is made trivial by the underlying geometry!</p>
<p>When we use the separating hyperplane from the PCA of these average embeddings on real data, we achieve an AUC of 0.91 on the ground truth of whether the next token should be a newline. This reflects both the error of the three dimensional classifier <span style='font-style: italic;'>and</span> the error from Haiku’s estimates of the next token.</p>
<p>If the length of the most likely next word is linearly represented, this scheme would allow the model to predict newlines when that word is longer than the length remaining in the line. One could imagine a more general mechanism where the model comprehensively redirects the probability mass from all words that exceed the line limit to the newline. Claude 3.5 Haiku does not seem to leverage such a mechanism: when we compare the predicted distribution of tokens at the end of a line to the distribution on an identical prompt with the newlines stripped, we find them to be quite different.</p>
<h3><a id='count-algo' href='#count-algo'>A Distributed Character Counting Algorithm</a></h3>
<p>Having described how the various character counting representations are <span style='font-style: italic;'>used</span>, the last big remaining question is: how are they <span style='font-style: italic;'>computed</span>?</p>
<p>We will show how Haiku uses many attention heads across multiple layers to cooperatively compute an increasingly accurate estimate of the character count. This turned out to be the most complicated mechanism we studied, though there are many similarities with the boundary detection mechanism.</p>
<p>To get an intuitive understanding of the behavior of the heads important for counting, we project their outputs into the PCA space of the line character count probes.<d-footnote>We display the average outputs over many prompts.</d-footnote> Layer 0 heads (left) each write along what appears as a ray when visualized in the first 3 principal components—it is their sum that generates a curved manifold. Layer 1 heads (right) instead output curves which combine to produce an increasingly complex manifold. They appear responsible for sharpening the Layer 0 representation and thus the estimate of the count. We find that the <d-math>R^2</d-math> for the character count prediction <d-footnote>The prediction is the argmax of the head outputs projected on the character count probes.</d-footnote> of the 5 key Layer 0 heads is 0.93, compared to 0.97 using 11 heads in the first two layers.</p>
<figure key="head_tiling_per_layer" class="gdoc-image">
<script src="https://cdn.plot.ly/plotly-2.32.0.min.js" integrity="sha384-7TVmlZWH60iKX5Uk7lSvQhjtcgw2tkFjuwLcXoRSR4zXTyWFJRm9aPAguMh7CIra" crossorigin="anonymous"></script>
<div>                            <div id="941760f9-2dcb-43e9-9923-c907cc2bb4ed" class="plotly-graph-div" style="height:600px; width:1200px;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("941760f9-2dcb-43e9-9923-c907cc2bb4ed")) {                    Plotly.newPlot(                        "941760f9-2dcb-43e9-9923-c907cc2bb4ed",                        [{"hovertemplate":"%{text}","legend":"legend","legendgroup":"Head 0_0","marker":{"color":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140],"colorscale":[[0,"rgb(230, 230, 255)"],[0.5,"rgb(65, 105, 225)"],[1,"rgb(0, 0, 139)"]],"opacity":1.0,"showscale":false,"size":2,"symbol":"circle"},"mode":"markers","name":"Head 0","showlegend":false,"text":[1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0],"x":[-0.000404,-0.000859,-0.001072,-0.001055,-0.000541,-0.00046,-0.000675,-0.000453,-0.000615,-0.000608,0.000217,0.000131,0.000345,0.000645,0.000853,0.000947,0.001574,0.000981,0.00133,0.00153,0.001781,0.001738,0.001757,0.001798,0.00168,0.001799,0.001911,0.00188,0.002383,0.002217,0.002092,0.001965,0.002079,0.002071,0.001911,0.002156,0.002336,0.002351,0.00213,0.002009,0.002506,0.002166,0.002186,0.002353,0.002169,0.002263,0.002385,0.002179,0.002334,0.002227,0.002213,0.002332,0.002147,0.002173,0.002058,0.002013,0.002414,0.002184,0.002305,0.002232,0.002345,0.002422,0.002281,0.002291,0.002437,0.002024,0.002307,0.00226,0.002054,0.002154,0.002178,0.002245,0.00217,0.002046,0.00221,0.002302,0.002197,0.002132,0.00225,0.002141,0.002166,0.002434,0.002284,0.002203,0.002066,0.002154,0.002367,0.002211,0.002408,0.002322,0.0022,0.002261,0.001981,0.002297,0.002178,0.002275,0.002195,0.001918,0.001742,0.002348,0.002144,0.002208,0.002157,0.002179,0.002322,0.002246,0.00242,0.002003,0.002138,0.002294,0.002306,0.00231,0.002104,0.002123,0.002204,0.002225,0.002266,0.002308,0.002103,0.002076,0.002435,0.002332,0.002299,0.002468,0.002133,0.002207,0.002123,0.002214,0.002503,0.002185,0.002665,0.002379,0.002179,0.001966,0.002227,0.002125,0.002123,0.00216,0.002002,0.001914],"y":[0.006113,0.005865,0.00544,0.005353,0.005375,0.005448,0.005268,0.004854,0.004732,0.004731,0.00428,0.004104,0.003882,0.003815,0.003366,0.00294,0.002797,0.002638,0.002423,0.002243,0.002234,0.002145,0.001786,0.001619,0.001403,0.001341,0.001315,0.001154,0.001574,0.001303,0.001139,0.001036,0.00105,0.000972,0.000827,0.001063,0.001066,0.001211,0.000917,0.000789,0.001275,0.000888,0.000874,0.001101,0.000869,0.001003,0.001068,0.000802,0.000975,0.000846,0.000858,0.00091,0.000747,0.000778,0.000661,0.000606,0.000989,0.000772,0.00086,0.000812,0.000923,0.001006,0.000817,0.000841,0.000986,0.000557,0.000822,0.000818,0.000572,0.000684,0.000689,0.000786,0.000699,0.000588,0.000763,0.000846,0.000778,0.000685,0.000766,0.000688,0.00063,0.001052,0.00087,0.000727,0.000652,0.00064,0.000887,0.000766,0.000979,0.000882,0.000689,0.000809,0.000547,0.000825,0.000741,0.000794,0.00071,0.000423,0.000205,0.000907,0.000631,0.000783,0.000695,0.000648,0.000879,0.00082,0.000938,0.000514,0.000658,0.000834,0.000849,0.000806,0.00059,0.000659,0.000739,0.000732,0.000801,0.000863,0.00061,0.000637,0.000967,0.000847,0.000831,0.000924,0.000681,0.000756,0.000688,0.000716,0.001057,0.000685,0.001261,0.000885,0.000729,0.000523,0.000736,0.000661,0.000665,0.000706,0.000552,0.00044],"z":[0.00858,0.00867,0.008277,0.008137,0.007674,0.007699,0.007659,0.006867,0.006842,0.006804,0.005421,0.005231,0.004726,0.004321,0.003506,0.002801,0.002026,0.00234,0.001724,0.001263,0.001004,0.000926,0.000385,0.000134,-0.000071,-0.000259,-0.00041,-0.000632,-0.000482,-0.000715,-0.000849,-0.000858,-0.000942,-0.001045,-0.001112,-0.000998,-0.001154,-0.000976,-0.001205,-0.001296,-0.001045,-0.001283,-0.0013,-0.001152,-0.001298,-0.001213,-0.001217,-0.001422,-0.001309,-0.001404,-0.001386,-0.001398,-0.001449,-0.00145,-0.001508,-0.001524,-0.001359,-0.001485,-0.001443,-0.001452,-0.001397,-0.00133,-0.001476,-0.001471,-0.001383,-0.001596,-0.001501,-0.001462,-0.001594,-0.001548,-0.001545,-0.001491,-0.001539,-0.001567,-0.001485,-0.00144,-0.001451,-0.001519,-0.001511,-0.001528,-0.001611,-0.001312,-0.00139,-0.001519,-0.00153,-0.001596,-0.00143,-0.001475,-0.001368,-0.001421,-0.001571,-0.001469,-0.001573,-0.001477,-0.001481,-0.001503,-0.001538,-0.001701,-0.001843,-0.001411,-0.001615,-0.001461,-0.001531,-0.001611,-0.001446,-0.001463,-0.001429,-0.001657,-0.001589,-0.001465,-0.00145,-0.001499,-0.001626,-0.001562,-0.001517,-0.001532,-0.001472,-0.001465,-0.001604,-0.001552,-0.001413,-0.001476,-0.001461,-0.001455,-0.001537,-0.001505,-0.001494,-0.001537,-0.001364,-0.001542,-0.001208,-0.001444,-0.001525,-0.00158,-0.001527,-0.001559,-0.001557,-0.001475,-0.001601,-0.001663],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","legend":"legend","legendgroup":"Head 0_0","marker":{"color":"rgb(65, 105, 225)","line":{"color":"rgb(65, 105, 225)","width":0},"showscale":false,"size":12,"symbol":"circle"},"mode":"markers","name":"Head 0","showlegend":true,"x":[null],"y":[null],"z":[null],"type":"scatter3d","scene":"scene"},{"hovertemplate":"%{text}","legend":"legend","legendgroup":"Head 1_0","marker":{"color":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140],"colorscale":[[0,"rgb(230, 255, 230)"],[0.5,"rgb(50, 205, 50)"],[1,"rgb(0, 100, 0)"]],"opacity":1.0,"showscale":false,"size":2,"symbol":"circle"},"mode":"markers","name":"Head 1","showlegend":false,"text":[1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0],"x":[-0.000455,-0.000447,-0.000425,-0.000425,-0.000384,-0.000359,-0.000342,-0.00024,-0.000238,-0.000206,-0.000016,0.000039,0.00011,0.000321,0.000415,0.000444,0.000867,0.000765,0.000999,0.001196,0.0015,0.001591,0.001623,0.001832,0.001886,0.002047,0.002276,0.002373,0.002739,0.002788,0.002804,0.002892,0.002977,0.003016,0.003078,0.003194,0.003351,0.003352,0.003336,0.0033,0.003514,0.003459,0.003485,0.003528,0.003559,0.003546,0.003615,0.003567,0.003596,0.003617,0.003618,0.003639,0.00363,0.003602,0.003594,0.003564,0.003653,0.003618,0.003661,0.003612,0.003646,0.003684,0.003656,0.003652,0.003672,0.003651,0.00366,0.003656,0.00361,0.003649,0.003681,0.003688,0.003676,0.003662,0.003707,0.003695,0.003673,0.003666,0.003691,0.003667,0.003683,0.00372,0.003698,0.003664,0.003636,0.003676,0.00374,0.003649,0.003707,0.003706,0.003657,0.003686,0.003616,0.0037,0.003656,0.0037,0.003697,0.003602,0.003588,0.003676,0.003651,0.003687,0.003668,0.003659,0.003633,0.003654,0.00368,0.003685,0.003626,0.003706,0.003684,0.003668,0.003647,0.003637,0.003687,0.003688,0.003667,0.003689,0.003654,0.003606,0.003702,0.003655,0.003717,0.003718,0.003617,0.00366,0.003663,0.0037,0.003722,0.003669,0.003749,0.003708,0.003633,0.003614,0.003687,0.003646,0.003654,0.003706,0.003627,0.003567],"y":[0.012269,0.012611,0.01265,0.012767,0.01252,0.012474,0.012519,0.012388,0.012296,0.01226,0.011841,0.011704,0.011459,0.011073,0.010742,0.010462,0.009595,0.009878,0.009184,0.008622,0.007903,0.007668,0.007329,0.00686,0.006428,0.006028,0.005394,0.005003,0.004189,0.003969,0.003747,0.003393,0.003256,0.002818,0.002581,0.002364,0.001945,0.001956,0.001847,0.001561,0.001544,0.001419,0.001277,0.00135,0.00113,0.00113,0.001081,0.000976,0.000925,0.000914,0.00084,0.000869,0.000846,0.00081,0.000785,0.000641,0.000751,0.000694,0.000795,0.000705,0.000717,0.000768,0.000778,0.000756,0.000784,0.00064,0.000737,0.000745,0.000601,0.000656,0.000668,0.000696,0.000712,0.000657,0.000758,0.000711,0.000752,0.000726,0.000722,0.000741,0.000645,0.0009,0.000808,0.000687,0.000635,0.000651,0.000769,0.000659,0.000786,0.000701,0.000644,0.000713,0.000642,0.000672,0.0007,0.000657,0.000738,0.000562,0.000454,0.000692,0.000611,0.000702,0.000658,0.000556,0.000694,0.000666,0.000697,0.000663,0.000548,0.000722,0.000759,0.000673,0.000577,0.000618,0.000723,0.000682,0.000668,0.000733,0.000595,0.000612,0.000718,0.000655,0.000744,0.000651,0.00065,0.000657,0.000664,0.000712,0.000758,0.000743,0.000836,0.000737,0.000611,0.000694,0.000805,0.000675,0.000717,0.000788,0.000775,0.000586],"z":[0.009667,0.00991,0.00993,0.010014,0.009815,0.00977,0.009797,0.009652,0.009587,0.00955,0.009155,0.009036,0.008835,0.008435,0.008187,0.007995,0.007155,0.007422,0.006822,0.006353,0.005666,0.005457,0.005264,0.004825,0.004539,0.004183,0.003634,0.003333,0.002579,0.002423,0.002274,0.002002,0.001866,0.001585,0.001429,0.00117,0.000839,0.000794,0.000755,0.000638,0.000459,0.00044,0.000312,0.000324,0.000168,0.000178,0.000114,0.000057,-3e-6,0.000017,-0.000033,-0.000035,-0.000029,-0.00005,-0.000076,-0.00009,-0.00012,-0.000138,-0.000091,-0.000096,-0.000145,-0.00012,-0.000087,-0.000125,-0.000103,-0.000163,-0.000138,-0.000098,-0.000147,-0.000145,-0.000134,-0.000134,-0.000165,-0.000158,-0.000132,-0.000157,-0.000121,-0.000122,-0.00016,-0.000136,-0.00016,-0.000075,-0.0001,-0.000148,-0.000147,-0.000178,-0.000149,-0.000138,-0.000139,-0.000165,-0.000177,-0.000155,-0.000139,-0.000212,-0.000106,-0.000204,-0.000157,-0.000184,-0.000214,-0.000156,-0.000205,-0.000151,-0.00016,-0.000198,-0.000148,-0.000154,-0.000168,-0.000194,-0.000204,-0.000184,-0.00014,-0.000151,-0.00024,-0.000197,-0.000133,-0.000173,-0.000168,-0.000168,-0.000196,-0.000156,-0.000144,-0.000174,-0.000174,-0.000199,-0.000159,-0.000175,-0.000168,-0.000165,-0.000184,-0.000115,-0.000153,-0.000132,-0.000185,-0.000095,-0.000106,-0.000153,-0.000122,-0.000068,-0.000095,-0.000119],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","legend":"legend","legendgroup":"Head 1_0","marker":{"color":"rgb(50, 205, 50)","line":{"color":"rgb(50, 205, 50)","width":0},"showscale":false,"size":12,"symbol":"circle"},"mode":"markers","name":"Head 1","showlegend":true,"x":[null],"y":[null],"z":[null],"type":"scatter3d","scene":"scene"},{"hovertemplate":"%{text}","legend":"legend","legendgroup":"Head 2_0","marker":{"color":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140],"colorscale":[[0,"rgb(255, 230, 230)"],[0.5,"rgb(220, 20, 60)"],[1,"rgb(139, 0, 0)"]],"opacity":1.0,"showscale":false,"size":2,"symbol":"circle"},"mode":"markers","name":"Head 2","showlegend":false,"text":[1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0],"x":[0.010902,0.011102,0.011445,0.011512,0.011674,0.011787,0.011925,0.012248,0.012169,0.012252,0.012294,0.012304,0.012208,0.012256,0.01226,0.012197,0.012071,0.012231,0.011953,0.011968,0.01164,0.011677,0.011654,0.01164,0.011361,0.011318,0.010971,0.010683,0.010248,0.010299,0.010075,0.009934,0.009434,0.009426,0.00923,0.008606,0.008107,0.007758,0.007798,0.007817,0.006377,0.006581,0.006283,0.00572,0.005492,0.005137,0.004673,0.004367,0.003878,0.003754,0.003283,0.002761,0.00299,0.002342,0.002038,0.001879,0.000918,0.001224,0.000701,0.000102,-0.000149,-0.000695,-0.000424,-0.000727,-0.001212,-0.001305,-0.001603,-0.001779,-0.002124,-0.002088,-0.002366,-0.003006,-0.002789,-0.003111,-0.003165,-0.003483,-0.003437,-0.00372,-0.003818,-0.003961,-0.004205,-0.004376,-0.004428,-0.00459,-0.004788,-0.004807,-0.004805,-0.004846,-0.005067,-0.00527,-0.005319,-0.005281,-0.005363,-0.005509,-0.005571,-0.005487,-0.005562,-0.005806,-0.005873,-0.005807,-0.005741,-0.005877,-0.006128,-0.006137,-0.006207,-0.005882,-0.005924,-0.006249,-0.006405,-0.006097,-0.006038,-0.006166,-0.006176,-0.006441,-0.006252,-0.006203,-0.00623,-0.006308,-0.006578,-0.006537,-0.006244,-0.006299,-0.006537,-0.006217,-0.006427,-0.006517,-0.006501,-0.006448,-0.006301,-0.006432,-0.006428,-0.006483,-0.006582,-0.006715,-0.006311,-0.006618,-0.006575,-0.006357,-0.006462,-0.006657],"y":[0.010739,0.01078,0.011039,0.011082,0.011208,0.011264,0.011368,0.011553,0.011531,0.01157,0.011573,0.011591,0.011521,0.01153,0.01154,0.011486,0.011384,0.011493,0.011289,0.011319,0.010988,0.011031,0.011036,0.010943,0.010851,0.010777,0.010493,0.010255,0.00989,0.009944,0.009761,0.009703,0.009261,0.00935,0.009193,0.00865,0.008335,0.008083,0.008058,0.008232,0.007182,0.007283,0.006977,0.006658,0.006587,0.006172,0.005996,0.005796,0.005505,0.005286,0.00508,0.004693,0.004825,0.004533,0.004349,0.004287,0.00353,0.003737,0.003362,0.003146,0.002731,0.002471,0.002684,0.002392,0.002092,0.002248,0.001799,0.001708,0.001839,0.001625,0.001593,0.001242,0.001019,0.001124,0.000991,0.000666,0.00072,0.00083,0.00044,0.000495,0.000322,0.000231,0.000109,0.000083,0.000226,-0.000029,-0.000287,-0.00002,-0.000303,-0.000227,-0.000334,-0.00034,-0.000253,-0.00046,-0.000378,-0.000618,-0.000617,-0.000414,-0.000479,-0.000627,-0.000666,-0.000768,-0.000687,-0.000715,-0.000742,-0.000769,-0.000807,-0.000821,-0.000821,-0.000884,-0.000838,-0.000885,-0.000895,-0.000823,-0.000868,-0.000909,-0.000937,-0.000911,-0.000847,-0.000829,-0.000978,-0.001004,-0.000958,-0.001088,-0.000814,-0.000956,-0.000907,-0.000969,-0.001021,-0.000931,-0.001057,-0.000996,-0.000961,-0.000832,-0.00092,-0.000937,-0.00091,-0.000908,-0.000833,-0.000799],"z":[-0.00204,-0.001988,-0.002114,-0.002099,-0.002241,-0.002293,-0.002325,-0.002441,-0.002438,-0.002466,-0.002478,-0.002476,-0.002442,-0.002463,-0.002468,-0.002435,-0.002379,-0.002425,-0.002337,-0.002327,-0.002149,-0.002171,-0.002186,-0.002108,-0.002057,-0.00195,-0.001862,-0.001743,-0.001463,-0.00151,-0.001361,-0.001263,-0.001056,-0.001082,-0.000948,-0.000636,-0.000478,-0.000242,-0.000231,-0.000251,0.000411,0.000339,0.000534,0.000746,0.000904,0.001061,0.001306,0.001443,0.0017,0.001766,0.001966,0.002272,0.002153,0.002423,0.002597,0.002665,0.003152,0.00302,0.003253,0.003542,0.003678,0.003946,0.003837,0.003953,0.004204,0.004255,0.004401,0.004477,0.004654,0.004638,0.004786,0.005091,0.004978,0.005135,0.005162,0.005346,0.005307,0.005439,0.005483,0.005556,0.005688,0.00576,0.005776,0.005879,0.005963,0.005974,0.005971,0.006001,0.006092,0.006205,0.00623,0.006202,0.006238,0.006322,0.006357,0.006302,0.006349,0.006466,0.006494,0.006464,0.006437,0.006504,0.006621,0.006638,0.00667,0.006496,0.006537,0.006687,0.006769,0.006615,0.006572,0.006652,0.006654,0.006772,0.006685,0.006674,0.00668,0.006708,0.006845,0.006836,0.006696,0.0067,0.006827,0.006681,0.006764,0.006809,0.006808,0.006787,0.00671,0.006782,0.00678,0.006807,0.006851,0.006902,0.006703,0.006867,0.006834,0.006727,0.00677,0.006882],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","legend":"legend","legendgroup":"Head 2_0","marker":{"color":"rgb(220, 20, 60)","line":{"color":"rgb(220, 20, 60)","width":0},"showscale":false,"size":12,"symbol":"circle"},"mode":"markers","name":"Head 2","showlegend":true,"x":[null],"y":[null],"z":[null],"type":"scatter3d","scene":"scene"},{"hovertemplate":"%{text}","legend":"legend","legendgroup":"Head 3_0","marker":{"color":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140],"colorscale":[[0,"rgb(255, 245, 220)"],[0.5,"rgb(255, 165, 0)"],[1,"rgb(184, 134, 11)"]],"opacity":1.0,"showscale":false,"size":2,"symbol":"circle"},"mode":"markers","name":"Head 3","showlegend":false,"text":[1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0],"x":[0.002839,0.00193,0.002572,0.002059,0.002598,0.002477,0.002335,0.0025,0.002914,0.002457,0.002816,0.003078,0.002932,0.003256,0.003413,0.003387,0.003564,0.00355,0.00357,0.003708,0.004031,0.003966,0.00404,0.004,0.004375,0.004172,0.0043,0.004421,0.004596,0.004806,0.004862,0.004929,0.005002,0.004691,0.005091,0.004948,0.00521,0.00541,0.005285,0.005218,0.005441,0.005403,0.0056,0.005355,0.005796,0.005607,0.005613,0.005604,0.005821,0.005759,0.005873,0.005863,0.005576,0.005833,0.005955,0.005929,0.005996,0.005821,0.005609,0.005963,0.005457,0.005745,0.005751,0.005476,0.00559,0.005659,0.005444,0.005426,0.00561,0.005326,0.0056,0.005489,0.005215,0.005322,0.005162,0.004908,0.005013,0.005216,0.004853,0.004991,0.004771,0.004427,0.004788,0.00463,0.004546,0.004362,0.004277,0.004322,0.004026,0.004084,0.00406,0.003992,0.00392,0.003736,0.003841,0.003457,0.003582,0.003539,0.00367,0.00312,0.003086,0.003263,0.003141,0.00296,0.002918,0.002657,0.002652,0.00282,0.002438,0.002083,0.002558,0.002228,0.002167,0.002121,0.002181,0.002218,0.001753,0.002069,0.002057,0.001938,0.001666,0.001319,0.001589,0.001238,0.001691,0.001306,0.001665,0.001283,0.001,0.001021,0.000935,0.000819,0.000908,0.001035,0.000771,0.000743,0.00072,0.000784,0.00072,0.000759],"y":[0.004543,0.005574,0.005106,0.005705,0.005129,0.005212,0.005456,0.005275,0.004923,0.005405,0.004915,0.004731,0.004854,0.00452,0.004384,0.004348,0.004177,0.004249,0.004157,0.004054,0.003682,0.0038,0.003689,0.00373,0.003404,0.003588,0.003385,0.003255,0.003059,0.002841,0.002825,0.002757,0.002651,0.00297,0.002609,0.002708,0.002405,0.002229,0.002357,0.002442,0.002158,0.002224,0.002011,0.002253,0.001823,0.001959,0.002003,0.00205,0.001776,0.001813,0.001715,0.001705,0.002,0.001736,0.001617,0.001653,0.001547,0.001759,0.001964,0.001622,0.002082,0.001832,0.00182,0.002057,0.001964,0.001938,0.002081,0.002092,0.001992,0.002236,0.001986,0.0021,0.002286,0.002231,0.002385,0.002633,0.002543,0.002378,0.002668,0.002602,0.002806,0.003149,0.002767,0.00301,0.003074,0.003206,0.003258,0.003308,0.003573,0.003508,0.003584,0.003648,0.003728,0.003892,0.003778,0.004116,0.004038,0.004181,0.003991,0.004532,0.004541,0.004364,0.004506,0.00473,0.004768,0.005,0.005001,0.004885,0.00531,0.005556,0.00513,0.005574,0.005535,0.005637,0.005578,0.005558,0.005913,0.005692,0.005738,0.005915,0.006101,0.006408,0.006217,0.006551,0.006161,0.006473,0.006216,0.006546,0.006771,0.006797,0.006885,0.007033,0.006962,0.006894,0.007052,0.007207,0.007223,0.007125,0.007237,0.007297],"z":[0.001408,0.000929,0.001255,0.000952,0.001228,0.001178,0.001091,0.001198,0.001383,0.001142,0.00137,0.001491,0.00142,0.001595,0.001669,0.001675,0.001758,0.001749,0.001757,0.001835,0.002024,0.001963,0.002015,0.002002,0.002174,0.002094,0.002157,0.002221,0.002341,0.002446,0.002477,0.002519,0.002553,0.002392,0.002591,0.002527,0.002654,0.002773,0.002714,0.002662,0.002795,0.002776,0.002892,0.002739,0.002994,0.002908,0.002895,0.00287,0.00302,0.00298,0.003033,0.003052,0.002895,0.003014,0.003097,0.003067,0.003119,0.003037,0.002894,0.003091,0.002834,0.002986,0.002991,0.002829,0.002893,0.00293,0.002836,0.002818,0.002892,0.002768,0.002904,0.002834,0.002727,0.002766,0.002684,0.002557,0.002617,0.002689,0.002525,0.002585,0.002467,0.002275,0.002487,0.002386,0.002334,0.002248,0.002212,0.002223,0.002074,0.002096,0.002073,0.002041,0.002,0.001922,0.001973,0.001779,0.001837,0.001756,0.001878,0.001585,0.001561,0.001664,0.001577,0.001486,0.001449,0.001337,0.001322,0.001418,0.001183,0.001038,0.001272,0.001064,0.001063,0.001012,0.00105,0.001072,0.000842,0.000995,0.000973,0.000908,0.000782,0.0006,0.000726,0.000546,0.000765,0.000586,0.000748,0.000555,0.000411,0.000428,0.00037,0.000301,0.000362,0.000404,0.000293,0.000252,0.000236,0.000288,0.000222,0.000217],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","legend":"legend","legendgroup":"Head 3_0","marker":{"color":"rgb(255, 165, 0)","line":{"color":"rgb(255, 165, 0)","width":0},"showscale":false,"size":12,"symbol":"circle"},"mode":"markers","name":"Head 3","showlegend":true,"x":[null],"y":[null],"z":[null],"type":"scatter3d","scene":"scene"},{"hovertemplate":"%{text}","legend":"legend","legendgroup":"Sum of Key Heads_0","line":{"color":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139],"colorscale":[[0.0,"#440154"],[0.1111111111111111,"#482878"],[0.2222222222222222,"#3e4989"],[0.3333333333333333,"#31688e"],[0.4444444444444444,"#26828e"],[0.5555555555555556,"#1f9e89"],[0.6666666666666666,"#35b779"],[0.7777777777777778,"#6ece58"],[0.8888888888888888,"#b5de2b"],[1.0,"#fde725"]],"width":2},"marker":{"color":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140],"colorscale":[[0.0,"#440154"],[0.1111111111111111,"#482878"],[0.2222222222222222,"#3e4989"],[0.3333333333333333,"#31688e"],[0.4444444444444444,"#26828e"],[0.5555555555555556,"#1f9e89"],[0.6666666666666666,"#35b779"],[0.7777777777777778,"#6ece58"],[0.8888888888888888,"#b5de2b"],[1.0,"#fde725"]],"opacity":1.0,"showscale":false,"size":0.1,"symbol":"circle"},"mode":"lines+markers","name":"Sum of Key Heads","showlegend":false,"text":[1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0],"x":[0.001555,0.000415,0.001145,0.000791,0.002485,0.002889,0.002773,0.003852,0.004079,0.003832,0.005638,0.006038,0.006124,0.007031,0.007626,0.007647,0.008776,0.008192,0.008566,0.00914,0.009658,0.009686,0.009807,0.009957,0.010006,0.010054,0.010137,0.010065,0.010692,0.010807,0.010509,0.010425,0.010177,0.0099,0.010016,0.009594,0.009714,0.009585,0.009248,0.009076,0.00851,0.008292,0.008218,0.007638,0.007739,0.007237,0.006989,0.006459,0.00633,0.006048,0.005702,0.005291,0.00506,0.004667,0.004348,0.004108,0.003672,0.003565,0.002992,0.002623,0.002014,0.001878,0.001962,0.001422,0.001212,0.00072,0.00052,0.000274,-0.000175,-0.000268,-0.000194,-0.000874,-0.001,-0.001358,-0.001386,-0.001863,-0.001856,-0.001987,-0.002357,-0.00248,-0.002862,-0.003077,-0.002944,-0.003373,-0.003806,-0.003886,-0.003727,-0.003949,-0.004205,-0.004455,-0.004698,-0.004613,-0.005135,-0.005049,-0.005199,-0.005341,-0.005393,-0.006018,-0.006173,-0.005943,-0.006164,-0.006025,-0.006455,-0.006644,-0.006626,-0.006589,-0.006453,-0.007054,-0.007476,-0.007276,-0.006763,-0.007241,-0.007543,-0.007861,-0.007473,-0.007375,-0.007801,-0.007532,-0.008061,-0.008156,-0.007733,-0.008264,-0.008219,-0.008068,-0.008281,-0.008641,-0.008331,-0.008544,-0.008355,-0.008812,-0.008386,-0.008878,-0.009148,-0.009411,-0.008956,-0.009377,-0.009359,-0.009004,-0.009458,-0.009758],"y":[0.019079,0.020373,0.019684,0.020531,0.02055,0.021042,0.021392,0.021287,0.020717,0.021325,0.020558,0.020218,0.019836,0.019098,0.018384,0.017621,0.016362,0.016598,0.015408,0.014647,0.013134,0.013027,0.012232,0.011534,0.01054,0.010085,0.008991,0.007997,0.007108,0.006438,0.005832,0.005301,0.004635,0.004461,0.00355,0.003179,0.002085,0.001934,0.001575,0.001471,0.000557,0.000217,-0.00046,-0.000218,-0.001233,-0.001363,-0.001501,-0.002003,-0.002466,-0.002743,-0.003101,-0.003494,-0.003183,-0.003775,-0.004206,-0.004406,-0.00477,-0.004628,-0.004659,-0.005279,-0.005194,-0.00548,-0.00553,-0.005499,-0.005751,-0.006266,-0.006179,-0.006255,-0.006598,-0.006401,-0.006658,-0.006794,-0.00689,-0.007034,-0.006688,-0.006705,-0.006761,-0.007001,-0.007019,-0.007079,-0.007181,-0.006227,-0.007046,-0.007128,-0.007038,-0.007122,-0.007003,-0.00694,-0.00658,-0.006742,-0.007091,-0.006801,-0.006984,-0.006678,-0.006761,-0.006644,-0.006747,-0.006924,-0.007462,-0.006079,-0.006505,-0.006532,-0.006402,-0.006363,-0.006028,-0.005881,-0.00578,-0.006411,-0.005893,-0.005322,-0.005768,-0.005427,-0.005816,-0.005558,-0.005435,-0.005592,-0.005098,-0.005224,-0.005503,-0.00526,-0.004763,-0.004665,-0.004766,-0.00453,-0.004948,-0.004649,-0.004974,-0.004588,-0.004013,-0.004317,-0.003643,-0.00393,-0.004242,-0.004333,-0.003979,-0.004038,-0.003975,-0.003902,-0.003933,-0.004161],"z":[0.015719,0.015715,0.015615,0.015157,0.013817,0.012937,0.01242,0.011095,0.010843,0.010217,0.007823,0.007211,0.006212,0.005608,0.004302,0.00346,0.002001,0.002416,0.001391,0.000553,-0.000123,-0.000404,-0.00105,-0.001817,-0.001939,-0.002613,-0.003095,-0.003539,-0.003579,-0.004012,-0.004086,-0.004225,-0.004204,-0.004787,-0.004763,-0.004572,-0.004815,-0.004133,-0.004569,-0.004742,-0.003982,-0.004332,-0.004188,-0.003948,-0.003903,-0.003662,-0.003558,-0.003683,-0.003239,-0.003219,-0.002963,-0.002878,-0.003054,-0.002706,-0.002546,-0.002439,-0.001751,-0.002128,-0.002087,-0.001412,-0.001709,-0.001009,-0.001291,-0.00136,-0.000924,-0.001316,-0.000983,-0.000919,-0.000861,-0.000828,-0.000577,-0.000305,-0.000661,-0.000502,-0.000352,-0.000268,-0.000183,-0.000105,-0.000277,-0.000168,-0.000234,0.000154,0.000176,-0.000036,-0.000028,-0.000088,-0.000085,-0.000087,0.000035,0.000112,-0.000185,0.000027,-0.000185,-0.000079,0.000143,-0.000246,-0.0002,-0.000416,-0.000341,-0.000051,-0.000468,-0.000093,-0.000061,-0.000312,-0.000015,-0.000375,-0.000367,-0.000457,-0.000425,-0.000486,-0.000405,-0.000512,-0.000798,-0.00071,-0.000538,-0.000676,-0.000636,-0.000552,-0.000598,-0.000519,-0.000618,-0.000901,-0.000679,-0.000976,-0.000785,-0.000884,-0.000718,-0.00093,-0.000978,-0.001049,-0.000789,-0.001027,-0.001058,-0.001033,-0.0014,-0.001258,-0.001298,-0.001161,-0.001461,-0.001548],"type":"scatter3d","scene":"scene"},{"hoverinfo":"skip","legend":"legend","legendgroup":"Sum of Key Heads_0","marker":{"color":"rgb(34, 144, 140)","line":{"color":"rgb(34, 144, 140)","width":0},"showscale":false,"size":12,"symbol":"circle"},"mode":"markers","name":"Sum of Key Heads","showlegend":true,"x":[null],"y":[null],"z":[null],"type":"scatter3d","scene":"scene"},{"hovertemplate":"%{text}","legend":"legend2","legendgroup":"Head 0_1","marker":{"color":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140],"colorscale":[[0,"rgb(230, 230, 255)"],[0.5,"rgb(65, 105, 225)"],[1,"rgb(0, 0, 139)"]],"opacity":1.0,"showscale":false,"size":2,"symbol":"circle"},"mode":"markers","name":"Head 0","showlegend":false,"text":[1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0],"x":[0.011074,0.011215,0.011106,0.011197,0.011064,0.010892,0.01087,0.010779,0.010465,0.01043,0.009678,0.008897,0.008828,0.007205,0.006164,0.005868,0.004489,0.004179,0.003145,0.002367,0.001285,0.000696,-0.000138,-0.000335,-0.000031,-0.000433,-0.000356,0.000139,0.000667,0.000565,0.00127,0.00264,0.003171,0.003838,0.004993,0.006527,0.007003,0.008214,0.008928,0.009937,0.011036,0.01139,0.012571,0.013243,0.014349,0.014765,0.015209,0.015624,0.016527,0.017021,0.017589,0.017646,0.018118,0.018226,0.018804,0.018868,0.018569,0.018673,0.018671,0.018708,0.018367,0.018161,0.019011,0.018306,0.0182,0.018716,0.0181,0.017984,0.017793,0.017585,0.01804,0.017211,0.016983,0.016931,0.016957,0.016422,0.016329,0.016042,0.015858,0.015854,0.015834,0.015142,0.01509,0.01528,0.014868,0.014769,0.01489,0.01449,0.014325,0.014474,0.014304,0.014276,0.014134,0.014162,0.014064,0.013891,0.01375,0.013657,0.013725,0.013507,0.013449,0.013421,0.013381,0.013346,0.013123,0.013188,0.013157,0.013233,0.0129,0.013014,0.013033,0.012985,0.012975,0.012931,0.012913,0.012782,0.012751,0.012865,0.012838,0.012736,0.012637,0.012551,0.012558,0.012661,0.012679,0.012574,0.012552,0.012547,0.012493,0.012506,0.012429,0.012357,0.012385,0.012367,0.012417,0.012397,0.012352,0.01247,0.012332,0.012293],"y":[-0.017583,-0.015992,-0.014751,-0.013296,-0.010728,-0.008888,-0.006461,-0.003686,-0.002479,-0.001784,-0.00053,-0.000079,-0.00047,-0.001614,-0.002629,-0.00472,-0.00673,-0.005997,-0.010245,-0.011682,-0.014868,-0.015915,-0.020001,-0.02281,-0.024723,-0.026876,-0.029429,-0.031824,-0.036098,-0.03655,-0.039151,-0.040588,-0.041559,-0.04206,-0.043911,-0.043363,-0.043942,-0.044417,-0.045482,-0.04535,-0.043453,-0.044418,-0.044527,-0.042482,-0.043685,-0.041511,-0.041498,-0.040299,-0.039082,-0.038136,-0.037207,-0.036369,-0.036546,-0.034102,-0.033827,-0.032489,-0.030376,-0.030442,-0.029031,-0.028208,-0.026997,-0.026144,-0.026344,-0.025419,-0.024344,-0.024444,-0.023188,-0.022622,-0.022317,-0.02185,-0.021774,-0.020506,-0.020425,-0.020093,-0.019831,-0.019517,-0.0195,-0.019007,-0.018824,-0.01854,-0.018379,-0.018289,-0.018177,-0.018027,-0.018119,-0.017958,-0.017647,-0.01778,-0.017572,-0.017716,-0.017545,-0.017498,-0.017489,-0.017343,-0.01741,-0.017525,-0.017441,-0.017569,-0.017602,-0.017515,-0.017528,-0.017502,-0.01748,-0.017544,-0.017741,-0.017619,-0.017545,-0.017552,-0.017852,-0.017684,-0.017581,-0.01763,-0.017759,-0.017738,-0.017637,-0.017764,-0.018064,-0.01763,-0.017682,-0.017905,-0.017842,-0.018057,-0.018044,-0.017809,-0.017971,-0.018033,-0.017838,-0.018005,-0.018022,-0.018076,-0.018036,-0.018075,-0.018223,-0.018275,-0.018124,-0.018191,-0.018123,-0.018212,-0.018241,-0.018254],"z":[0.015409,0.015344,0.015662,0.01572,0.016415,0.016852,0.017184,0.017923,0.018662,0.018775,0.020503,0.021997,0.022063,0.024818,0.026837,0.02724,0.029597,0.030299,0.031551,0.033143,0.035004,0.036035,0.037,0.037408,0.03665,0.037471,0.036891,0.035941,0.034848,0.034929,0.033656,0.031089,0.03023,0.028921,0.026837,0.023972,0.022853,0.020844,0.019609,0.017725,0.01563,0.014858,0.01289,0.011564,0.009592,0.00878,0.008118,0.007234,0.00563,0.004425,0.003739,0.003491,0.002678,0.002443,0.001387,0.001128,0.001581,0.001348,0.001417,0.001246,0.001783,0.002005,0.000694,0.001821,0.00199,0.000955,0.002038,0.002044,0.002629,0.002897,0.002045,0.003411,0.003763,0.003787,0.003838,0.004756,0.004834,0.00537,0.005641,0.005587,0.005712,0.007001,0.007029,0.006615,0.007505,0.007598,0.00733,0.008099,0.008285,0.008148,0.008418,0.008405,0.008672,0.008633,0.008886,0.009137,0.009403,0.009582,0.009467,0.009866,0.009962,0.009972,0.010055,0.010185,0.01061,0.01041,0.010486,0.010338,0.011043,0.010766,0.010721,0.010795,0.010853,0.010991,0.010932,0.011242,0.011292,0.011045,0.011117,0.011357,0.011504,0.011705,0.011722,0.01144,0.011404,0.011667,0.01166,0.011674,0.011766,0.011772,0.011895,0.012078,0.012013,0.012108,0.011927,0.011991,0.012088,0.011856,0.012123,0.012177],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","legend":"legend2","legendgroup":"Head 0_1","marker":{"color":"rgb(65, 105, 225)","line":{"color":"rgb(65, 105, 225)","width":0},"showscale":false,"size":12,"symbol":"circle"},"mode":"markers","name":"Head 0","showlegend":true,"x":[null],"y":[null],"z":[null],"type":"scatter3d","scene":"scene2"},{"hovertemplate":"%{text}","legend":"legend2","legendgroup":"Head 1_1","marker":{"color":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140],"colorscale":[[0,"rgb(230, 255, 230)"],[0.5,"rgb(50, 205, 50)"],[1,"rgb(0, 100, 0)"]],"opacity":1.0,"showscale":false,"size":2,"symbol":"circle"},"mode":"markers","name":"Head 1","showlegend":false,"text":[1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0],"x":[0.006392,0.004355,0.00285,0.002276,0.00328,0.003492,0.002039,0.001345,0.000814,0.001191,0.00174,0.001571,0.00193,0.002677,0.002839,0.002484,0.00357,0.002221,0.003973,0.003834,0.00444,0.004433,0.005362,0.005302,0.005156,0.005164,0.006225,0.007048,0.008138,0.008014,0.007893,0.007263,0.007757,0.00806,0.007573,0.008516,0.010083,0.008912,0.009024,0.008028,0.009716,0.009821,0.009635,0.009491,0.009054,0.009368,0.0093,0.008742,0.008808,0.009149,0.008055,0.008901,0.007577,0.007453,0.007126,0.007111,0.007795,0.007007,0.006347,0.006405,0.006458,0.006459,0.005611,0.005637,0.005604,0.004546,0.00504,0.005274,0.003416,0.003961,0.003029,0.002903,0.003786,0.002442,0.00215,0.002607,0.002728,0.001291,0.001803,0.001638,0.001092,0.001007,0.001066,0.000681,-0.000482,-0.000166,0.000447,-0.000047,0.000036,-0.001376,-0.001199,-0.001251,-0.001707,-0.002073,-0.002509,-0.001433,-0.002072,-0.003286,-0.003536,-0.002479,-0.00274,-0.00264,-0.003499,-0.003566,-0.003458,-0.002905,-0.00273,-0.004575,-0.004333,-0.004034,-0.003692,-0.00356,-0.004548,-0.005274,-0.00473,-0.004261,-0.004306,-0.004482,-0.006044,-0.005721,-0.004402,-0.004739,-0.005703,-0.004372,-0.005755,-0.005859,-0.005948,-0.005533,-0.00497,-0.005534,-0.004996,-0.006093,-0.006303,-0.006951,-0.005748,-0.006324,-0.006616,-0.006069,-0.00642,-0.00661],"y":[-0.029597,-0.029761,-0.029075,-0.028656,-0.029253,-0.029765,-0.029784,-0.030422,-0.030622,-0.030998,-0.032872,-0.033753,-0.034331,-0.034779,-0.035606,-0.034734,-0.036064,-0.035776,-0.035403,-0.035612,-0.035127,-0.035001,-0.034282,-0.034294,-0.033301,-0.032929,-0.031403,-0.030693,-0.030222,-0.029307,-0.028084,-0.026658,-0.02594,-0.024732,-0.023463,-0.022341,-0.021528,-0.019534,-0.019149,-0.017029,-0.016704,-0.016327,-0.01563,-0.014453,-0.012486,-0.012718,-0.010967,-0.010313,-0.010093,-0.009384,-0.007825,-0.007904,-0.00645,-0.005753,-0.005094,-0.005048,-0.005684,-0.004376,-0.003541,-0.003801,-0.003587,-0.00399,-0.002263,-0.00257,-0.00284,-0.000816,-0.002176,-0.002531,-0.000263,-0.001135,0.000262,-0.000262,-0.001558,0.000333,-0.000068,-0.00046,-0.001225,0.000131,-0.000636,-0.000533,0.000217,-0.001021,-0.001169,-0.000297,0.001044,0.000129,-0.001358,-0.000789,-0.001522,0.000315,-0.000188,-0.000304,0.000087,0.000079,0.000885,-0.001148,-0.000697,0.000793,0.001192,-0.001071,-0.00089,-0.001359,-0.000633,-0.000297,-0.001424,-0.002021,-0.002435,-0.000375,-0.000967,-0.001515,-0.002186,-0.002656,-0.001331,-0.000764,-0.001857,-0.002725,-0.002357,-0.002908,-0.000474,-0.001669,-0.003345,-0.002845,-0.002119,-0.003684,-0.002277,-0.002197,-0.002631,-0.002767,-0.004192,-0.003362,-0.004532,-0.003143,-0.002627,-0.001944,-0.003225,-0.003213,-0.00288,-0.003434,-0.003009,-0.003318],"z":[0.033928,0.035702,0.035894,0.035189,0.035773,0.036891,0.038539,0.040539,0.041621,0.042294,0.045714,0.047252,0.048366,0.048434,0.049692,0.048946,0.050049,0.050772,0.04857,0.049028,0.047632,0.047336,0.045044,0.045155,0.043268,0.042817,0.038814,0.037063,0.034837,0.033264,0.031288,0.029406,0.027614,0.025124,0.023286,0.020641,0.01735,0.015388,0.014443,0.012018,0.009377,0.008601,0.007725,0.005785,0.002984,0.003384,0.00032,-0.000556,-0.000581,-0.002293,-0.003724,-0.004016,-0.005495,-0.006324,-0.007159,-0.007198,-0.006681,-0.007935,-0.008898,-0.008407,-0.008795,-0.00816,-0.010448,-0.00993,-0.009325,-0.011593,-0.00988,-0.009555,-0.011495,-0.010711,-0.012241,-0.011305,-0.010011,-0.011575,-0.010945,-0.011101,-0.009754,-0.010561,-0.010055,-0.010026,-0.011045,-0.009031,-0.008705,-0.01019,-0.010947,-0.010057,-0.00849,-0.008878,-0.007922,-0.009551,-0.009335,-0.009097,-0.009234,-0.009171,-0.010162,-0.007956,-0.008338,-0.009693,-0.01005,-0.00765,-0.007857,-0.007195,-0.007805,-0.00847,-0.006787,-0.006286,-0.005998,-0.007938,-0.007408,-0.006674,-0.006133,-0.005673,-0.006758,-0.007245,-0.006017,-0.005299,-0.005617,-0.004993,-0.007605,-0.006241,-0.004536,-0.005148,-0.005761,-0.004683,-0.005324,-0.005526,-0.005001,-0.005081,-0.003598,-0.004473,-0.00318,-0.004533,-0.004915,-0.005376,-0.004478,-0.004481,-0.004717,-0.004082,-0.004435,-0.004221],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","legend":"legend2","legendgroup":"Head 1_1","marker":{"color":"rgb(50, 205, 50)","line":{"color":"rgb(50, 205, 50)","width":0},"showscale":false,"size":12,"symbol":"circle"},"mode":"markers","name":"Head 1","showlegend":true,"x":[null],"y":[null],"z":[null],"type":"scatter3d","scene":"scene2"},{"hovertemplate":"%{text}","legend":"legend2","legendgroup":"Head 2_1","marker":{"color":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140],"colorscale":[[0,"rgb(255, 230, 230)"],[0.5,"rgb(220, 20, 60)"],[1,"rgb(139, 0, 0)"]],"opacity":1.0,"showscale":false,"size":2,"symbol":"circle"},"mode":"markers","name":"Head 2","showlegend":false,"text":[1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0],"x":[0.010441,0.010632,0.010621,0.010723,0.010726,0.010746,0.010771,0.010752,0.010732,0.010704,0.010668,0.010634,0.010619,0.010444,0.010492,0.010452,0.010132,0.009988,0.010056,0.009833,0.009408,0.009264,0.009138,0.008802,0.008904,0.008255,0.008504,0.007698,0.007409,0.007119,0.007173,0.007487,0.006907,0.007682,0.007345,0.007749,0.007852,0.008309,0.008728,0.009355,0.010104,0.010725,0.011688,0.012405,0.013831,0.014793,0.015993,0.016857,0.017768,0.020011,0.020445,0.022536,0.023569,0.023724,0.025044,0.025216,0.027124,0.027336,0.029151,0.028578,0.030098,0.029865,0.032338,0.030042,0.031065,0.032317,0.031191,0.0308,0.030549,0.030949,0.031548,0.02997,0.029429,0.028795,0.027651,0.026528,0.025935,0.024799,0.02414,0.024112,0.022869,0.020425,0.020534,0.019729,0.018599,0.016929,0.016076,0.015714,0.013986,0.013175,0.012092,0.011345,0.010138,0.009299,0.008835,0.007896,0.006466,0.005759,0.00523,0.003939,0.00285,0.003039,0.002104,0.001462,0.001381,0.00039,-0.000911,-0.001696,-0.001336,-0.001705,-0.002749,-0.001894,-0.003488,-0.003837,-0.004451,-0.004595,-0.003991,-0.005125,-0.006193,-0.005975,-0.006063,-0.005503,-0.005467,-0.007064,-0.006053,-0.006437,-0.007263,-0.006401,-0.006319,-0.005525,-0.006321,-0.007178,-0.006594,-0.005007,-0.00696,-0.006224,-0.006968,-0.005732,-0.005727,-0.006484],"y":[-0.020817,-0.020832,-0.020805,-0.020829,-0.020848,-0.020845,-0.020853,-0.020856,-0.020863,-0.020877,-0.020881,-0.020894,-0.020909,-0.020978,-0.020991,-0.021005,-0.021114,-0.021244,-0.021215,-0.021346,-0.021584,-0.021711,-0.021795,-0.022191,-0.022161,-0.022871,-0.022756,-0.023562,-0.02408,-0.024673,-0.025079,-0.025409,-0.026401,-0.025511,-0.027278,-0.027128,-0.028326,-0.028659,-0.029213,-0.029094,-0.030298,-0.030418,-0.031309,-0.030182,-0.031891,-0.031333,-0.031619,-0.032019,-0.031375,-0.030575,-0.030746,-0.030339,-0.030187,-0.028808,-0.028348,-0.027734,-0.026928,-0.025726,-0.024177,-0.023993,-0.021856,-0.02139,-0.020612,-0.019516,-0.017338,-0.017135,-0.015082,-0.01469,-0.013673,-0.012831,-0.011728,-0.009172,-0.009236,-0.00872,-0.007286,-0.007236,-0.007712,-0.006448,-0.005292,-0.004587,-0.004065,-0.004925,-0.003727,-0.003034,-0.003732,-0.003527,-0.002447,-0.002982,-0.003191,-0.003883,-0.002376,-0.002845,-0.003294,-0.002932,-0.002531,-0.003929,-0.003456,-0.004187,-0.003693,-0.005746,-0.005153,-0.005609,-0.005983,-0.006098,-0.007576,-0.006798,-0.007421,-0.006899,-0.008392,-0.008661,-0.008266,-0.010331,-0.009199,-0.009625,-0.010046,-0.010671,-0.011315,-0.01118,-0.010609,-0.011585,-0.012189,-0.01272,-0.013125,-0.01291,-0.01342,-0.013609,-0.013832,-0.014135,-0.014975,-0.015223,-0.015252,-0.014977,-0.015467,-0.015732,-0.015533,-0.016171,-0.016087,-0.016225,-0.016505,-0.01674],"z":[0.015304,0.015271,0.015276,0.015253,0.015259,0.015255,0.015244,0.015247,0.015252,0.015266,0.015276,0.015283,0.015287,0.015343,0.015309,0.015321,0.015457,0.015437,0.015448,0.015497,0.015602,0.015625,0.015661,0.015688,0.015614,0.01564,0.015575,0.015679,0.015568,0.015466,0.015263,0.014888,0.014786,0.014785,0.014135,0.014088,0.013435,0.013073,0.012616,0.012339,0.011419,0.011045,0.01009,0.010242,0.008777,0.008524,0.007797,0.007187,0.006974,0.006225,0.005816,0.005102,0.004553,0.004983,0.004538,0.004719,0.004124,0.004586,0.004319,0.004563,0.004867,0.005116,0.004197,0.00573,0.006084,0.005607,0.006954,0.007296,0.00773,0.007898,0.007869,0.009597,0.009923,0.01033,0.01117,0.0118,0.011974,0.012892,0.013472,0.013687,0.014216,0.015142,0.01538,0.015843,0.016087,0.016764,0.017358,0.017368,0.017962,0.017907,0.018671,0.018808,0.019009,0.019372,0.019579,0.019599,0.020139,0.020059,0.020272,0.02024,0.020653,0.020495,0.020512,0.020731,0.020406,0.020841,0.020839,0.021209,0.020841,0.020809,0.021149,0.020315,0.021009,0.020887,0.020942,0.020752,0.020378,0.020762,0.02113,0.020743,0.020471,0.020343,0.020234,0.020385,0.020195,0.02011,0.020321,0.019987,0.019761,0.019491,0.019667,0.019858,0.019602,0.019236,0.01962,0.019273,0.019427,0.019147,0.01904,0.019074],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","legend":"legend2","legendgroup":"Head 2_1","marker":{"color":"rgb(220, 20, 60)","line":{"color":"rgb(220, 20, 60)","width":0},"showscale":false,"size":12,"symbol":"circle"},"mode":"markers","name":"Head 2","showlegend":true,"x":[null],"y":[null],"z":[null],"type":"scatter3d","scene":"scene2"},{"hovertemplate":"%{text}","legend":"legend2","legendgroup":"Head 3_1","marker":{"color":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140],"colorscale":[[0,"rgb(255, 245, 220)"],[0.5,"rgb(255, 165, 0)"],[1,"rgb(184, 134, 11)"]],"opacity":1.0,"showscale":false,"size":2,"symbol":"circle"},"mode":"markers","name":"Head 3","showlegend":false,"text":[1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0],"x":[0.00881,0.010166,0.009971,0.010436,0.010326,0.010452,0.010574,0.010531,0.010508,0.010577,0.010471,0.010481,0.010628,0.010592,0.010591,0.010696,0.010741,0.010718,0.01066,0.010795,0.010765,0.0108,0.010792,0.010819,0.010796,0.010842,0.010871,0.010892,0.01087,0.010892,0.010894,0.010953,0.010921,0.010941,0.010965,0.011044,0.010995,0.01108,0.011134,0.011241,0.011262,0.011199,0.011311,0.011348,0.01144,0.011672,0.011578,0.011822,0.011925,0.011977,0.012049,0.012231,0.012133,0.012485,0.012447,0.012601,0.012834,0.012878,0.013281,0.013403,0.013474,0.013622,0.013785,0.013715,0.014226,0.013796,0.014321,0.014583,0.014136,0.014246,0.014182,0.014807,0.014763,0.014867,0.0143,0.01416,0.013994,0.013949,0.01385,0.014005,0.013721,0.013342,0.013082,0.012956,0.012398,0.012091,0.01207,0.011359,0.011128,0.010327,0.010088,0.009383,0.009724,0.008626,0.007488,0.00732,0.006825,0.007588,0.006194,0.004801,0.004689,0.004292,0.003523,0.002669,0.002965,0.001701,-0.000024,-0.000127,0.000157,0.00009,-0.001619,-0.001315,-0.002519,-0.003094,-0.003752,-0.003449,-0.004452,-0.005642,-0.005863,-0.00573,-0.00758,-0.00608,-0.007235,-0.00874,-0.008785,-0.007842,-0.009181,-0.009321,-0.008936,-0.009375,-0.008998,-0.008966,-0.009153,-0.009885,-0.009683,-0.009515,-0.009174,-0.009557,-0.00992,-0.011309],"y":[-0.022375,-0.021466,-0.021463,-0.021214,-0.021244,-0.021126,-0.021134,-0.021134,-0.021137,-0.021086,-0.021214,-0.021188,-0.021099,-0.021106,-0.021135,-0.021092,-0.021049,-0.02104,-0.021083,-0.020994,-0.021022,-0.020982,-0.021005,-0.020962,-0.020986,-0.020934,-0.020919,-0.020897,-0.020918,-0.020883,-0.020875,-0.020799,-0.02079,-0.020825,-0.020762,-0.020642,-0.020606,-0.020623,-0.020464,-0.020293,-0.020084,-0.020257,-0.020032,-0.019982,-0.019634,-0.019318,-0.019321,-0.018731,-0.018522,-0.018402,-0.018171,-0.017521,-0.01754,-0.01705,-0.01673,-0.016569,-0.015604,-0.015578,-0.014518,-0.014141,-0.013452,-0.012673,-0.012498,-0.012405,-0.010914,-0.011265,-0.009762,-0.009012,-0.00918,-0.008396,-0.008589,-0.00619,-0.005382,-0.005831,-0.005496,-0.005973,-0.006055,-0.005209,-0.003911,-0.002604,-0.002677,-0.003681,-0.002586,-0.002563,-0.003555,-0.003008,-0.001955,-0.002496,-0.003188,-0.004354,-0.003247,-0.003053,-0.003014,-0.003142,-0.002795,-0.003094,-0.0038,-0.005221,-0.00448,-0.006616,-0.005947,-0.006587,-0.006966,-0.007732,-0.008373,-0.009121,-0.009083,-0.009385,-0.011355,-0.011815,-0.011128,-0.012262,-0.012431,-0.014497,-0.013445,-0.013928,-0.016678,-0.015779,-0.016536,-0.017237,-0.018865,-0.019326,-0.020089,-0.020669,-0.021034,-0.02223,-0.021743,-0.023367,-0.024822,-0.024714,-0.025158,-0.026085,-0.026759,-0.026359,-0.027132,-0.027752,-0.027709,-0.028146,-0.028668,-0.029977],"z":[0.014442,0.014893,0.014897,0.015021,0.015006,0.015055,0.01506,0.015061,0.015053,0.015095,0.015021,0.015039,0.01508,0.015077,0.015059,0.015083,0.015102,0.015111,0.015081,0.015128,0.015115,0.015135,0.015123,0.015142,0.015131,0.015155,0.015157,0.015171,0.015162,0.015175,0.015174,0.015206,0.015212,0.015201,0.015218,0.015264,0.015282,0.015269,0.015336,0.015397,0.015493,0.015423,0.015506,0.015528,0.015662,0.015781,0.015788,0.016037,0.016077,0.016141,0.016235,0.016483,0.016516,0.016668,0.016806,0.016865,0.017276,0.017275,0.017672,0.017818,0.018105,0.018431,0.018482,0.018505,0.019132,0.019014,0.019578,0.019916,0.019827,0.020199,0.02008,0.021063,0.021431,0.021172,0.021336,0.021225,0.021138,0.021498,0.022049,0.022542,0.022578,0.022054,0.022607,0.022649,0.022348,0.02239,0.022889,0.022762,0.022456,0.02195,0.022469,0.022477,0.02251,0.02237,0.022787,0.022658,0.022273,0.021549,0.022078,0.021039,0.021394,0.021194,0.020911,0.020732,0.020398,0.020078,0.020099,0.020026,0.019172,0.018972,0.019225,0.018666,0.01869,0.017864,0.018357,0.018039,0.016815,0.017219,0.017056,0.016669,0.015951,0.015621,0.015249,0.015185,0.015004,0.014318,0.014636,0.013881,0.013218,0.013288,0.013053,0.012584,0.01224,0.01243,0.012162,0.011874,0.011848,0.011656,0.011456,0.010795],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","legend":"legend2","legendgroup":"Head 3_1","marker":{"color":"rgb(255, 165, 0)","line":{"color":"rgb(255, 165, 0)","width":0},"showscale":false,"size":12,"symbol":"circle"},"mode":"markers","name":"Head 3","showlegend":true,"x":[null],"y":[null],"z":[null],"type":"scatter3d","scene":"scene2"},{"hovertemplate":"%{text}","legend":"legend2","legendgroup":"Sum of Key Heads_1","line":{"color":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139],"colorscale":[[0.0,"#440154"],[0.1111111111111111,"#482878"],[0.2222222222222222,"#3e4989"],[0.3333333333333333,"#31688e"],[0.4444444444444444,"#26828e"],[0.5555555555555556,"#1f9e89"],[0.6666666666666666,"#35b779"],[0.7777777777777778,"#6ece58"],[0.8888888888888888,"#b5de2b"],[1.0,"#fde725"]],"width":2},"marker":{"color":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140],"colorscale":[[0.0,"#440154"],[0.1111111111111111,"#482878"],[0.2222222222222222,"#3e4989"],[0.3333333333333333,"#31688e"],[0.4444444444444444,"#26828e"],[0.5555555555555556,"#1f9e89"],[0.6666666666666666,"#35b779"],[0.7777777777777778,"#6ece58"],[0.8888888888888888,"#b5de2b"],[1.0,"#fde725"]],"opacity":1.0,"showscale":false,"size":0.1,"symbol":"circle"},"mode":"lines+markers","name":"Sum of Key Heads","showlegend":false,"text":[1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0],"x":[0.006252,0.006679,0.0044,0.004893,0.006901,0.00749,0.006438,0.006214,0.004697,0.005108,0.005247,0.0041,0.004137,0.002977,0.002206,0.001236,0.000762,-0.001006,-0.000627,-0.001509,-0.001915,-0.002406,-0.002332,-0.002026,-0.001914,-0.001768,-0.00017,0.001108,0.003779,0.003808,0.005116,0.007045,0.008204,0.009736,0.010867,0.014359,0.01693,0.017796,0.019809,0.020681,0.024947,0.026045,0.029198,0.029312,0.032584,0.034424,0.035976,0.037062,0.039576,0.04191,0.041806,0.04539,0.045414,0.044468,0.04672,0.04646,0.048833,0.047941,0.049023,0.048611,0.048757,0.04789,0.051251,0.046019,0.04684,0.047524,0.045544,0.044544,0.041596,0.041914,0.041473,0.038337,0.037356,0.034533,0.032228,0.029861,0.028363,0.024944,0.023354,0.023296,0.019948,0.015567,0.014995,0.013204,0.009174,0.007075,0.006268,0.004147,0.001188,-0.001949,-0.003968,-0.005689,-0.007897,-0.010221,-0.012707,-0.01307,-0.016286,-0.018041,-0.02021,-0.021866,-0.024103,-0.024206,-0.026828,-0.028661,-0.028423,-0.03023,-0.033197,-0.036629,-0.035555,-0.035158,-0.038096,-0.03646,-0.040653,-0.04245,-0.043238,-0.042761,-0.04264,-0.045277,-0.048467,-0.047534,-0.04777,-0.045823,-0.048268,-0.04954,-0.050414,-0.049826,-0.052307,-0.050731,-0.049341,-0.049389,-0.049448,-0.051464,-0.051102,-0.050578,-0.051295,-0.051113,-0.05156,-0.050005,-0.050531,-0.053215],"y":[-0.030477,-0.028242,-0.025575,-0.023191,-0.020279,-0.018312,-0.014664,-0.010906,-0.008494,-0.007835,-0.00652,-0.005875,-0.006031,-0.007643,-0.008674,-0.010119,-0.014363,-0.012664,-0.017249,-0.019382,-0.023705,-0.025467,-0.029077,-0.033377,-0.035087,-0.038673,-0.040728,-0.044622,-0.050986,-0.051997,-0.054578,-0.056521,-0.058881,-0.05689,-0.060758,-0.059819,-0.061423,-0.060997,-0.062688,-0.060275,-0.060537,-0.06111,-0.062158,-0.057118,-0.058475,-0.055868,-0.054197,-0.052408,-0.050839,-0.046885,-0.044394,-0.042784,-0.041083,-0.035129,-0.033623,-0.030859,-0.0274,-0.023846,-0.019121,-0.01736,-0.011854,-0.009453,-0.007346,-0.003591,0.001822,0.003449,0.008303,0.010804,0.014865,0.016773,0.019473,0.026324,0.02722,0.030308,0.032212,0.033025,0.032329,0.036672,0.040054,0.042444,0.04505,0.041708,0.044796,0.047491,0.047618,0.047617,0.048899,0.04877,0.047316,0.047534,0.050684,0.04999,0.050819,0.05083,0.052923,0.04852,0.048992,0.04856,0.05024,0.04288,0.044553,0.042918,0.042611,0.042426,0.03808,0.037752,0.037088,0.039295,0.034276,0.032801,0.033933,0.029229,0.031716,0.029378,0.029093,0.026867,0.022622,0.023605,0.026147,0.022533,0.018306,0.016971,0.016408,0.014738,0.015059,0.01346,0.013485,0.011003,0.006514,0.007324,0.004986,0.006123,0.005238,0.005609,0.00378,0.002517,0.002876,0.001591,0.000885,-0.000621],"z":[0.022523,0.021078,0.020506,0.019843,0.025684,0.028472,0.030375,0.035518,0.037669,0.039611,0.051438,0.05583,0.058072,0.062112,0.068307,0.067648,0.072695,0.071251,0.072019,0.074063,0.07484,0.074219,0.073453,0.072811,0.069069,0.069046,0.063469,0.060011,0.057676,0.054241,0.049896,0.043282,0.040642,0.036491,0.029738,0.02472,0.020109,0.015786,0.01054,0.005754,0.001465,-0.00231,-0.006855,-0.009439,-0.01686,-0.016933,-0.020506,-0.024636,-0.025885,-0.030089,-0.032745,-0.033761,-0.037803,-0.03769,-0.040775,-0.041069,-0.037853,-0.04039,-0.040873,-0.040098,-0.039549,-0.03697,-0.041642,-0.038765,-0.036414,-0.043011,-0.036467,-0.035581,-0.036293,-0.034689,-0.0365,-0.03189,-0.02994,-0.031631,-0.028365,-0.027183,-0.025712,-0.024121,-0.021998,-0.021774,-0.021932,-0.015839,-0.015328,-0.017736,-0.017977,-0.015765,-0.012406,-0.012443,-0.00976,-0.01309,-0.011617,-0.01106,-0.012058,-0.01062,-0.010869,-0.008701,-0.008649,-0.012123,-0.013073,-0.007818,-0.008819,-0.007738,-0.008777,-0.009105,-0.006398,-0.006761,-0.005699,-0.009799,-0.008882,-0.008334,-0.006634,-0.007677,-0.009724,-0.010531,-0.007956,-0.007095,-0.009879,-0.007822,-0.011549,-0.010715,-0.008107,-0.009721,-0.011143,-0.009265,-0.012131,-0.012776,-0.011216,-0.012868,-0.010628,-0.013407,-0.009299,-0.012086,-0.014706,-0.016353,-0.014442,-0.015291,-0.015294,-0.015633,-0.016658,-0.01712],"type":"scatter3d","scene":"scene2"},{"hoverinfo":"skip","legend":"legend2","legendgroup":"Sum of Key Heads_1","marker":{"color":"rgb(34, 144, 140)","line":{"color":"rgb(34, 144, 140)","width":0},"showscale":false,"size":12,"symbol":"circle"},"mode":"markers","name":"Sum of Key Heads","showlegend":true,"x":[null],"y":[null],"z":[null],"type":"scatter3d","scene":"scene2"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"scene":{"domain":{"x":[0.0,0.475],"y":[0.0,1.0]},"xaxis":{"title":{"text":""},"range":[-0.10975800000000001,0.112304],"showticklabels":false,"showgrid":false,"backgroundcolor":"rgba(255, 255, 255, 0)","gridcolor":"rgba(255, 255, 255, 0)","showbackground":false,"showspikes":false},"yaxis":{"title":{"text":""},"range":[-0.107462,0.121392],"showticklabels":false,"showgrid":false,"backgroundcolor":"rgba(255, 255, 255, 0)","gridcolor":"rgba(255, 255, 255, 0)","showbackground":false,"showspikes":false},"zaxis":{"title":{"text":""},"range":[-0.104815,0.115719],"showticklabels":false,"showgrid":false,"backgroundcolor":"rgba(255, 255, 255, 0)","gridcolor":"rgba(255, 255, 255, 0)","showbackground":false,"showspikes":false},"camera":{"eye":{"x":0.11,"y":0.11,"z":0.11}},"dragmode":"orbit","aspectmode":"cube"},"scene2":{"domain":{"x":[0.525,1.0],"y":[0.0,1.0]},"xaxis":{"title":{"text":""},"range":[-0.153215,0.151251],"showticklabels":false,"showgrid":false,"backgroundcolor":"rgba(255, 255, 255, 0)","gridcolor":"rgba(255, 255, 255, 0)","showbackground":false,"showspikes":false},"yaxis":{"title":{"text":""},"range":[-0.162688,0.152923],"showticklabels":false,"showgrid":false,"backgroundcolor":"rgba(255, 255, 255, 0)","gridcolor":"rgba(255, 255, 255, 0)","showbackground":false,"showspikes":false},"zaxis":{"title":{"text":""},"range":[-0.143011,0.17484],"showticklabels":false,"showgrid":false,"backgroundcolor":"rgba(255, 255, 255, 0)","gridcolor":"rgba(255, 255, 255, 0)","showbackground":false,"showspikes":false},"camera":{"eye":{"x":0.4,"y":0.4,"z":0.4}},"dragmode":"orbit","aspectmode":"cube"},"annotations":[{"font":{"size":16},"showarrow":false,"text":"Layer 0 Heads","x":0.2375,"xanchor":"center","xref":"paper","y":0.95,"yanchor":"bottom","yref":"paper"},{"font":{"size":16},"showarrow":false,"text":"Layer 1 Heads","x":0.7625,"xanchor":"center","xref":"paper","y":0.95,"yanchor":"bottom","yref":"paper"}],"title":{"text":"\u003cb\u003eIndividual Head Outputs Tile the Joint Output Space\u003c\u002fb\u003e","x":0.5,"xanchor":"center","y":0.98},"margin":{"l":0,"r":0,"t":30,"b":0},"legend":{"itemsizing":"constant","orientation":"h","x":0.25,"xanchor":"center","y":0.02,"yanchor":"bottom"},"legend2":{"itemsizing":"constant","orientation":"h","x":0.75,"xanchor":"center","y":0.02,"yanchor":"bottom"},"width":1200,"height":600,"showlegend":true,"font":{"family":"-apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, Oxygen, Ubuntu, Cantarell, \"Fira Sans\", \"Droid Sans\", \"Helvetica Neue\", Arial, sans-serif","color":"rgba(0, 0, 0, 0.8)"}},                        {"displayModeBar": false, "responsive": true}                    )                };                            </script>        </div>
<figcaption class='text-caption'>Comparison of Layer 0 (left) vs Layer 1 (right) average attention outputs in the PCA basis of the character count probes from 1 (dark) to 150 (light) characters. In each layer, the outputs from each head tile the space.<br>In Layer 0, each head output is almost 1-dimensional, while in Layer 1 heads display more curvature (which they got from Layer 0!).</figcaption></figure>
<h4>Embedding Geometry</h4>
<p>To understand how the character count is computed, we start at the very beginning: the embedding matrix.</p>
<p>As before, we can train probes or compute the average weights for every distinct token length in the embedding. We visualize the <span style='font-style: italic;'>token</span> character count probes for character length 1–14 and visualize their top principal components. Using the first 3 principal components, which capture 70% of the variance, we see that embedding character counts are arranged in a circular pattern (PC1 vs PC2) with an oscillating component (PC3). This pattern is consistent with the ones observed in <a href='#rippled-representations'>Rippled Representations are Optimal</a>.</p>
<figure class="gdoc-image" style="--img-width: 1158px; "><img src='img_019.png' /><figcaption class='text-caption'>PCA of embedding vectors in <d-math>W_E</d-math> averaged by token character length.</figcaption></figure>
<p>As with all of the counting manifolds, we also find <a href='#appendix-token-lengths'>features</a> that discretize this space into overlapping notions of short, medium, and long words.</p>
<h4>Attention Head Outputs Sum To Produce the Count</h4>
<p>To understand the counting mechanism, we will work backwards from the summed attention outputs to the embedding. Notably, we:</p>
<ul><li style='margin-left: 36pt;'><span style='font-weight: 700;'>Ignore MLPs</span> – The attention head outputs affect the character count representation 4× more than the MLPs, so we restrict our focus to attention;</li><li style='margin-left: 36pt;'><span style='font-weight: 700;'>Focus on </span><span style='font-weight: 700;'>First Two Layers</span> – Even after layer 0, counting probes have reasonable accuracy and there are coarse positional features. Therefore, we focus on how attention transforms the embeddings into the count and how layer 1 further refines this representation.</li></ul>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_020.png' /><figcaption class='text-caption'>The summed output of 5 important Layer 0 heads on one prompt by token. (Left) The inner product of the summed attention outputs and the character counting probes; (Right) How the argmax of this product compares to the true line count. Context position starts at the first newline, with newlines denoted with dashes.</figcaption></figure>
<p>We can decompose the sum above into the contribution from the output of each individual head in layer 0.<d-footnote>We omit a previous token head for visual presentation.</d-footnote> Under this lens, we see each head performing a relatively low rank computation akin to a classification.</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_021.png' /><figcaption class='text-caption'>The individual outputs of 4 important Layer 0 heads on one prompt projected onto the character count probes.</figcaption></figure>
<p>How do individual heads implement this behavior? We can break down the behavior of an individual head by analyzing its QK circuit (where it attends) and OV circuit (the linear transformation from the embeddings to the output) <d-cite key="nelhage2021mathematical"></d-cite>. </p>
<p><span style='font-weight: 700;'>QK Circuit</span>.  Each head <d-math>h</d-math> uses the previous newline as an “attention sink,” such that for some number of tokens after the newline (<d-math>s_h</d-math>), the head just attends to the newline. After <d-math>s_h</d-math> tokens, the head begins to smear its attention over its receptive field, which goes up to a maximum of <d-math>r_h</d-math> tokens.</p>
<figure class="gdoc-image" style="--img-width: 1498px; "><img src='img_022.png' /><figcaption class='text-caption'>The average attention to the previous newline as a function of the token index in the line. Like boundary heads, these counting heads specialize with different positional offsets.</figcaption></figure>
<p><span style='font-weight: 700;'>OV Circuit</span>. The OV circuit coordinates with the QK circuit to create a heuristic estimate based on the number of tokens in the line multiplied by the average token length (<d-math>\mu_c \approx 4</d-math>), with an additional length correction term. When attending to the newline, each head upweights the average token length multiplied by the head’s sink size: <d-math>s_h\times\mu_c</d-math> characters. If no attention is paid to the newline, then from the perspective of the head, the current token must be at least <d-math>s_h+r_h</d-math> tokens into the line and should upweight <d-math>(s_h+r_h) \times \mu_c</d-math> character outputs. Finally, the OV circuit applies an additional correction depending on whether the tokens in the receptive field are above or below average in length.</p>
<p>Below, we include a detailed walkthrough of L0H1.</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_023.png' /><figcaption class='text-caption'>The QK and OV circuit of counting head L0H1. Top right: the head output projected onto the character counting probes for 64 tokens of a single prompt (truncated to the first newline). Bottom right: the attention pattern (transposed of the canonical ordering). Top right: the average embedding vectors projected onto the character count probes via the OV matrix. Bottom right: a summary of the overall computation.</figcaption></figure>
<p>For a more detailed analysis of each head, see <a href='#appendix-mechanics'>The Mechanics of Head Specialization</a>. Layer 1 attention heads perform a similar operation, but additionally leverage the initial estimate of the character count (see <a href='#appendix-l1-attn'>Layer 1 Head OVs</a>).</p>
<h4>Computing the Line Width</h4>
<p>To compute the line width, the model seems to use a similar distributed counting algorithm to count the characters between adjacent newlines. However, one subtlety that we do not address in this work is how the line width is actually aggregated. It is possible that the model computes a global line width by taking the max over all line lengths in the document or uses an exponentially weighted moving average of the last several line lengths. We do note that the line width uses a partially disjoint set of heads, likely because the “attend to previous newline as a sink” mechanism needs modification when the <span style='font-style: italic;'>current</span> token is also a newline.</p>
<h3><a id='illusion' href='#illusion'>Visual Illusions</a></h3>
<p>Humans are susceptible to “visual illusions” in which contextual cues can modulate perception in seemingly unexpected ways. Famous examples include the Müller-Lyer illusion, in which arrows placed on the ends of a line can alter the perceived length of the line <d-cite key="howe2005muller"></d-cite>; the Ponzo and Sander illusions which also modulate perceived line length <d-cite key="yildiz2022review"></d-cite>; and others <d-cite key="schwartz2007space"></d-cite>. </p>
<figure class="gdoc-image" style="--img-width: 1396px; "><img src='img_024.png' /><figcaption class='text-caption'>Classic visual illusions in which perception of line-length is modulated.</figcaption></figure>
<p>Can we use our understanding of the character counting mechanism to construct a “visual illusion” for language models?</p>
<p>To get started, we took the important attention heads for character counting and investigated what other roles they perform on a wider data distribution. We identified instances in which heads that normally attend from a newline to the previous newline would instead attend from a newline to the two-character string <span class='prompt-inline'>@@</span>. This string occurs as a delimiter in git diffs, a circumstance in which you might want to start your line count at a location other than the newline:</p>
<div class='prompt-block'>
<p>⏎@@-14,30 +31,24 @@ export interface ClaudeCodeIAppTheme {⏎</p>
</div>
<p>But what happens when this sequence appears outside of a git diff context—for instance, if we insert <span class='prompt-inline'>@@</span> in the aluminum prompt without changing the line length?</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_025.png' /></figure>
<p>We find that it does modulate the predicted next token, disrupting the newline prediction! As predicted, the relevant heads get distracted: whereas with the original prompt, the heads attend from newline to newline, in the altered prompt, the heads also attend to the <span class='prompt-inline'>@@</span>.</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_026.png' /><figcaption class='text-caption'>Insertion of <span class='prompt-inline'>@@</span> ‘distracts’ an attention head which normally attends from \n back to the previous \n. (left) Original attention pattern (truncated). (right) Attention pattern (truncated) with <span class='prompt-inline'>@@</span> insertion. Now it also attends back to the <span class='prompt-inline'>@@</span>.</figcaption></figure>
<p>How specific is this result: does any pair of letters nonsensically inserted into the prompt fully disrupt the newline prediction? We analyzed the impact of inserting (at the same two positions) 180 different two-character sequences, half of which were a repeated character. We found that while most inserted sequences moderately impact the probability of predicting a newline, newline usually remains the top prediction. There was also no clear difference between sequences consisting of the same or different characters. However, a few sequences substantially disrupted newline prediction, most of which appeared to be related to code or delimiters of some kind: <span class='prompt-inline'>``</span>  <span class='prompt-inline'>>></span>  <span class='prompt-inline'>}}</span>  <span class='prompt-inline'>;|</span>  <span class='prompt-inline'>||</span>  <span class='prompt-inline'>`,</span>  <span class='prompt-inline'>@@</span>.</p>
<p>We further analyzed the extent to which there was a relationship between ‘distraction’ of the important attention heads and the impact on the newline prediction. Indeed we found that many of the sequences with potent modulation of newline probability––and especially code-related character pairs––also exhibited substantial modulation of attention patterns. </p>
<figure class="gdoc-image" style="--img-width: 972px; "><img src='img_027.png' /><figcaption class='text-caption'>Insertion of most pairs of characters only moderately impacts the probability of predicting a newline. A subset of pairs, most of which appear related to code or delimiters, substantially disrupt newline prediction. The impact on newline prediction (originally 0.79) is correlated with how much inserted tokens ‘distracts’ character counting attention heads.</figcaption></figure>
<p>While in the aluminum prompt the task is implicit, this illusion generalizes to settings where the comparison task is made explicit. These direct comparisons are perhaps more analogous to the Ponzo, Sander, and Müller-Lyer illusions, where the perception and comparison is more direct.</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_028.png' /></figure>
<p>These effects are robust to multiple choice orderings. Moreover, if the length of the text following the <span class='prompt-inline'>@@</span> exceeds that of the alternative choice, the alternative choice is selected as being shorter. </p>
<p>While we are not claiming any direct analogy between illusions of human visual perception and this alteration of line character count estimates, the parallels are suggestive. In both cases we can see the broader phenomena of contextual cues, and the application of learned priors about those cues, modulating estimates of object properties of entities. In the human case, priors such as three-dimensional perspective can influence perception of object size, or color constancy can influence estimates of luminance (such as in the checker shadow illusion). Here, one possible interpretation of our results is that mis-application of a learned prior, including the role of cues such as <span class='prompt-inline'>@@</span> in git diffs, can also modulate estimates of properties such as line length. </p>
<h3><a id='related-work' href='#related-work'>Related Work</a></h3>
<p><span style='font-weight: 700;'>Objective</span>.<span style='font-weight: 700;'>  </span>This work is at the intersection of LLM “biology” (making empirical observations about what is going on inside models; e.g. <d-cite key="lindsey2025biology,rogers2020primer"></d-cite>) and low level reverse engineering of neural networks  (attempting to fully characterize an algorithm or mechanism; e.g. <d-cite key="olah2020zoom,wang2022interpretability,nanda2023progress,li2025language"></d-cite>). Methodologically, our work makes heavy use of attribution graphs <d-cite key="ameisen2025circuit,ge2024automatically,dunefsky2024transcoders"></d-cite> with QK attributions <d-cite key="kamath2025tracing"></d-cite> built on top of crosscoders <d-cite key="lindsey2024crosscoders"></d-cite>.</p>
<p><span style='font-weight: 700;'>Linebreaking</span>. Michaud<span style='font-style: italic;'> et al. </span><d-cite key="michaud2023the"></d-cite> identified linebreaking in fixed-width text as one of the top 400 “quanta” of model behavior in the smallest model (70m parameters) in the Pythia suite.</p>
<p><span style='font-weight: 700;'>Position. </span> Prior interpretability work on positional mechanism has largely focused on <span style='font-style: italic;'>token</span> position (e.g., <d-cite key="yedidia2023gpt2,yedidia2023positional,voita2023neurons,chughtai2024understanding,gurnee2024universal"></d-cite>). These works have shown that there exist MLP neurons <d-cite key="voita2023neurons,gurnee2024universal"></d-cite>, SAE features <d-cite key="chughtai2024understanding"></d-cite>, and learned position embeddings <d-cite key="yedidia2023gpt2"></d-cite> with periodic structure encoding absolute token position. Our work illustrates how a model might also want to construct non-token based position schemes that are more natural for many downstream prediction tasks.</p>
<p>Others have also studied, even going back to LSTMs, the existence of mechanisms in language models for controlling the length of output responses <d-cite key="shi2016neural,moon2025length"></d-cite>, as well as performed more theoretical analyses of the space of counting algorithms <d-cite key="suzgun2019lstm,chang2024language"></d-cite>.</p>
<p><span style='font-weight: 700;'>Geometry and Feature Manifolds. </span>  Beyond position, there has been extensive work in understanding the geometric representation of numbers, especially in toy models (e.g., <d-cite key="nanda2023progress,zhong2023clock,morwani2023feature"></d-cite>) and in the context of arithmetic in LLMs (e.g., <d-cite key="stolfo2023mechanistic,zhou2024pre,nikankin2024arithmetic,kantamneni2025trig,hu2025understanding"></d-cite>). Collectively, these works have shown that both real LLMs and toy transformers learn periodic representations <d-cite key="zhou2024pre,kantamneni2025trig,hu2025understanding"></d-cite>, with numbers arranged in a helix to enable certain matrix multiplication based addition algorithms <d-cite key="nanda2023progress,kantamneni2025trig"></d-cite>, and that these representations are provably optimal in certain settings <d-cite key="morwani2023feature"></d-cite>. In our context, we similarly observe helical representations <d-cite key="kantamneni2025trig"></d-cite>, numeric dilation <d-cite key="alquboj2025number"></d-cite>, and distributed algorithms across components that collectively implement a correct computation <d-cite key="hanna2023does,hu2025understanding"></d-cite>.</p>
<p>Multidimensional features with clear geometric structure have been found in more natural contexts <d-cite key="gould2023successor,engels2025not,modell2025origins"></d-cite>, like in the representation and computation of certain ordinal relationships (e.g., months of the year). In vision models, curve detector neurons <d-cite key="cammarata2020curve"></d-cite> and features <d-cite key="gorton2024missing"></d-cite> have been especially well studied and closely resemble the kind of discretization we observe with the families of character counting features. Many other topics have received interpretability analysis of the underlying geometry, such as grammatical relations <d-cite key="hewitt2019structural,reif2019visualizing"></d-cite>, multilingual representations <d-cite key="chang2022geometry"></d-cite>, truth <d-cite key="marks2023geometry"></d-cite>, binding <d-cite key="feng2023language"></d-cite>, refusal <d-cite key="wollschlager2025geometry"></d-cite>, features <d-cite key="hindupur2025projecting,li2025geometry"></d-cite>, and hierarchy <d-cite key="park2024geometry"></d-cite>, though more conceptual research is needed <d-cite key="wattenberg2024relational"></d-cite>.</p>
<p>Perhaps most relevant is recent work from Modell et al. <d-cite key="modell2025origins"></d-cite>, who provide a more formal notion of a feature manifold, and propose that cosine similarity encodes the intrinsic geometry of features. When testing their theory, they observe highly structured and interpretable data manifolds that have ripples and dilation, similar to our counting manifolds. These observations raise a methodological challenge in how to best capture data with different structure (see e.g. <d-cite key="hindupur2025projecting,michaud2025understanding,huang2025decomposing"></d-cite>), but also the exciting hypothesis that many naturally continuous variables (e.g., <d-cite key="heinzerling2024monotonic,gurnee2024language"></d-cite>) exist in more organized manifolds.</p>
<p><span style='font-weight: 700;'>Biological Analogues</span><span style='font-weight: 700;'>.  </span>The geometric and algorithmic patterns we observe have suggestive parallels to perception in biological neural systems. Our character count features are analogous to place cells on a 1-D track <d-cite key="Moser2008PlaceCG"></d-cite> and our boundary detecting features are analogous to boundary cells <d-cite key="solstad2008representation"></d-cite>. These features exhibit dilation—representing increasingly large character counts activating over increasingly large ranges—mirroring the dilation of number representations in biological brains <d-cite key="dehaene2003neural,piazza2004tuning"></d-cite>. Moreover, the organization of the features on a low dimensional manifold is an instance of a common motif in biological cognition (e.g., <d-cite key="perich2025neural"></d-cite>). While the analogies are not perfect, we suspect that there is still fruitful conceptual overlap from increased collaboration between neuroscience and interpretability <d-cite key="vilas2024position,he2024multilevel,leshinskaya2025cognitively"></d-cite>.</p>
<h3><a id='discussion' href='#discussion'>Discussion</a></h3>
<p>In this paper, we studied the steps involved in a large model performing a naturalistic behavior. The linebreaking task, frequently encountered in training, requires the model to represent and compute a number of scalar quantities involving position in character count units that are not explicit in its input or output<d-footnote>Tokens do not come annotated with character counts, and there are no vertical bars on the page showing the line width.</d-footnote>, then integrate those values with the outputs of complex semantic circuits (that predict the next proper word) to predict the next token. We found sparse features corresponding to each important step of the computation, and for those steps involving scalar quantities, we were able to find a geometric description that significantly simplified the interpretation of the algorithm used by the model. We now reflect on what we learned from that process:</p>
<p><span style='font-weight: 700;'>Naturalistic Behavior and Sensory Processing</span>. Deep mechanistic case studies benefit from choosing behaviors that the model performs consistently well, as these are more likely to have crisper mechanisms. This means prioritizing tasks that are natural in pretraining over tasks that seem natural to human investigators, and ideally, that are easily supervisable. As in biological neuroscience, perceptual tasks are often both natural and easy to supervise for interpretability (e.g., it is easy to modify the input in a programmatic way). Although we sometimes describe the early layers of language models as responsible for “detokenizing” the input <d-cite key="elhage2022solu,gurnee2023finding,ferrando2024information,lad2024remarkable"></d-cite>, it is perhaps more evocative to think of this as perception. The beginning of the model is really responsible for <span style='font-style: italic;'>seeing</span> the input, and much of the early circuitry is in service of sensing or perceiving the text similar to how early layers in vision models implement low level perception <d-cite key="olah2020zoom,lepori2024beyond"></d-cite>.</p>
<p><span style='font-weight: 700;'>The Utility of Geometry.  </span>Many of the representations and computations we studied had elegant geometric interpretations. For example, the counting manifolds are the result of an optimal tradeoff between capacity and resolution, with deep connections to space-filling curves and Fourier features. The boundary head twist was especially beautiful, and after discovering one such head, we were able to correctly predict that there would need to be additional heads to provide curvature in the output. The distributed character counting algorithm was more complex, but we were still able to clarify our view by studying linear actions on these manifolds. For other computations, like the final breaking decision, the linear separation was clearly a part of the story but there must be some additional complexity we were not able to see yet to handle multitoken outputs. For the more semantic operations, we purely relied on the feature view. Of course, describing any behavior in full is immensely complicated, and there is a long list of possible subtleties we did not study: how the model accounts for uncertainty in its counting, its mechanism for estimating the line width given multiple prior lines of text, how it adapts to documents with variable line width, how it handles multiple plausible output tokens of different lengths or multitoken words, or various special cases (e.g., a LaTeX \footnote{} or a markdown link). For the inspired, we share transcoder attribution graphs for a fixed-width line break prompt on <a href='https://www.neuronpedia.org/gemma-2-2b/graph?slug=fourscoreandseve-1757368139332&pruningThreshold=0.8&densityThreshold=0.99&pinnedIds=14_19999_37&clerps=%5B%5B%2214_200290090_37%22%2C%22nearing+end+of+the+line%22%5D%5D'>Gemma 2 2B</a> and <a href='https://www.neuronpedia.org/qwen3-4b/graph?slug=fourscoreandseve-1757451285996&pruningThreshold=0.8&densityThreshold=0.99&clerps=%5B%5B%2230_117634760_39%22%2C%22nearing+end+of+line%22%5D%5D&pinnedIds=30_15307_39'>Qwen 3 4B</a>, using the new neuronpedia interactive interface.</p>
<p><span style='font-weight: 700;'>Unsupervised Discovery  </span>It likely would not have been possible to develop this clarity if it were not for the unsupervised sparse features. In fact, when we started this project, we attempted to just probe and patch our way to understanding, but this turned out poorly. Specifically, we did not understand what we were looking for (e.g. we didn’t know to distinguish line width vs. character count), where to look for it (e.g., we didn’t expect line width to only be represented on the newline), or how to look for it (we started by training 1-D linear regression probes). However, after identifying some relevant features but before spending substantial effort systematically characterizing their activity profiles, we were also confused by what they were representing. We saw dozens of features that were vaguely about newlines and linebroken text, but their differences were not obvious from flipping through the activating examples. Only after we tested these features on synthetic datasets did their role in the graph and the underlying computation become clear. We suspect better automatic labels <d-cite key="bills2023language,paulo2024automatically,gur2025enhancing"></d-cite> enhanced with agentic workflows <d-cite key="shaham2024multimodal,bricken2025automating"></d-cite> would accelerate this work, especially in less verifiable domains.</p>
<p><span style='font-weight: 700;'>Feature-Manifold Duality.  ​​</span>The discrete feature and geometric feature-manifold perspectives offer dual lenses on the same underlying object. For example, in this work the model's representation of character count can be completely described (modulo reconstruction error) by the activities of the features we identified, where the action of the boundary heads is described by virtual weights that expand out the feature interactions via attention head matrices. The same character count representation can be described by a 1-dimensional feature manifold – a curve in the residual stream parametrized by the character count variable – where linear action of the boundary heads is described by continuous “twisting” of the manifold. In general, geometric structures learned by the model will likely admit both global parametrizations and local discrete approximations.</p>
<p><span style='font-weight: 700;'>The Complexity Tax.</span> Despite this duality, the descriptions produced by the two perspectives differ in their simplicity. The discrete features shatter the model into many pieces, producing a complex understanding of the computation. This seems like a general lesson. It seems like discrete features and attribution graphs may provide a true description of model computation, which can be found in an automated way using dictionary learning. Getting any true, understandable description of the computation is a very non-trivial victory! However, if we stop there, and don't understand additional structure which is present, we pay a <span style='font-style: italic;'>complexity tax</span>, where we understand things in a needlessly complicated way. In the line breaking problem, constructing the manifold paid down this tax, but one could imagine other ways of reducing the interpretation burden.</p>
<p><span style='font-weight: 700;'>A Call for Methodology. </span>Armed with our feature understanding, we were able to directly search for the relevant geometric structures. This was an existence proof more than a general recipe, and we need methods that can automatically surface simpler structures to pay down the complexity tax. In our setting, this meant studying feature manifolds, and it would be nice to see unsupervised approaches to detecting them. In other cases we will need yet other tools to reduce the interpretation burden, like finding hierarchical representations <d-cite key="costa2025flat"></d-cite> or macroscopic structure <d-cite key="olah2023interpretability"></d-cite> in the global weights <d-cite key="ameisen2025circuit"></d-cite>.</p>
<p><span style='font-weight: 700;'>A Call for Biology.  </span>The model must perform other elegant computations. We can find these by starting with a specific task the model performs well, study this from multiple perspectives, develop methodology to answer the remaining questions, and relentlessly attempt to simplify our explanations. Because the investigation is grounded in specific examples of a behavior, it provides a fast feedback loop, can shed light on weaknesses of existing methods and inspire new ones, and can sharpen our conceptual language for understanding neural networks. We would be excited to see more deep case studies that adopt this approach.</p>
<!--
<h3><a id='appendix' href='#appendix'>Appendix</a></h3>
<p>--></p>
</d-article> <d-appendix id="appendix">
<h3><a id='citation-info' href='#citation-info'>Citation Information</a></h3>
<p>For attribution in academic contexts, please cite this work as</p>
<pre class="citation short">Gurnee, et al., "When Models Manipulate Manifolds: The Geometry of a Counting Task", Transformer Circuits, 2025.</pre>
<p>BibTeX citation</p>
<pre class="citation long">@article{gurnee2025when,<br>  author={Gurnee, Wes and Ameisen, Emmanuel and Kauvar, Isaac and Tarng ,Julius and Pearce, Adam and Olah, Chris and Batson, Joshua},<br>  title={When Models Manipulate Manifolds: The Geometry of a Counting Task},<br>  journal={Transformer Circuits Thread},<br>  year={2025},<br>  url={https://transformer-circuits.pub/2025/linebreaks/index.html}<br>}</pre>
<h3><a id='acknowledgments' href='#acknowledgments'>Acknowledgments</a></h3>
<p>We would like to thank the following people who reviewed an early version of the manuscript and provided helpful feedback that we used to improve the final version: Owen Lewis, Tom McGrath, Eric Michaud, Alexander Modell, Patrick Rubin-Delanchy, Nicholas Sofroniew, and Martin Wattenberg. We are also thankful to all the members of the interpretability team for their helpful discussion and feedback, especially Doug Finkbeiner for discussions of rippling and ringing, Jack Lindsey on framing, Tom Henighan for feedback on clarity, Brian Chen for improving the design of the figures and line edits of the text, and the team who built the attribution graph <d-cite key="ameisen2025circuit"></d-cite> and QK attribution infrastructure <d-cite key="kamath2025tracing"></d-cite>. </p>
<h3><a id='appendix-task-performance' href='#appendix-task-performance'>Haiku Task Performance</a></h3>
<p>Haiku is able to adapt to the line length for every value of <d-math>k</d-math>, predicting newlines at the correct positions with high probability by the third line. Of course, some error is to be expected even with a perfect estimate of line length, as the model may incorrectly predict the next semantic token. Below is the mean log-prob and accuracy for newline prediction of Haiku on 200 prose sequences that were synthetically wrapped to have lines of character length <d-math>k</d-math>, for <d-math>k = 20, 40, \ldots, 140</d-math>.</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_029.png' /></figure>
<h3><a id='appendix-feature-splitting' href='#appendix-feature-splitting'>Feature Splitting and Universality</a></h3>
<p>It is natural to ask if the character counting features are fundamental, or simply one discretization of the space among many. We found that dictionaries of different sizes learn features with very similar receptive fields, so this featurization – including the slowly dilating widths – is in some sense <span style='font-style: italic;'>canonical</span>. We hypothesize that this canonical structure emerges from boundary constraint: positions near zero (start of line) create a natural anchoring point for feature development.</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_030.png' /></figure>
<p>The geometry of the decoder directions is also fairly consistent between the dictionaries, showing characteristic ringing.</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_031.png' /></figure>
<p>However, we do see some evidence of feature splitting. For example, below are three character count feature which activate on the same interval (~20–45 characters in the line), but differentially activate for lines of different widths: LCC2.a activates on all line widths, LCC2.b preferentially activates on long line widths, and LCC2.c preferentially activates when close to the line width boundary.</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_032.png' /></figure>
<p>Recent work has raised the possibility that feature dictionaries could behave pathologically where there exist feature manifolds <d-cite key="michaud2025understanding"></d-cite>, because a dictionary could allocate an increasing number of features in a finer tiling of the space. However, our observation that cross-coders of varying size tile this feature manifold in a canonical way suggest that this behavior does not occur in this setting.</p>
<h3><a id='appendix-width-features' href='#appendix-width-features'>Line Width Features</a></h3>
<p>Line width features tile the space similar to character count features.</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_033.png' /></figure>
<h3><a id='appendix-dynamical' href='#appendix-dynamical'>Dynamical System Model</a></h3>
<p>We simulate <d-math>N = 100</d-math> points on the unit <d-math>(n-1)</d-math>-sphere in <d-math>\mathbb{R}^n</d-math> (<d-math>n \in \{3,\ldots,8\}</d-math>) with pairwise forces: <d-math>\mathbf{F}_{ij} = \begin{cases} \frac{1 - (d_{ij} - 1)/2}{r_{ij}} \hat{\mathbf{r}}_{ij} & \text{when }d_{ij} \leq w \\ -\frac{\min(5, 1/r_{ij})}{r_{ij}} \hat{\mathbf{r}}_{ij} & \text{when }d_{ij} > w \end{cases}</d-math>, where <d-math>r_{ij} = \|\mathbf{x}_j - \mathbf{x}_i\|</d-math>, <d-math>\hat{\mathbf{r}}_{ij} = (\mathbf{x}_j - \mathbf{x}_i)/r_{ij}</d-math>, <d-math>w</d-math> is the attractive zone width parameter, and <d-math>d_{ij} = \min(|j-i|, |j-i+N|, |j-i-N|)</d-math> is the index distance (for the circular topology; for the interval it is just <d-math>d_{ij} = |j-i|</d-math>). Evolution follows <d-math>\dot{\mathbf{v}}_i = \sum_{j \neq i} \mathbf{F}_{ij} - 0.05\mathbf{v}_i</d-math> and <d-math>\dot{\mathbf{x}}_i = \mathbf{v}_i</d-math> with sphere constraint <d-math>\mathbf{x}_i \leftarrow \mathbf{x}_i/\|\mathbf{x}_i\|</d-math> enforced after each timestep (<d-math>\Delta t = 0.01</d-math>, damping <d-math>\alpha = 0.95</d-math>).</p>
<h3><a id='appendix-gibbs' href='#appendix-gibbs'>Analytic Construction of Ringing and Fourier Modes</a></h3>
<p>We explore a deeper connection between the ringing observed in the character count feature manifold, and a connection to Fourier analysis in an analytical construction.</p>
<p>Suppose that we wish to have a discretized circle's worth of unit vectors, each similar to its neighbors but orthogonal to those further away. Then the cosine similarity matrix <d-math>X</d-math> of these will be the circulant matrix of a narrow-peaked function <d-math>f</d-math> (left, below). The columns of the square root of <d-math>X</d-math> are <d-math>n</d-math> vectors <d-math>v_1,\ldots,v_n \in \mathbb{R}^n</d-math>, where <d-math>n</d-math> is the number of discrete points on the circle, whose inner products reproduce the similarity matrix.<d-footnote>The entire continuous circle embeds into the infinite-dimensional Hilbert space <d-math>L^2\mathbb{S^1}</d-math> via this construction.</d-footnote> Now suppose we would like to find vectors in a lower-dimensional space whose similarity matrix approximates <d-math>X</d-math>, like the model does for character counts. The best <d-math>k</d-math>-dimensional approximation of this similarity, in an <d-math>L^2</d-math> sense, is given by taking the eigendecomposition of <d-math>X</d-math> and truncating it to the top <d-math>k</d-math> eigenvectors; the square root of the result will provide <d-math>n</d-math> vectors in <d-math>k</d-math>-dimensions with the corresponding similarity pattern. If <d-math>\pi_k</d-math> is the projector onto the span of the top <d-math>k</d-math> eigenvectors, then the images <d-math>\pi_k v_i</d-math>, which live in a <d-math>k</d-math>-dimensional subspace, are precisely those vectors. We can see below that the resulting low-rank matrix has ringing (<a href='https://colab.research.google.com/drive/13L51UzyNQ6SnjNZRyhWsx_QuyQ3LoosW#scrollTo=c1q8ozayknvl'>colab notebook</a>). Finally, because the <a href='https://en.wikipedia.org/wiki/Circulant_matrix'>discrete Fourier transform diagonalizes circulant matrices</a>, the Fourier coefficients of <d-math>f</d-math> are in fact the principal values of <d-math>X</d-math>; the low-rank approximation consists of truncating the small fourier coefficients of <d-math>f</d-math>; and the resulting rows of <d-math>X</d-math> exhibit ringing.</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_034.png' /></figure>
<p>One essential feature of the representation of line character counts is that the “boundary head” twists the representation, enabling each count to pair with a count slightly larger, indicating that the boundary is close. That is, there is a linear map QK which slides the character count curve along itself. Such an action is not admitted by generic high-curvature embeddings of the circle or the interval like the ones in the physical model we constructed. But it is present in both the manifold we observe in Haiku and, as we now show, in the Fourier construction. First, note permutating the coordinates of <d-math>\mathbb{R}^n</d-math> by taking <d-math>e_i \mapsto e_{i+1}</d-math> has the effect of mapping <d-math>v_i \mapsto v_{i+1}</d-math>. That is, the (linear) action of this permutation <d-math>\rho</d-math> on <d-math>\mathbb{R}^n</d-math> acts by a rotation of the embedded circle with respect to its intrinsic geometry. Because conjugation by <d-math>\rho</d-math> fixes the circulant matrix <d-math>X</d-math>, it therefore respects its eigendecomposition, and thus commutes with the projection <d-math>\pi_k</d-math> onto the vector space spanned by its top <d-math>k</d-math> eigenvectors. The restriction of <d-math>\rho</d-math> to that subspace, <d-math>\overline{\rho}:=\pi_k \circ \rho \circ \pi_k</d-math>, acts by rotation on the lower-dimensional vectors <d-math>\overline{\rho}: \pi_k v_i \mapsto \pi_k v_{i+1}</d-math>. Thus we have found <d-math>n</d-math> vectors in a <d-math>k</d-math>-dimensional space, whose similarity is as close as possible to <d-math>X</d-math> (and has ringing), with a linear action of <d-math>\mathbb{R}^k</d-math> that rotates the vectors along a rippled embedded circle.</p>
<p>We evaluate whether a Fourier decomposition of the character count curve is optimal, and find that it is quite close given that it does not account for dilation. Fourier components explain at most 10% less variance than an equivalent number of PCA components, which are optimal for capturing variance.</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_035.png' /></figure>
<p>Finally, we note that as one moves through layers, the representation becomes more peaked. This sharpening of the receptive field is useful to the model to better estimate character counts, and corresponds to higher curvature in the embedding and, as predicted by the model above, more pronounced ringing. Below we show cross-sections (at character count 30, 60, 90, 120) of the cosine similarity matrix of probes trained after layers 0, 1, 2, and 3. With each subsequent layer, the graphs get more tightly peaked and secondary rings go higher.</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_036.png' /></figure>
<h3><a id='appendix-twisting' href='#appendix-twisting'>Geometry of Twisting</a></h3>
<p>Different heads access and manipulate the space in different ways. Below, we show the cosine similarity of both probe sets through QK for three heads: one which keeps them aligned, one which shifts character count to align better with later line widths, and one which does the opposite.</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_037.png' /></figure>
<p>We can also look at this transformation by visualizing the Singular Value Decomposition of each set of probes in a joint basis after passing them through QK. Once more, the alignment, left offset, and right offset can be read directly from the components.</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_038.png' /></figure>
<p>We can directly plot the first 3 components of the joint probe space after passing them through each QK. Doing so shows that one head keeps the representations aligned, while the others twist them either clockwise or counterclockwise.</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_039.png' /></figure>
<h3><a id='appendix-break-predictors' href='#appendix-break-predictors'>Break Predictor Features</a></h3>
<p>Boundary detector features (at about ~⅓ model depth) do not take into account the length of the next token.</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_040.png' /></figure>
<p>Later in the model, there exist features which incorporate both the number of characters remaining <span style='font-style: italic;'>and</span> the length of the most likely next token. These features only activate when the most likely next token is longer than the number of characters remaining (i.e. below the red diagonal below), as is the case in our aluminum prompt.</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_041.png' /></figure>
<p>We also found features for the converse: features which suppress the newline because the predicted next token is <span style='font-style: italic;'>shorter</span> than the number of characters remaining in the line.</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_042.png' /></figure>
<p>Both break prediction and suppression features sometimes also have interpretable logit effects on the output of all tokens, not just the newline. For instance, the features below respectively excite and suppress the newline as their top effect, but also systematically suppress tokens with more characters. This is because if the model is wrong about the value of the next token (and whether it's a newline), the token must at least be short enough to fit on the line.</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_043.png' /></figure>
<h3><a id='appendix-token-lengths' href='#appendix-token-lengths'>Representing Token Lengths</a></h3>
<p>We find layer 0 features that activate as a function of the character count of individual tokens.</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_044.png' /></figure>
<p>These features are overlapping (e.g. there are tokens for which the long word and medium word features are both active) and non-exhaustive (none of them fire on some common tokens, where we suspect the representation of character length is partially absorbed <d-cite key="chanin2024absorption"></d-cite> into features which just activate for that token).</p>
<h3><a id='appendix-mechanics' href='#appendix-mechanics'>The Mechanics of Head Specialization</a></h3>
<p>Heads collaborate to generate the count manifold, but <span style='font-style: italic;'>how</span> does each head aggregate counts?</p>
<p>As a toy model, consider the following construction for character counting with a single attention head:</p>
<ul><li style='margin-left: 36pt;'>The head uses the previous newline token as a “sink” where it defaults all of its attention to (i.e., attention 1)</li><li style='margin-left: 36pt;'>Each token since the newline gets <d-math>\alpha</d-math> attention, such that after <d-math>j</d-math> tokens the newline has <d-math>1 - \alpha j</d-math> attention. Note this limits the construction to only work on lines of up to <d-math>1 / \alpha</d-math> tokens, but the model could use multiple heads at different offsets to count for longer sequences. The model could also dedicate attention to tokens proportionally to token length</li><li style='margin-left: 36pt;'>The output of the head on the newline is 0, and the output of each non-newline token is a vector with the same direction but magnitude proportional to the character count of the token.</li></ul>
<p>This produces a ray with total length proportional to the character count of the line.</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_045.png' /></figure>
<p>In practice, we observe that individual attention heads do indeed use the newline as an attention sink, but at different offsets. As an example, we visualize the attention patterns of 4 important Layer 0 heads on several prompts with different line widths (starting from the first newline in the sequence).</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_046.png' /><figcaption class='text-caption'>Attention patterns of four important Layer 0 heads (columns) for 3 different prompts (rows) prompt showing head specialization. Patterns start from the first newline with red dashes indicating linebreaks.</figcaption></figure>
<p>To characterize the mechanism more precisely, we compute the average attention as a function of the number of tokens since the previous newline and also as a function of the character length of individual tokens.</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_047.png' /><figcaption class='text-caption'>Normalized attention as a function of tokens since newline (left) and token character length (right) for four heads. Each head specialized in a different offset similar to boundary heads.</figcaption></figure>
<p>Similar to boundary detection, individual attention heads specialize in particular offsets to tile the space. Moreover, we observe that most of these attention heads have a bias towards attending to longer tokens.</p>
<p>In addition to QK, a head can change its output based on the OV circuit <d-cite key="nelhage2021mathematical"></d-cite>. We study this by analyzing the pairwise interaction of probes as mediated by the OV matrix. Specifically, for the averaged token embedding vectors for each token length <d-math>E_t</d-math> <d-footnote>That is, for each token character length <d-math>i</d-math>, we compute the average embedding vector in <d-math>W_E</d-math>. We also prepend this with the newline embedding vector to make the plot below.</d-footnote>, our line length probes <d-math>P_c</d-math>, and the weight matrices for the attention output of each head <d-math>W_{OV}</d-math>, we compute <d-math>P_c^T W_{OV} E_t</d-math>.</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_048.png' /><figcaption class='text-caption'>Inner product of line character count and token character count through OV for 4 important layer 0 heads. The OV responses reflect the difference in attention pattern biases.</figcaption></figure>
<p>The output of each head can be thought of as having two components: (1) a character offset from the newline driven by the attention pattern and (2) an adjustment based on the actual character length of the tokens. Note that the average character count of a token is approximately 4.5 (and the median is 4), so we can interpret these effects shifting from a mean response (i.e. the transition point is always around count 4).</p>
<p>To walk through a head in action, consider the perspective of L0H1, which attends to the newline for the first ~4 tokens and then spreads out attention over the previous ~4–8 tokens:</p>
<ul><li style='margin-left: 36pt;'>While attending to the newline, L0H1 writes to the 5–20 character count (5–20CC) directions and suppresses the 30–80CC directions. This makes sense as an approximation because attending to the newline implies that the line is currently at most 3 tokens long (newline has no width) and on average, any 3 token span is ~15 characters (and is unlikely to be >30). </li><li style='margin-left: 36pt;'>While <span style='font-style: italic;'>not</span> attending to the newline at all, L0H1 defaults to predicting CC40, since not attending to the newline implies that there are ~8 tokens in the line with ~5 characters each on average (including spaces). Then, there is an additional correction applied depending on how long the tokens are:</li></ul>
<ul><li style='margin-left: 72pt;'>(1) If tokens being attended to are <span style='font-style: italic;'>short</span> (&lt;4 chars), then upweight 10–35CC and downweight >40CC.</li><li style='margin-left: 72pt;'>(2) If tokens being attended to are <span style='font-style: italic;'>long</span> (≥5 chars) then do the opposite.</li></ul>
<ul><li style='margin-left: 36pt;'>In cases with some attention on newlines and nonnewlines, linearly interpolate the above predictions.</li></ul>
<p>Other heads perform a similar operation, except with different offsets depending on their newline sink behavior. Layer 1 heads also perform a similar operation, though they also can leverage the character count estimate of the Layer 0 heads (see <a href='#appendix-l1-attn'>Layer 1 Head OVs</a>).</p>
<h3><a id='appendix-l1-attn' href='#appendix-l1-attn'>Layer 1 Head OVs</a></h3>
<p>Similar to the OVs of the Layer 0 attention heads, Layer 1 heads write to the character count features in accordance to how long the tokens they attend to are.</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_049.png' /></figure>
<p>However, in addition to the token character length, Layer 1 heads also use the initial line length estimate constructed in Layer 0 to create a more refined estimate of the character count.</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_050.png' /></figure>
<p>These repeated computations appear responsible for implementing the sharpening of representations.</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_051.png' /></figure>
<h3><a id='appendix-full-l0-attn' href='#appendix-full-l0-attn'>Full Layer 0 Attention Results</a></h3>
<p>Below, we show head sums for 3 different prompts with different line widths.</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_052.png' /></figure>
<p>As before, we can look at their decomposition.</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_053.png' /></figure>
<h3><a id='appendix-sensory' href='#appendix-sensory'>More Sensory and Counting Representations</a></h3>
<p>While in this work we carefully studied the perception of line lengths and fixed width text, there are many tasks which language models must perform which benefit from a positional, visual, or spatial representation of the underlying text. In the course of our investigation, we came across several other feature families and representations for these behaviors and report several below.</p>
<h4>What Follows an Early Linebreak?</h4>
<p>In addition to line width for tracking the absolute character length of a full line of text, there also exist features that are sensitive to lines which have ended early (i.e, lines where the character count is substantially shorter than the line width <d-math>k</d-math>). While these features are less useful for linebreaking, they enable the model to better predict the token following a linebreak. Specifically, if a line ends <d-math>c</d-math> characters before the line limit <d-math>k</d-math>, the next word should be at least <d-math>c</d-math> characters, otherwise it would have been able to fit in the previous line.</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_054.png' /><figcaption class='text-caption'>A feature family for how many characters were remaining in a line after it was broken.</figcaption></figure>
<p>It is worth emphasizing that the role of these features, like others in this work, is not obvious from a typical workflow of quickly looking at dataset examples. It might be tempting to ignore these as "newline" features, but careful analysis yields quite clear behavior.</p>
<h4>Markdown Table Representations</h4>
<p>In addition to prose, language models must parse other kinds of more structured data like tables. Accurate prediction of a table’s content requires careful integration of row and column information (e.g. is this a column of text or numbers?). To facilitate this, we use a synthetic dataset of 20 markdown tables to find feature families which activate on separator tokens, specialized to particular rows or columns. Visualizing feature activations on each of these 20 tables (arranged by location in the table) showed clear patterns.</p>
<figure class="gdoc-image" style="--img-width: 1408px; "><img src='img_055.png' /><figcaption class='text-caption'>A feature family for the row index in markdown tables. Activations are shown for 20 tables on the "|" token in the first column of the nth row.</figcaption></figure>
<figure class="gdoc-image" style="--img-width: 1092px; "><img src='img_056.png' /><figcaption class='text-caption'>A feature family for the column index in markdown tables. Activations are shown for 20 tables on the "|" tokens in the nth column.</figcaption></figure>
<p>On a synthetic dataset of larger tables, we also observe counting representations for the column and row index that resemble the character counting representations. Specifically, we see ringing in the pairwise probe cosine similarities and the characteristic “baseball seam” in the PCA basis.</p>
<figure class="gdoc-image" style="--img-width: 1999px; "><img src='img_057.png' /><figcaption class='text-caption'>Representations of markdown table row indices (left) and column indices (right). (Top) pairwise inner products of probes trained to predict the index; (bottom) probes projected into a 3D PCA basis.</figcaption></figure>
<h3><a id='appendix-title' href='#appendix-title'>Rejected Titles</a></h3>
<ul><li style='margin-left: 36pt;'>A General Language Assistant as a Laboratory for A-line-ment</li><li style='margin-left: 36pt;'>A-line-ment Science: The Geometry of Textual Perception</li><li style='margin-left: 36pt;'>The Geometry of Textual Perception: How Models Stay Aligned</li><li style='margin-left: 36pt;'>The Geometry of Counting: How Transformers Perceive and Manipulate Spatial Structure in Text</li><li style='margin-left: 36pt;'>The Mechanistic Basis of Alignment</li><li style='margin-left: 36pt;'>Reading between the lines: The Perception of Linebreaks</li><li style='margin-left: 36pt;'>Linebreaking: More than you wanted to know</li><li style='margin-left: 36pt;'>The Line Must Be Drawn Here! Character Counting in Neural Networks</li><li style='margin-left: 36pt;'>Newline, Who Dis? Attention Heads and Their Distractible Nature</li><li style='margin-left: 36pt;'>The End of the Line: How Language Models Count Characters </li><li style='margin-left: 36pt;'>How I Learned to Stop Worrying and Love the Carriage Return</li><li style='margin-left: 36pt;'>Breaking down the Linebreak Mechanism: The Geometry of Text Perception</li><li style='margin-left: 36pt;'>We Found a GPS Inside a GPT</li></ul>
</body>
</html>