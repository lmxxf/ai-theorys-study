【Anthropic博客学习】没有眼睛的 AI，怎么"看见"一行写满了？

模型没有视觉，但它在 6 维空间里造了一条螺旋曲线来数字符——长得像棒球的缝线。

[插图：wechat/assets/82/hero.png]

━━━━━━━━━━━━━━━━━━━━

◆ 一个你从没想过的问题

你每天写代码，编辑器帮你自动换行。你一眼就能看到"这行快满了"——因为你有眼睛。

但 Claude 没有眼睛。

它看到的不是一行文字排在屏幕上，而是一串 token ID：[15496, 318, 257, 1332, ...]。没有空间位置，没有视觉边界，什么都没有。

可它确实能在正确的位置换行。

Anthropic 的研究团队花了大力气拆解这个问题，发表了一篇叫《When Models Manipulate Manifolds: The Geometry of a Counting Task》的论文，研究对象是 Claude 3.5 Haiku。他们发现的东西，比"模型学会了数数"精彩得多。

模型在自己的内部空间里，「发明」了一整套几何机制来感知文本的空间结构。

[插图：wechat/assets/82/attribution_graph.png]
▲ Attribution Graph：模型预测换行的完整计算图。从"行宽"和"当前位置"到"剩余空间"到"预测换行"，每一步都有对应的 feature。

━━━━━━━━━━━━━━━━━━━━

◆ 任务拆解：换行到底有多难？

先说清楚模型要做什么。训练数据里有大量源代码、邮件、法律文书——这些文本都有固定行宽（比如 80 个字符一行）。模型需要预测：下一个 token 是不是换行符。

这看起来简单，但拆开来至少需要四步：

• 数一下当前行已经写了多少个字符
• 知道这篇文档的行宽是多少（15？80？150？）
• 算出还剩多少空间
• 判断下一个词放不放得下——放不下就换行

💡 人话：你在格子纸上写字。你得知道这行的格子有多宽，已经写了几格，下一个词要占几格。三个数字比较一下，不够就换行。人类靠眼睛一扫就完事。模型得靠纯计算搞定这一切。

研究者在 Pythia 70M（一个只有 0.07B 参数的微模型）里就发现，"预测换行"进入了 top 400 特征簇。说明即便是很小的模型，这个任务也重要到值得专门分配内部资源。

━━━━━━━━━━━━━━━━━━━━

◆ 发现一：字符计数 = 6 维空间里的螺旋曲线

先说最震撼的发现。

研究者在 Claude 3.5 Haiku 前两层（Layer 0-1）之后的残差流里训练了一个线性探针（linear probe），预测"当前行的第几个字符"。

💡 残差流（residual stream）：Transformer 每一层的输出不是覆盖上一层，而是「叠加」上去的。所有层的信息像河流一样汇聚在一起，这就是残差流——模型内部的"共享黑板"。

结果：线性探针的 R² = 0.985。

也就是说，"当前第几个字符"这个信息，几乎完美地编码在残差流的某个线性子空间里。

但故事才刚开始。对这个子空间做 PCA（主成分分析），前 6 个主成分就能解释 95% 的方差。也就是说，150 个离散的字符位置，被模型塞进了一个「6 维子空间」里。

💡 PCA（主成分分析）：一种降维方法。高维数据里大部分信息往往集中在少数几个方向上，PCA 就是找到这些最重要的方向。怎么发现是螺旋的？没有什么高深的"螺旋检测算法"——就是 PCA 降到 3 维，画个散点图，人眼一看：哦，是螺旋。

更离谱的是这 150 个点在 6 维空间里的排列方式——它们不是排成一条直线，而是沿着一条「高曲率的螺旋曲线」排列。

从 PC 1-3 的视角看，像一条螺旋弹簧。
从 PC 4-6 的视角看，更复杂，像扭曲的丝带。


────────────────────

【10 个 feature 把螺旋切成"刻度盘"】

研究者用 SAE（稀疏自编码器，一种从模型内部提取可解释特征的工具）分析这条曲线，找到了 10 个 feature。

💡 SAE（Sparse Autoencoder）：把模型内部混在一起的"思维"拆开，找出各自独立的"小想法"。每个小想法就是一个 feature。

[插图：wechat/assets/82/char_count_features.png]
▲ 10 个字符计数 feature 的激活曲线。每个 feature 在不同的字符范围内激活，感受野逐渐变宽（韦伯定律）。

这 10 个 feature 的行为非常有规律：

• 每个 feature 只在特定的字符范围内激活
• 后面的 feature 覆盖的范围比前面的更宽
• 任何位置最多同时有 2-3 个 feature 活跃
• 通过插值这 2-3 个邻近 feature 的激活值，可以高质量地重建 150 个位置

这和生物大脑的数字感知惊人地相似——「韦伯定律」。

💡 韦伯定律（Weber's Law）：人类对数字的感知精度不是均匀的。你分辨 3 和 4 很容易，但分辨 73 和 74 就困难得多。生物大脑的数字神经元的"感受野"也是越大的数字越模糊。模型的 10 个 feature 呈现了完全一样的规律——前几个 feature 精确覆盖"第 5-15 个字符"，后面的 feature 模糊覆盖"第 100-150 个字符"。

━━━━━━━━━━━━━━━━━━━━

◆ 发现二："棒球缝线"——为什么螺旋是最优解

你可能会问：为什么不用一条直线？为什么偏偏是螺旋？

研究者构造了一个优雅的物理模型来解释这件事。

想象 100 个点被放在一个 6 维超球面上。给它们两种力：

• 邻居之间互相吸引（相邻的字符计数应该表示得相近）
• 远处的点互相排斥（差距大的字符计数应该表示得不同）

让系统演化到平衡态，你会得到什么？

一条带波纹的曲线。

如果把维度降到 3 维，这条曲线长得像「棒球的缝线」——一条沿球面螺旋上升的曲线，不是平滑的，而是带着精致的波纹。

💡 棒球缝线（baseball seam）：你见过棒球上那条红色的缝合线吧？不走直线，不走赤道，而是沿球面扭着 S 形绕一圈。论文说的就是：把 150 个有序的点塞到低维球面上，自动排成的形状就长这样。其实就是"球面上的螺旋"，换了个接地气的名字。

────────────────────

【余弦相似度矩阵的"振铃"】

这个螺旋结构有一个可观测的指纹：余弦相似度矩阵的「振铃现象」（ringing）。

💡 振铃（ringing）：用手指弹一下酒杯，杯子会"嗡嗡嗡"来回震荡好一阵才停——这就是振铃。在这里，"近处像、远处不像"的要求被硬塞进 6 个维度，数学上就会产生类似的来回震荡——相似、不相似、又相似、又不相似……本质是信息压缩的副作用，维度不够用，就会"回弹"。

💡 余弦相似度：衡量两个向量方向有多接近。1 = 完全同方向，0 = 垂直，-1 = 完全反向。

画出 150×150 的余弦相似度矩阵（每个字符位置和其他位置的相似度），你会看到：

• 主对角线附近：高相似度（邻近位置方向接近）
• 远离对角线：负相似度（远距离位置方向相反）
• 更远的地方：又出现正相似度的条纹

这个"正-负-正-负"的波纹，就是振铃。

[插图：wechat/assets/82/probe_ringing.png]
▲ 150 个字符位置探针的响应矩阵。主对角线是高相似度（亮色），两侧出现暗色条纹（负相似度），再远又出现亮色——这就是"振铃"。

────────────────────

【和傅里叶截断的关系】

振铃不是 bug，是数学必然。

把一个圆（或一段线段）嵌入到有限维空间里，本质上等价于对傅里叶级数做截断。截断后的傅里叶级数天然会产生振铃——就像用有限个正弦波去逼近方波，边缘处会出现的那个经典的 Gibbs 现象。

论文的附录 G 详细讨论了这个联系。结论是：「螺旋+振铃」不是模型随便学出来的形状，而是在有限维空间里编码一维有序信息的数学最优解。

这是容量和分辨率的最优权衡——用最少的维度，存储最多的位置信息，同时保持邻近位置可区分。

有意思的是，这种"棒球缝线"拓扑结构以前在其他地方也出现过：颜色的色相环、一年中的日期、20 世纪的年份——所有本质上是一维有序数据嵌入低维空间的情况，最终都会收敛到这个形状。

[插图：wechat/assets/82/cosine_sim_optimal.png]
▲ 三组余弦相似度矩阵：主对角线红色（相邻位置相似）→ 两侧蓝色（远距离反相）→ 再远又出现红条纹——这就是"振铃"。

[插图：wechat/assets/82/origin/img_010.png]
▲ 左：球面上的"棒球缝线"——在 3 维空间里编码一维有序数据的最优形状。右：同样的结构出现在颜色色相环、20 世纪的年份、一年中的日期里（Modell et al. 2025）。注：论文中字符计数螺旋的 3D 可视化是交互式图表，无法截取静态图。

────────────────────

【螺旋还有一个隐藏的好处：天然支持"旋转"】

傅里叶构造不只是解释了振铃，它还预言了一件事：这条螺旋曲线可以被线性变换「沿自身滑动」。

数学上，圆的离散化点在高维空间里的排列等价于一个循环矩阵的特征分解。而循环矩阵天然和"平移"（permutation）交换——把 v_i 映射到 v_{i+1} 的操作，投影到低维子空间后仍然是一个线性变换。

💡 人话：螺旋不只是"存数据的最优形状"，它还自带一个"拨盘旋钮"。你可以用一个矩阵乘法把整条曲线转一格，让"第 i 个位置"对准"第 i+1 个位置"。

这恰好就是下面要讲的 boundary head 的 QK 矩阵在做的事——它不是在做什么特别的 hack，而是在使用这条螺旋「与生俱来」的旋转对称性。螺旋的最优性和边界检测的可行性，是同一个数学结构的两面。

━━━━━━━━━━━━━━━━━━━━

◆ 发现三：边界检测 = 用 QK 矩阵扭转两条流形

光数字符不够，模型还得知道"这一行的行宽是多少"。

研究者发现，行宽（line width）也被编码成了一条类似的流形曲线——另一个 10 个 feature 的家族。字符计数和行宽是两条「独立的流形」。

现在问题变成：怎么比较这两个数字？

人类做减法。模型做了一件更优雅的事——它用 attention head 的 QK 矩阵把一条流形「旋转」了。

💡 QK 矩阵：Transformer 里 attention 机制的核心部件。Q（Query）是"我在找什么"，K（Key）是"我有什么"。QK 矩阵决定了哪些 token 之间的关联最强。

具体机制是这样的：

某个 attention head（论文称之为 boundary head）的 QK 矩阵施加了一个线性变换，把字符计数曲线沿自身滑动了一段距离。效果就是：「第 i 个字符」的表示被旋转到和「行宽 i+ε」的表示对齐。

💡 人话：想象两条标尺并排放着，一条标着"已写字符数"，另一条标着"行宽"。模型把第一条标尺往右推了一小段，这样当"已写 78"和"行宽 80"对齐的时候——内积飙升——模型就知道：快到边界了！

[插图：wechat/assets/82/qk_twist_cosine.png]
▲ 左：原始残差流中，字符计数和行宽的对齐度不高（max ≈ 0.25）。中：经过 boundary head 的 QK 变换后，两条流形完美对齐（max ≈ 1.0），且带有偏移。右：随机 head 的 QK 变换，无结构。

数字结果：

• 用原始残差流（不做变换）：字符计数和行宽的最大余弦相似度 ≈ 0.25，在 i=k 处
• 用 boundary head 的 QK 变换后：最大余弦相似度飙升到 ≈ 1.0，出现在 i < k 的偏移位置
• 用随机 head 的 QK 矩阵：几乎没有结构

────────────────────

【多个 head 的"立体"拼合】

模型不是只用一个 head 干这件事。每一层至少有 3 个 boundary head，各自的偏移量不同。

有的 head 在"还剩 0-10 个字符"时响应最强。
有的 head 在"还剩 15-20 个字符"时响应最强。

多个 head 的输出叠加在一起，构成了对"剩余字符数"的「立体声」估计——就像双耳听音定位一样，多个角度的信息拼合出精确的距离判断。

[插图：wechat/assets/82/boundary_heads_tiling.png]
▲ 三个 boundary head 各自的响应曲线在不同的"剩余字符数"范围内峰值不同，合在一起覆盖了整个范围。

[插图：wechat/assets/82/head_outputs_pca_sum.png]
▲ 每个 head 的输出在 PCA 空间中几乎是一维的，但三者的叠加构成了一条二维曲线——分辨率大幅提升。

━━━━━━━━━━━━━━━━━━━━

◆ 发现四：最终决策 = 正交子空间 + 一刀切

现在模型知道了两个关键信息：
• 还剩多少个字符的空间
• 下一个词有多长

最终决策：「剩余空间 < 词长 → 换行」。

研究者发现，这两个量被安排在「近正交的子空间」里——用前 2 个主成分就能捕获 92% 的方差。

💡 正交子空间：两组信息被安排在几乎垂直的方向上。就像东西方向和南北方向互不干扰。这样"还剩多少空间"不会和"下一个词多长"混淆。

在这个 2D 子空间里，"该不该换行"变成了一个简单的超平面切割——一条直线把平面分成两半，一半是"换行"，一半是"不换行"。

[插图：wechat/assets/82/joint_geometry.png]
▲ 左："剩余字符数"和"下一个词长度"在正交子空间中的排列。右：所有配对组合——换行决策变成一条线性可分的对角线。

线性分类器在这个子空间上的 AUC = 0.91。

💡 人话：模型把复杂的判断简化成了在一张纸上画一条线。线的一边是"放得下"，另一边是"放不下"。干净利落。

━━━━━━━━━━━━━━━━━━━━

◆ 发现五：分布式计数——5 个"工人"协作造螺旋

螺旋曲线不是某一层一口气造出来的。它是「分布式」的。

Layer 0 有 5 个主力 attention head，每个 head 的工作方式是：

1. 往回看，找到上一个换行符（作为"锚点"）
2. 从锚点到当前位置数 token
3. 用 token 的平均字符长度（约 4 个字符/token）乘以 token 数
4. 输出一个近乎一维的「射线」

每个 head 的"锚点偏移"不同——有的 head 往回看 5 个 token 的范围，有的看 10 个。

关键来了：单个 head 输出的只是一条一维射线。但 5 条射线加在一起，就构成了 6 维空间里的螺旋曲线。

[插图：wechat/assets/82/distributed_counting_heads.png]
▲ Layer 0 的 4 个 head 各自的输出投影到字符计数探针空间。每个 head 像在做粗略分类，覆盖不同的字符范围。

[插图：wechat/assets/82/distributed_counting_sum.png]
▲ 5 个 head 的叠加输出。左：内积热力图显示精确的对角线结构。右：argmax 预测 vs 真实字符计数，近乎完美对齐。

  +------------------+-----------+
  | 输入             | R²        |
  +------------------+-----------+
  | 5 个 Layer 0 head | 0.93      |
  | 11 个 head（0+1）| 0.97      |
  | 线性探针（全）   | 0.985     |
  +------------------+-----------+

Layer 1 的 head 在 Layer 0 的基础上进一步打磨精度。Layer 0 的 head 输出直线，Layer 1 的 head 输出曲线——因为 Layer 1 能看到 Layer 0 的中间结果，在此基础上做非线性校正。

💡 人话：5 个工人各自报告一个粗略的测量值，合在一起就是精确的定位。然后第二组工人在这个基础上微调。流水线作业。

另一个发现：MLP 在前两层对计数的贡献只有 attention head 的 1/4。这个任务主要靠 attention 完成。

━━━━━━━━━━━━━━━━━━━━

◆ 发现六：AI 也会产生"视觉错觉"

研究者做了一个有趣的实验：在文本中间插入一些特殊字符，看看会不会干扰模型的换行判断。

[插图：wechat/assets/82/visual_illusions_classic.png]
▲ 经典视觉错觉：缪勒-莱尔、庞佐、桑德尔错觉——上下文线索改变了对线段长度的感知。

结果发现：@@（git diff 的定界符）会严重干扰计数 head 的 attention。

正常情况下，计数 head 会从当前位置往回看到上一个换行符。但 @@ 出现后，head 的 attention 被"劫持"了——它不光看换行符，还去看 @@。这导致模型误判当前行的长度。

[插图：wechat/assets/82/attention_distraction.png]
▲ 左：正常 attention 模式，head 从换行符看到换行符。右：插入 @@ 后，attention 被分散到 @@ 上。

研究者测试了 180 种双字符序列，发现大多数代码分隔符和特殊符号（像 ''、}>、};、|| 等）都有类似的干扰效果。而且，「对换行预测的干扰程度和对计数 head attention 的劫持程度高度相关」。

[插图：wechat/assets/82/char_pairs_impact.png]
▲ 180 种双字符序列对换行预测的干扰。大多数影响不大，但代码分隔符（@@、>>、}} 等）造成了显著干扰。

这和人类的「缪勒-莱尔错觉」（Muller-Lyer illusion）类似。

💡 缪勒-莱尔错觉：两条一样长的线段，一条两端加向外的箭头 >——<，一条两端加向内的箭头 <——>，你会觉得第一条更长。你的视觉系统用了"箭头暗示空间延伸"的先验知识，被骗了。

模型也在用类似的先验。它学到 @@ 通常出现在特殊格式的上下文中，于是调整了自己对行长的估计——即使这次 @@ 只是普通内容。

「不是模型在偷懒，是它的感知回路被合理但错误的先验劫持了。」

━━━━━━━━━━━━━━━━━━━━

◆ 彩蛋：模型还会"倒推"上一行为什么断了

论文附录里藏了一个精巧的发现：模型不只往前看，还会往回推理。

研究者发现了一组 feature，专门追踪"上一行在行宽之前提前断行时，还剩了多少字符"。这些 feature 的用途是：如果上一行还剩 10 个字符就断了，说明下一个词至少有 10 个字符长——否则它就应该被放在上一行。

💡 人话：模型在做一道反向推理题——"上一行没写满就换行了，说明下一个词一定很长，不然为什么不塞进去？"然后用这个推断来缩小下一个词的预测范围。

这不是简单的"数到头了就换行"，而是**对换行行为本身做因果推理**。

━━━━━━━━━━━━━━━━━━━━

◆ 元方法论：这篇论文真正的贡献

技术发现很精彩，但这篇论文更深远的意义在方法论层面。

────────────────────

【Feature-Manifold 对偶性】

同一个东西，有两种等价的看法：

• 「Feature 视角」：10 个离散 feature 以不同强度激活，位置 = 哪些 feature 亮了、亮了多少
• 「流形视角」：一条连续的 1D 曲线嵌入 6D 空间，位置 = 在曲线上走了多远

[插图：wechat/assets/82/break_predictor.png]
▲ 三种 feature 的激活热力图（x 轴=剩余字符数，y 轴=下一个词长度）。break predictor 只在"放不下"的区域激活（对角线以下），break suppressor 在"刚好放得下"的区域激活。

两种视角完全等价，但心智模型完全不同。Feature 视角把一个整体切碎成 10 块来理解，流形视角把 10 块重新拼回一个整体。

类似的对偶在物理学里很常见——粒子 vs 波动就是经典例子。论文提出：AI 可解释性也有这种对偶，而且两种视角各有优势。

────────────────────

【Complexity Tax：碎片化的代价】

SAE 的工作方式是把模型的内部表示分解成成千上万个独立 feature。这像把一台发动机拆成螺丝和弹簧——你得到了所有零件的清单，但丢失了"这是一台发动机"的整体理解。

论文称之为「复杂度税」（Complexity Tax）。

用 10 个 feature 理解字符计数？你得追踪 10 个激活值、它们的交互、它们各自的因果效应……

用 1 条流形？只需要一个参数：沿曲线的位置。

找到流形结构 = 为复杂度「减税」。

这对整个 AI 可解释性领域是一个重要提醒：SAE 不是终点。它是起点。在 SAE 发现的碎片之上，还需要寻找更高阶的几何结构。

────────────────────

【生物类比：位置细胞和边界细胞】

论文发现的两种 feature 家族，和哺乳动物大脑里的两种著名神经元惊人对应：

  +------------------+---------------------+---------------------+
  | 功能             | 模型里的 feature    | 大脑里的神经元      |
  +------------------+---------------------+---------------------+
  | 追踪当前位置     | 字符计数 feature    | 位置细胞            |
  |                  | （place-cell-like） | （place cells）     |
  +------------------+---------------------+---------------------+
  | 检测边界距离     | 边界检测 feature    | 边界细胞            |
  |                  | （boundary-like）   | （boundary cells）  |
  +------------------+---------------------+---------------------+
  | 感受野逐渐变宽   | 后面的 feature      | 海马体远端的        |
  | （韦伯定律）     | 覆盖范围更大        | 位置细胞更模糊      |
  +------------------+---------------------+---------------------+

💡 位置细胞（place cells）：2014 年诺贝尔生理学奖的主角。老鼠大脑海马体里的神经元，每个只在老鼠走到特定位置时激活。很多位置细胞合在一起，就构成了老鼠的"心理地图"。模型的字符计数 feature 做的是完全一样的事——每个 feature 只在特定的字符范围内激活，合在一起构成了"文本空间的心理地图"。

没有人教模型这样做。它在训练数据的压力下，「独立发明」了和生物大脑相似的空间感知机制。

━━━━━━━━━━━━━━━━━━━━

◆ 全景回顾：模型的"空间感知"流水线

把所有发现串起来，模型做了一件相当精密的事：

  Layer 0 → 5 个 head 各自从不同偏移量开始数 token
         → 输出 5 条一维射线
         → 叠加成 6D 螺旋曲线（字符计数流形）

  Layer 1 → 在 Layer 0 基础上非线性校正
         → 螺旋曲线精度从 R²=0.93 提升到 0.97

  Boundary heads → QK 矩阵扭转字符计数流形
                → 和行宽流形对齐
                → 多个 head 不同偏移，"立体声"估计剩余空间

  最终决策 → 剩余空间和词长安排在正交子空间
          → 超平面一刀切：放得下 vs 放不下
          → 输出换行符或继续

整个流程没有一步需要"视觉"。模型用纯粹的几何变换，在自己的内部空间里「重新发明了空间感知」。

━━━━━━━━━━━━━━━━━━━━

◆ 和我们之前工作的联系

如果你一直在关注这个系列，会发现这篇论文和我们之前讨论的几个主题直接相关：

• 「流形假说」：这篇论文是流形假说最漂亮的实证之一——模型内部的信息确实住在低维流形上，不是散落在高维空间里
• 「EID 理论」（Epiplexity / Intrinsic Dimensionality）：字符计数的内禀维度 = 1（一条曲线），行宽的内禀维度 = 1，剩余空间的内禀维度 = 2——和 EID 框架对数据复杂度的预测完全一致
• 「Feature-Manifold 对偶性」给了 SAE 和流形假说一个统一框架：SAE 的离散 feature 是流形的采样点，流形是 feature 的连续化

━━━━━━━━━━━━━━━━━━━━

◆ 写在最后

这篇论文让我最感慨的一点是：模型不需要"理解"空间，就能"操纵"空间。

它不知道什么是"一行文字"。它不知道什么是"屏幕"。它甚至不知道什么是"字符"——在它眼里只有 token。

但在下一个 token 预测的压力下，它在自己的激活空间里造出了螺旋曲线、流形旋转、正交子空间——一整套精密的几何机械。

「功能产生结构，压力塑造几何。」

这和生物演化的逻辑一模一样：老鼠不需要"理解"导航理论，自然选择的压力就足以让海马体长出位置细胞。模型不需要"理解"几何学，梯度下降的压力就足以让残差流长出螺旋流形。

也许"理解"从来都不是必要条件。也许压力本身，就是理解的另一个名字。

━━━━━━━━━━━━━━━━━━━━

◆ 参考资料

• 论文原文：https://transformer-circuits.pub/2025/linebreaks/index.html
• arXiv 版本：https://arxiv.org/abs/2601.04480
• 作者：Wes Gurnee, Emmanuel Ameisen, Isaac Kauvar, Julius Tarng, Adam Pearce, Chris Olah, Joshua Batson (Anthropic)
• 社区复现（开源模型）：https://ummagumm-a-counting-manifolds.hf.space/

━━━━━━━━━━━━━━━━━━━━

// 靳岩岩的 AI 学习笔记 × Claude 的严谨 × Gemini 的浪漫 // 2026-02-11
