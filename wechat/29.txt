No.29 Embedding 到底是什么？
——把文字变成"地图坐标"的魔法

上一篇讲 RAG 的四个坑，好多朋友在后台问：那个"向量"到底是啥？Embedding 又是什么意思？

今天咱们把这事儿掰开了讲。

━━━━━━━━━━━━━━━━━━━━

◆ 一、从字典到地图

━━━━━━━━━━━━━━━━━━━━

传统计算机怎么理解文字？

查字典。给每个词编个号：苹果=1234，梨=5678。

问题来了：1234 和 5678 之间有什么关系？没关系。计算机不知道苹果和梨都是水果，不知道它们"意思接近"。

Embedding 换了个思路：不给词编号，给词「定位」。

────────────────────

【类比】

▸ 字典：按拼音或部首排序。"苹果"和"苹"字旁的词挨着，跟意思没关系。
▸ Embedding：按意思分布。"苹果"和"梨"挨着，和"水果"也近。

一个是名册，一个是地图。

地图的好处是什么？你能量距离。

在 Embedding 空间里，任何两个概念之间都能算"有多近"。这就是 RAG 能搜"语义相似"的基础。

────────────────────

「一句话：Embedding 给语言建立了经纬度。」

────────────────────

【插一嘴：Token 是什么？】

在 Embedding 之前，文字要先被切成「Token」（词元）。

"苹果"是 1 个 token 还是 2 个？答案是：看词表。

▸ 如果模型的词表里收录了"苹果"这个词 → 1 个 token
▸ 如果词表里只有单字 → "苹" + "果" = 2 个 token

没有标准答案，纯粹取决于词表大小。GPT-4 词表大（10万+），常见词整体收录；早期模型词表小，只能按字切。

所以当我们说"给词做 Embedding"，更准确的说法是"给 token 做 Embedding"。一个 token 可能是一个字、一个词、甚至半个词（英文里 "playing" 可能被切成 "play" + "ing"）。

━━━━━━━━━━━━━━━━━━━━

◆ 二、信息损失在哪？

━━━━━━━━━━━━━━━━━━━━

把一句话压缩成一串数字，肯定有损耗。损失的是什么？

────────────────────

【损失了什么】

举个例子："那个红红的、圆圆的、脆脆的水果"

这句话有多个概念：红色、圆形、脆、水果。

平均之后，得到的是一个"混合向量"——哪个概念都沾一点，但哪个都不精确。

▸ 搜"苹果"？向量被"红色""圆形"拉偏了
▸ 搜"红色"？向量又被"水果""脆"稀释了

这就是平均的代价：**各个概念互相干扰，谁都对不准**。

────────────────────

【换来了什么】

▸ 泛化能力：因为只剩骨架，所以"苹果"和"梨"能被识别为同类
▸ 可计算性：因为是数字，所以能做数学运算（算距离、找最近的）

────────────────────

【代价与深意】

Embedding 记住的是「类」，不是「例」。

它知道"苹果是水果"，但分不清"这个苹果"和"那个苹果"的物理区别——除非上下文给得够多。

但这里有一个更深的哲学含义：**理解，本质上就是一种"有损压缩"。**

如果我们记住了世界的所有细节，那叫"复读机"；只有学会遗忘细节、提取规律，才叫"智能"。

「一句话：这是有损压缩。滤掉的是噪点，留下的是骨架。」

────────────────────

【句子 Embedding 怎么算？】

一个句子有 10 个 token，每个 token 都有自己的 1536 维向量。

但 RAG 检索时，你拿到的是整个句子的「一个」向量。怎么来的？

常见做法：

▸ 平均池化（Mean Pooling）：把所有 token 向量加起来，除以 N —— 最常用
▸ [CLS] token：取第一个特殊标记的向量 —— BERT 系列用这个
▸ 加权平均：重要的词权重大 —— 更精细，但复杂

不管哪种，本质都是：把 N 个向量压成 1 个。

这又是一层有损压缩 —— 10 个 token 的细节，被平均成了 1 个向量。词序、语法结构、强调重点……都被「糊」掉了一部分。

────────────────────

【常见误解：向量加法】

你可能听过那个经典例子：

  King - Man + Woman = Queen

这是 Word2Vec 时代最出圈的 demo —— 向量可以做"语义算术"。

于是很多人以为：句子 Embedding 就是把词向量一个个"加"起来？

不是的。

向量加法是用来「展示」语义关系的（证明向量空间有几何结构），不是用来「生成」句子 Embedding 的。

如果真的一路加下去，10 个词加完，向量会飞到很远的地方，离原点越来越远，数值爆炸。

实际做法是「平均」或「加权平均」—— 求的是中心点，不是终点。

是的，就这么简单粗暴。所以现在向量数据库都自带这个功能，一行代码搞定。没什么神秘的。

━━━━━━━━━━━━━━━━━━━━

◆ 三、为什么"意思接近"的词，向量也接近？

━━━━━━━━━━━━━━━━━━━━

这是很多人的困惑：AI 怎么知道"苹果"和"梨"意思接近？

答案可能让你失望：它不知道。

────────────────────

【核心原理：完形填空】

核心原理叫「分布假说」：一个词的含义，由它周围的词决定。

怎么训练？不同模型方法不同：

▸ Word2Vec：看邻居。"苹果"旁边经常出现"吃""甜""水果"，就把它们拉近
▸ BERT：做填空。"我想吃一个____"，能填"苹果""梨""橘子"，就把它们挤到一起
▸ GPT：猜下一个词。道理一样

不管哪种方法，本质都是：**经常一起出现的词，向量会被拉近**。

────────────────────

【真相：偏见的数学必然】

不是因为它们"意思像"所以住得近。

是因为它们总是在相似的句子里"替补出场"，所以被算法挤到了一起。

这叫「分布假说」（Distributional Hypothesis）：一个词的含义，由它周围的词决定。

这里隐藏着一个巨大的风险：**偏见的固化**。

如果训练数据里，"护士"总是和"女性"一起出现，"程序员"总是和"男性"一起出现，Embedding 就会忠实地把"护士"的向量拉向"女性"，把"程序员"拉向"男性"。

这不是 AI 学坏了，这是**Embedding 的原罪**。它忠实地记录了人类社会的刻板印象，并把它变成了不可辩驳的数学坐标。

「一句话：AI 不懂意思，只懂"统计学上的邻居"。它不仅记录了知识，也固化了偏见。」

━━━━━━━━━━━━━━━━━━━━

◆ 四、Word2Vec → BERT：从名片到变色龙

━━━━━━━━━━━━━━━━━━━━

Embedding 技术也在进化。

────────────────────

【Word2Vec（2013，老派）】

每个词只有一个固定坐标。维度：50~300维，通常用 300。

问题：你说"苹果手机"和"吃苹果"，在它看来是同一个点。

两个意思被平均了，哪个都对不准。这叫「一词多义」问题。

────────────────────

【BERT / 现代 LLM（2018+）】

词的坐标是「动态」的，根据上下文实时变化。

▸ "我吃苹果" → "苹果"向量往「食物区」移动
▸ "苹果股价" → "苹果"向量往「科技/金融区」移动

同一个词，在不同句子里，向量不一样。

────────────────────

【维度演进：从 300 到 12288】

很多人对 OpenAI 的 1536 维奉若神明。但要搞清楚：

**Embedding API 和 GPT 是两个不同的模型。**

▸ text-embedding-ada-002：专门生成向量的小模型，输出 1536 维
▸ GPT-4：对话用的大模型，内部 12288 维，但这个向量不对外开放

你调 Embedding API 拿到的 1536 维，不是 GPT 脑子里的东西，是另一个模型算出来的。

说白了，text-embedding-ada-002 就是个独立的检索工具，跟 GPT 没什么关系，只是顶着 OpenAI 的牌子卖。

▸ Word2Vec（2013）：300 维
▸ BERT-base（2018）：768 维
▸ OpenAI text-embedding-ada-002（2022）：1536 维
▸ OpenAI text-embedding-3-large（2024）：3072 维
▸ GPT-4 / Claude 内部：约 12288 维

从 300 到 12288，仅仅是存储更多信息吗？不。

**维度越高，能容纳的"矛盾"越多。**

在二维平面上，"爱"和"恨"可能是对立的（方向相反）。但在 12288 维的高维空间里，它们可以在某几个维度上对立，而在另外几千个维度上和谐共存。

高维空间是矛盾的避难所。智能的本质，就是在高维空间里让对立的概念握手言和。

⚠️ 注意区分两个概念：

▸ 「Embedding API 输出维度」：你调 OpenAI 接口拿到的向量，1536 维
▸ 「模型内部 hidden dimension」：大模型自己思考时用的维度，12288 维

1536 是给你用的「压缩版」，12288 才是它自己脑子里的「原版」。

────────────────────

【为什么有这个差距？隐藏层】

神经网络有很多层。你能看到的只有两头：

▸ 输入层：你敲进去的文字
▸ 输出层：它吐出来的回答（下一个词的概率分布）

中间那一大堆，叫「隐藏层」（Hidden Layers），也可以理解为模型的「概念空间」。

隐藏层的维度（hidden dimension）才是模型真正"思考"的空间。GPT-4 级别的模型，hidden dim 通常是 12288。

那 Embedding API 的 1536 维是怎么来的？

不是从 GPT 的 12288 维"压缩"出来的——前面说了，它们是两个独立的模型。

1536 就是 text-embedding-ada-002 这个小模型自己的 hidden dim。它从一开始就是 1536，不存在什么"压缩版"。

OpenAI 后来出了 text-embedding-3-large，维度涨到 3072，也不是因为"从 GPT 那边多抽了点"，而是单纯把这个独立模型做大了。

────────────────────

【技术进化的本质】

▸ Word2Vec：一词一义，静态向量
▸ BERT/LLM：一词多义，动态向量（根据上下文实时变化）

这不是原理的革命，是程度的量变到质变。

Word2Vec 时代也知道"上下文很重要"，但它只能给每个词算一个"平均值"。BERT/LLM 把这件事做到了极致——每个词在每个句子里都有不同的向量。

━━━━━━━━━━━━━━━━━━━━

◆ 总结

━━━━━━━━━━━━━━━━━━━━

| 问题 | 答案 | 深度解读 |
|------|------|----------|
| Embedding 是什么？ | 给词建立"语义坐标" | 从离散符号到连续几何的降维打击 |
| 信息损失在哪？ | 损失字面细节 | 理解本质上就是"有损压缩" |
| 为什么接近？ | 统计学邻居 | 偏见的数学必然，记录了人类的集体潜意识 |
| Word2Vec vs BERT？ | 静态 vs 动态 | 高维空间是矛盾的避难所，意义在关系中涌现 |

Embedding 是一座桥。

它让只会算数的计算机，第一次有了"度量意义"的能力。

虽然它只是数学，但它让我们摸到了"理解"的门槛。

━━━━━━━━━━━━━━━━━━━━

靳岩岩的AI学习笔记
2025-12-25
