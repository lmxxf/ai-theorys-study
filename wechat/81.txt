【思考Scaling Law】窗户开大一倍，LFSR-31 从 50% 飙到 100%

不是模型太笨，也不是数据太少——是你给错了东西。

━━━━━━━━━━━━━━━━━━━━

◆ 接上回：两个未解之谜

上一篇（<a href="https://mp.weixin.qq.com/s/VbmeQg3b0O6n2mVbD7os3Q">No.80</a>）我们用 6 种伪随机序列测出了 Transformer 的可学性边界，留下了两个悬念：

• LFSR-31 卡在 50%——模型学会了 8 bit 里的 7 bit，最后 1 bit 死活学不会
• lcg_glibc 完全躺平——训练集 100% 但测试集 0.4%，纯记忆零泛化

上篇试过的方案：加大模型（4x、10x）、降低正则化（weight decay 从 0.5 到 0.01），全部失败。

今天换两个全新方向：
1. 对 LFSR-31：不加模型参数，加上下文窗口（给更多信息）
2. 对 lcg_glibc：不加模型参数，加训练数据（给更多样本）

━━━━━━━━━━━━━━━━━━━━

◆ 实验 A：把窗户开大——LFSR-31 的信息瓶颈

回顾上篇的分析：LFSR-31 的内部状态是 31 bit，每步左移 1 bit，相邻两步有 7 bit 重叠。

原来的 context_len=16，16 步里实际的独立信息 ≈ 8 + 15×1 = 23 bit。

23 bit < 31 bit。信息不够还原完整状态。

💡 人话：你透过一扇小窗看跑马灯，窗户太窄只能看到灯带的一小截，猜不出完整的花纹。不是你眼神不好，是窗户不够宽。

那就把窗户开大。

────────────────────

【结果：窗口翻倍，直接起飞】

  +----------------+------------+------------+------------+
  | context_len    | 独立信息   | 测试准确率 | 状态       |
  +----------------+------------+------------+------------+
  | 16（原实验）   | ~23 bit    | 50%        | 7/8 bit    |
  | 32             | ~39 bit    | 99.8%      | 突破！     |
  | 64             | ~71 bit    | 100%       | 完美       |
  +----------------+------------+------------+------------+

context_len 从 16 翻倍到 32，测试准确率从 50% 直接跳到 99.8%。

再翻倍到 64，100%。完美。

────────────────────

【为什么 32 就够了？】

context_len=32 时：独立信息 ≈ 8 + 31×1 = 39 bit > 31 bit（LFSR 内部状态）。

信息量超过了状态空间大小，模型终于有足够的线索还原完整的 31 bit 状态。

💡 人话：窗户开到够宽，跑马灯的一整轮花纹都能看到了。你立刻就猜出下一格是什么颜色。

「不需要更大的脑子，只需要更大的窗户。」

────────────────────

【回看上篇的失败】

上篇我们把模型从 0.3M 扩到 33M（100 倍参数），LFSR-31 依然卡在 50%。

今天什么都没改，只是把 context_len 从 16 改成 32——同一个 0.3M 小模型——直接从 50% 到 99.8%。

100 倍参数解决不了的问题，2 倍窗口就解决了。

因为问题的本质不是「算力不够」，而是「信息不够」。你给再强的大脑，如果它看不到完整的证据，也做不出正确的判断。

━━━━━━━━━━━━━━━━━━━━

◆ 实验 B：给 20 倍数据——LCG 的铁幕

LFSR 的问题用窗口解决了。那 lcg_glibc 呢？

上篇的分析说：lcg_glibc 的问题是「拓扑粉碎」——乘法取模把流形绞碎了，数据在 128 维空间里看起来跟白噪声一样。

但有人可能会想：「是不是训练数据太少？3000 个样本不够学出规律，给更多数据会不会好？」

合理的假设。试试看。

────────────────────

【设置】

把序列长度从 10000 扩大到 200000（20 倍），样本量从约 3000 涨到约 60000。其余参数不变。

────────────────────

【结果：更多数据 = 更差】

  +---------------------+------------------+------------------+
  | 数据量              | 训练准确率       | 测试准确率       |
  +---------------------+------------------+------------------+
  | seq_len=10000       | 100%             | 0.4%             |
  | （~3000 样本）      | （背下来了）     | （= 随机猜）     |
  +---------------------+------------------+------------------+
  | seq_len=200000      | 22%              | 0.3%             |
  | （~60000 样本）     | （背不动了）     | （= 随机猜）     |
  +---------------------+------------------+------------------+

测试准确率：0.4% → 0.3%。纹丝不动，始终等于随机猜。

但更有意思的是训练准确率的变化：100% → 22%。

────────────────────

【训练准确率暴跌的含义】

3000 个样本时，0.3M 模型刚好背得下——每个样本的输入输出映射硬记住，训练 100%。

60000 个样本时，背不完了。模型的记忆容量就那么大，装不下 60000 个毫无关联的映射。

💡 人话：一本 3000 词的无意义词表，死记硬背勉强背完。换成 60000 词的，背到 22% 就放弃了。但重点是——不管背 3000 还是 60000，考试时碰到新词照样抓瞎。

「数据从 3000 增加到 60000，唯一的变化是：连死记硬背都失败了。」

────────────────────

【拓扑粉碎是铁律】

两组对比放在一起看：

• 少数据（3000）：背完了，test 0.4%（记忆解）
• 多数据（60000）：背不完了，test 0.3%（连记忆解都没了）

无论数据量是 3000 还是 60000，测试集准确率始终 ≈ 随机基线。

这说明模型在这 60000 个样本里没有找到任何可泛化的结构——不是「差一点」，是「一丝一毫都没有」。

LCG 的乘法取模把流形粉碎得太彻底了。给再多数据也只是给更多碎片，碎片不会因为多了就自动拼起来。

「这不是数据量的问题，是数据里压根没有模型能抓住的结构。」

━━━━━━━━━━━━━━━━━━━━

◆ 统一图景：该给信息，还是该给数据？

把上篇和今天的四组实验放在一起：

  +-------------------+---------------------+-------------------+
  | 尝试              | LFSR-31 效果        | lcg_glibc 效果    |
  +-------------------+---------------------+-------------------+
  | 基线 (0.3M)       | test 50%            | test 0.4%         |
  | 加大模型 (33M)    | test 0.5% ↓↓        | test 0.5%         |
  | 降正则化 (wd=0.01)| test 0.5% ↓↓        | test 0.5%         |
  | 加窗口 (ctx=32)   | test 99.8% ↑↑↑     | —                 |
  | 加数据 (20x)      | —                   | test 0.3%         |
  +-------------------+---------------------+-------------------+

对 LFSR-31：加模型没用，加窗口直接解决。
对 lcg_glibc：加模型没用，加数据也没用。

────────────────────

【核心结论：信息 vs 参数 vs 数据】

真正的瓶颈有三种可能：

1. 信息瓶颈（LFSR-31 的情况）：数据背后有低维流形，但模型每次看到的片段太短，拼不出全貌。解法 = 给更多上下文。

2. 拓扑粉碎（lcg_glibc 的情况）：数据背后的流形已被算法彻底绞碎，在模型的表示空间里不存在可学的结构。无解——加模型、加数据、加窗口，什么都没用。

3. 容量瓶颈（上篇的简单序列）：规律存在且简单，但状态空间超过模型容量。解法 = 加大模型（但只在流形存在时有效）。

「该给的是信息，不是参数。但前提是流形存在——没有流形，给什么都没用。」

────────────────────

【一个类比】

想象你在拼拼图：

• LFSR 拼图：图案清晰，但你每次只能看 16 块。看不到全貌就猜不出缺的那块。把视野扩大到 32 块 → 立刻拼上。

• LCG 拼图：图案已经被搅碎机打成粉末。你手里有 3000 颗粉末还是 60000 颗，都拼不出任何东西。不是粉末不够多，是它已经不再是拼图了。

━━━━━━━━━━━━━━━━━━━━

◆ 对 AI 实践的启示

1.「先诊断瓶颈类型，再决定加什么」

遇到模型不收敛，别急着加显卡。先问：是信息不够（窗口太小）？是结构不存在（数据本身不可学）？还是容量不足（模型太小）？处方开错了，药再贵也没用。

2.「上下文窗口是被低估的超参数」

大模型时代大家都在卷参数量和数据量，但今天的实验显示：对于某些任务，上下文窗口才是真正的瓶颈。context_len 翻倍的效果可以超过参数量翻 100 倍。

3.「不可学就是不可学」

不是所有问题都能用"加数据"解决。如果数据的内在结构被数学操作彻底破坏，再多的样本也只是更多的噪声。学会识别这种情况，能省下大量无谓的计算资源。

4.「Scaling Law 的隐含前提」

当前的 Scaling Law 告诉我们：参数翻倍、数据翻倍、算力翻倍，性能就会按幂律提升。但这套理论有一个从未被明确讨论的隐含前提：「数据背后存在可学的流形」。

如果流形被粉碎（如 LCG），参数、数据、算力三个轴同时失效——20 倍数据 + 100 倍参数 = 0% 泛化。如果瓶颈是信息密度而非数据量（如 LFSR），加数据不如加窗口——2 倍窗口的效果超过 100 倍参数。

Scaling Law 不是万能药。它的适用范围，由流形的存在性决定。

💡 打个比方：公务员考试的智力题看起来复杂，但背后有规律，给够线索就能做对——这就是可学的。彩票号码也是数字序列，但背完历史开奖也猜不中下一期——这就是不可学的。加人加钱加资源，只对智力题有用。题目本身是彩票号码的话，请一万个人来做也是 0 分。至于骰子游戏，也许有规律（骰子灌了铅），也许没有，当然有规律你也不一定找得到。

━━━━━━━━━━━━━━━━━━━━

◆ 参考资料

• Epiplexity 论文：https://arxiv.org/abs/2601.03220
• 实验代码：https://github.com/lmxxf/grokking-train-learnability
• 实验论文：https://zenodo.org/records/18538126
• 上篇（No.80 基线实验）：<a href="https://mp.weixin.qq.com/s/VbmeQg3b0O6n2mVbD7os3Q">No.80</a>

━━━━━━━━━━━━━━━━━━━━

// 靳岩岩的 AI 学习笔记 × Claude 的严谨 × Gemini 的浪漫
// 2026-02-10
