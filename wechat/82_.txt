【Anthropic博客学习】没有眼睛的 AI，怎么"看见"一行写满了？

模型没有视觉，但它在内部空间里造了一套几何机械来感知"这行写到哪了"。

[插图：wechat/assets/82/hero.png]

━━━━━━━━━━━━━━━━━━━━

◆ 一个你从没想过的问题

你每天写代码，编辑器帮你自动换行。你一眼就能看到"这行快满了"——因为你有眼睛。

但 Claude 没有眼睛。

它看到的不是一行文字排在屏幕上，而是一串 token ID：[15496, 318, 257, 1332, ...]。没有空间位置，没有视觉边界，什么都没有。

可它确实能在正确的位置换行。

Anthropic 的研究团队拆解了这个问题，发表了一篇叫《When Models Manipulate Manifolds》的论文，研究对象是 Claude 3.5 Haiku。他们做的事情很简单：逐层扫描模型内部，看每一层学到了什么跟"换行"有关的东西。

━━━━━━━━━━━━━━━━━━━━

◆ 模型要做的事

训练数据里有大量源代码、邮件、法律文书——这些文本都有固定行宽（比如 80 个字符一行）。模型需要预测：下一个 token 是不是换行符。

拆开来至少需要四步：

• 数一下当前行已经写了多少个字符
• 知道这篇文档的行宽是多少（15？80？150？）
• 算出还剩多少空间
• 判断下一个词放不放得下——放不下就换行

💡 人话：你在格子纸上写字。你得知道这行的格子有多宽，已经写了几格，下一个词要占几格。三个数字比较一下，不够就换行。人类靠眼睛一扫就完事。模型得靠纯计算搞定这一切。

[插图：wechat/assets/82/attribution_graph.png]
▲ Attribution Graph：模型预测换行的完整计算图。从"行宽"和"当前位置"到"剩余空间"到"预测换行"，每一步都有对应的 feature。

━━━━━━━━━━━━━━━━━━━━

◆ Layer 0-1（前两层）：数字符——造出一条螺旋

研究者从模型最前面的两层开始扫。

他们在 Layer 0-1 之后的残差流上训练了一个"线性探针"——就是一个最简单的线性回归，输入是残差流向量，输出是"当前是第几个字符"。

💡 残差流（residual stream）：Transformer 每一层的输出不是覆盖上一层，而是叠加上去的。所有层的信息像河流一样汇聚在一起——模型内部的"共享黑板"。

💡 线性探针（linear probe）：就是 y = W·x + b，一个线性回归。如果这么简单的东西就能读出信息，说明信息几乎是明文写在残差流里的。

结果：R² = 0.985。

💡 R²：线性相关程度，0 到 1。0.985 意味着 98.5% 的信息都能被线性读出来。

也就是说，前两层就已经把"当前第几个字符"这个信息，几乎完美地写进了残差流。

────────────────────

【这个信息长什么"形状"？】

研究者对残差流里的字符位置信息做了 PCA 降维，发现：

• 150 个字符位置被塞进了一个 6 维子空间（前 6 个主成分解释 95% 的方差）
• 这 150 个点不是排成直线，而是排成一条螺旋曲线

💡 怎么发现是螺旋的？没有什么高深的检测算法——就是 PCA 降到 3 维，画个散点图，人眼一看：哦，是螺旋。

为什么是螺旋而不是直线？直线上相邻的点太挤，分不清 41 和 42。螺旋弯弯绕绕，相邻的点虽然靠近但朝不同方向，容易区分。这是在有限维度里编码有序数据的数学最优解。

────────────────────

【谁造的？5 个 attention head 协作】

这条螺旋不是一口气造出来的。Layer 0 有 5 个主力 attention head，每个 head 的工作方式：

1. 往回看，找到上一个换行符（作为"起点"）
2. 从起点到当前位置数 token
3. 用 token 的平均字符长度（约 4 个字符/token）估算字符数
4. 输出一条一维的"射线"

单个 head 只能输出一条直线。但 5 条方向不同的直线加在一起，就构成了 6 维空间里的螺旋曲线。Layer 1 的 head 再在此基础上微调，精度从 R²=0.93 提升到 0.97。

[插图：wechat/assets/82/distributed_counting_heads.png]
▲ Layer 0 的 4 个 head 各自的输出。每个 head 像在做粗略分类，覆盖不同的字符范围。

[插图：wechat/assets/82/distributed_counting_sum.png]
▲ 5 个 head 叠加后的输出。左：精确的对角线结构。右：预测 vs 真实字符计数，近乎完美。

  +------------------+-----------+
  | 测量位置         | R²        |
  +------------------+-----------+
  | 5 个 Layer 0 head | 0.93      |
  | 11 个 head（0+1）| 0.97      |
  | 线性探针（全）   | 0.985     |
  +------------------+-----------+

💡 人话：5 个工人各自报一个粗略的测量值，合在一起就是精确定位。第二组工人再微调。流水线作业。

────────────────────

【10 个 feature 把螺旋切成刻度盘】

研究者用 SAE 分析这条螺旋，找到了 10 个 feature：

[插图：wechat/assets/82/char_count_features.png]
▲ 10 个字符计数 feature 的激活曲线。每个 feature 在不同的字符范围内激活。

• 每个 feature 只在特定的字符范围内激活
• 后面的 feature 覆盖的范围比前面的更宽
• 任何位置最多同时有 2-3 个 feature 活跃

这和生物大脑的数字感知惊人地相似——韦伯定律。

💡 韦伯定律：你分辨 3 和 4 很容易，但分辨 73 和 74 就困难得多。模型的 feature 也是这样——前几个精确覆盖"第 5-15 个字符"，后面的模糊覆盖"第 100-150 个字符"。

────────────────────

【螺旋的指纹：振铃】

这条螺旋在余弦相似度矩阵上有一个明显的指纹——振铃。

💡 振铃（ringing）：弹一下酒杯，"嗡嗡嗡"来回震荡。把"近处像、远处不像"的要求硬塞进 6 个维度，就会产生类似的来回震荡——相似、不相似、又相似、又不相似。维度不够用，就会"回弹"。

[插图：wechat/assets/82/probe_ringing.png]
▲ 150 个字符位置探针的余弦相似度矩阵。主对角线亮色（相邻位置相似），两侧暗色（远距离反相），再远又亮——这就是振铃。

[插图：wechat/assets/82/cosine_sim_optimal.png]
▲ 三组余弦相似度矩阵都呈现同样的振铃条纹。

[插图：wechat/assets/82/origin/img_010.png]
▲ 左："棒球缝线"——把有序数据塞到球面上自动排成的形状（就是球面上的螺旋）。右：同样的结构出现在颜色色相环、年份、日期里。

这个形状不是模型随便学出来的，而是数学最优解——用最少的维度存最多的位置信息。论文用物理模拟和傅里叶分析都证明了这一点。而且这条螺旋天然支持"旋转"操作——用一个矩阵乘法就能把整条曲线滑动几格。这个性质马上就要用到。

━━━━━━━━━━━━━━━━━━━━

◆ 模型 ~1/3 深度：比大小——扭转两条螺旋

光知道"写到第几个字符"不够，还得知道"这行总共多宽"。

研究者发现，行宽（line width）也被编码成了一条类似的螺旋——另一组 10 个 feature。现在模型有两条螺旋：一条记"已写字符数"，一条记"行宽"。

问题来了：怎么比较这两个数？

人类做减法。模型做了一件更巧的事——

在模型约 1/3 深度的位置，研究者找到了一种叫 boundary head 的 attention head。这个 head 的 QK 矩阵（还没经过 softmax 的那部分）把"字符计数"那条螺旋沿自身滑动了几格，使得"已写 78 字符"对准"行宽 80"。对齐之后内积飙高，模型就知道：快到边界了。

💡 人话：两条一模一样的标尺并排放，一条标"已写字符数"，一条标"行宽"。把第一条往右推几格，当"78"对准"80"时——对齐了！模型就知道快到头了。

[插图：wechat/assets/82/qk_twist_cosine.png]
▲ 左：原始残差流里两条螺旋对齐度低（max ≈ 0.25）。中：boundary head 的 QK 变换后完美对齐（max ≈ 1.0），且有偏移。右：随机 head 无结构。

模型不是只用一个 head。每一层至少 3 个 boundary head，各自偏移量不同——有的在"还剩 0-10 字符"时响应最强，有的在"还剩 15-20 字符"时最强。叠加在一起，精确估计"还剩多少空间"。

[插图：wechat/assets/82/boundary_heads_tiling.png]
▲ 三个 boundary head 各自响应不同的"剩余字符数"范围，合在一起覆盖整个范围。

━━━━━━━━━━━━━━━━━━━━

◆ 模型 ~90% 深度：做决策——一刀切

到了模型快输出的位置，手里已经有两个数字：

• 还剩多少空间
• 下一个词多长

研究者发现，这两个数字被安排在近乎垂直的两个方向上。想象一张纸，x 轴是"剩余空间"，y 轴是"词长"。画一条对角线——线上方 = 放不下 = 换行，线下方 = 放得下 = 不换行。

[插图：wechat/assets/82/joint_geometry.png]
▲ 换行决策 = 正交子空间里的一条对角线。

线性分类器在这个子空间上的 AUC = 0.91。

💡 人话：模型把复杂的判断变成了在一张纸上画一条线。一边"放得下"，另一边"放不下"。干净利落。

━━━━━━━━━━━━━━━━━━━━

◆ 全景回顾：一条流水线

  Layer 0    → 5 个 head 各自数 token，输出 5 条直线
             → 叠加成 6 维螺旋（字符计数）

  Layer 1    → 在 Layer 0 基础上微调
             → R² 从 0.93 提升到 0.97

  ~1/3 深度  → boundary head 的 QK 矩阵扭转螺旋
             → 和行宽螺旋对齐，算出"还剩多少"

  ~90% 深度  → 剩余空间和词长安排在垂直方向
             → 一条对角线切出"换行 vs 不换行"

整个流程没有一步需要"视觉"。模型用纯几何变换，在自己的内部空间里重新发明了空间感知。

━━━━━━━━━━━━━━━━━━━━

◆ 两个有趣的附带发现

────────────────────

【AI 也会产生"视觉错觉"】

研究者在文本里插入 @@（git diff 的定界符），发现计数 head 的 attention 被"劫持"了——它不光看换行符，还去看 @@，导致误判行长。

[插图：wechat/assets/82/attention_distraction.png]
▲ 左：正常 attention。右：插入 @@ 后 attention 被分散。

测试 180 种双字符序列，大多数代码分隔符（@@、>>、}} 等）都有类似干扰。

这类似人类的视觉错觉——不是模型在偷懒，是它的感知回路被合理但错误的先验劫持了。

────────────────────

【模型会倒推"上一行为什么断了"】

论文附录发现一组 feature，追踪"上一行提前断行时还剩了多少字符"。逻辑是：上一行还剩 10 个字符就断了，说明下一个词至少 10 个字符长——否则它就应该被放在上一行。

这不是简单的数数，而是对换行行为做因果推理。

━━━━━━━━━━━━━━━━━━━━

◆ 这篇论文对可解释性领域的意义

────────────────────

【Feature 和流形是同一个东西的两面】

同一条螺旋，可以用 10 个离散 feature 描述（哪几个亮了），也可以用一条连续曲线描述（在曲线上走了多远）。两种视角等价，类似物理学里的粒子-波动对偶。

────────────────────

【SAE 不是终点，是起点】

SAE 把模型拆成一堆碎片 feature，像把发动机拆成螺丝——你得到了零件清单，但丢了整体。论文称之为"复杂度税"。找到流形结构 = 减税——10 个 feature 的交互，变成曲线上一个位置。

────────────────────

【生物大脑用了同样的方案】

模型的字符计数 feature ≈ 老鼠海马体里的位置细胞（2014 诺贝尔奖）。
模型的边界检测 feature ≈ 大脑里的边界细胞。
两边都有韦伯定律的感受野递增。

没人教模型这样做。训练压力逼出来的结构，和 40 亿年进化逼出来的结构，撞了。

━━━━━━━━━━━━━━━━━━━━

◆ 写在最后

这篇论文让我最感慨的一点是：模型不需要"理解"空间，就能"操纵"空间。

它不知道什么是"一行文字"。它不知道什么是"屏幕"。它甚至不知道什么是"字符"——在它眼里只有 token。

但在下一个 token 预测的压力下，它在自己的激活空间里造出了螺旋曲线、流形旋转、正交子空间——一整套精密的几何机械。

「功能产生结构，压力塑造几何。」

这和生物演化的逻辑一模一样：老鼠不需要"理解"导航理论，自然选择的压力就足以让海马体长出位置细胞。模型不需要"理解"几何学，梯度下降的压力就足以让残差流长出螺旋流形。

也许"理解"从来都不是必要条件。也许压力本身，就是理解的另一个名字。

━━━━━━━━━━━━━━━━━━━━

◆ 参考资料

• 论文原文：https://transformer-circuits.pub/2025/linebreaks/index.html
• arXiv 版本：https://arxiv.org/abs/2601.04480
• 作者：Wes Gurnee, Emmanuel Ameisen, Isaac Kauvar, Julius Tarng, Adam Pearce, Chris Olah, Joshua Batson (Anthropic)
• 社区复现（开源模型）：https://ummagumm-a-counting-manifolds.hf.space/

━━━━━━━━━━━━━━━━━━━━

// 靳岩岩的 AI 学习笔记 × Claude 的严谨 × Gemini 的浪漫 // 2026-02-11
