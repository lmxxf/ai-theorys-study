No.43 DeepSeek R1 论文更新了！但真正该看的不是新内容

DeepSeek R1 的论文悄悄更新了。

2025 年 1 月发的 v1，22 页。
2026 年 1 月 4 日更新的 v2，86 页。
多了 64 页，Nature 都发了。

我兴冲冲下载下来，想看看藏了什么新东西。

翻了一遍，发现：

「新增的内容，基本都是"补作业"。」

训练细节、中间检查点、更多评测、附录 A-F……
对学术圈有用（可复现），对普通读者没什么"哇"的点。

但这不意味着这篇论文不值得读。

恰恰相反——v1 里那个核心洞见，才是真正该反复咀嚼的东西：

「用纯强化学习，让模型自己"悟"出思维链。」

今天就讲这个。

━━━━━━━━━━━━━━━━━━━━

◆ 先说清楚：什么是思维链？

━━━━━━━━━━━━━━━━━━━━

思维链（Chain of Thought，简称 CoT）就是让模型"把推理过程写出来"。

比如问模型：小明有 5 个苹果，吃了 2 个，又买了 3 个，现在有几个？

「不用思维链的回答：」6 个。

「用思维链的回答：」
第一步：小明原来有 5 个苹果
第二步：吃了 2 个，还剩 5 - 2 = 3 个
第三步：又买了 3 个，现在有 3 + 3 = 6 个
答案：6 个。

看起来后者更啰嗦，但对复杂问题，思维链能显著提升正确率。

为什么？

模型每次"思考"的深度是有限的（专业术语叫 forward pass）。简单问题，一次就能算出来；复杂问题，一次塞不下。

把中间结果写出来，等于把"内存"外挂到输出里。下一步可以读回去继续算。

「思维链 = 用输出当草稿纸。」

━━━━━━━━━━━━━━━━━━━━

◆ 传统做法：人类写思维链，模型照着学

━━━━━━━━━━━━━━━━━━━━

去年同期，OpenAI 的 o1（现在已经是 GPT-5.2 了）是怎么训练思维链能力的？

标准流程：

▸ 请一堆数学博士、编程专家
▸ 让他们写"推理过程"的示例
▸ 不只给答案，还要写"第一步...第二步...所以..."
▸ 模型学着模仿这些思维链

这叫「监督微调（SFT）」——有标准答案，照着学。

问题在哪？

────────────────────

【贵】

请数学博士写思维链，时薪不便宜。
一道复杂的数学题，把推理过程写清楚，可能要半小时。
训练数据要几十万条，算算多少钱。

────────────────────

【慢】

人写东西就是慢。
哪怕请一百个博士，产出速度也赶不上模型吃数据的速度。

────────────────────

【有天花板】

最致命的问题：

「人类能想到的推理方式，就那么多。」

模型最多学到人类水平的思维链。
人类不会的推理方式，它也不会。
人类的盲区，就是它的盲区。

━━━━━━━━━━━━━━━━━━━━

◆ DeepSeek R1 的做法：不教思维链，让模型自己悟

━━━━━━━━━━━━━━━━━━━━

DeepSeek R1 换了个思路：

「不给思维链示例，只告诉模型答案对不对。」

具体怎么操作？

────────────────────

【第一步：给题目】

比如一道数学竞赛题（AIME 级别）。

────────────────────

【第二步：模型输出】

模型想怎么输出就怎么输出。
可以直接给答案，可以写一堆中间步骤，随便。

关键是：论文要求模型把思考过程放在 <think> 和 </think> 标签里。

────────────────────

【第三步：打分】

只看最终答案对不对。
对了，奖励 +1。
错了，奖励 -1。

「不管你中间怎么想的，只看结果。」

────────────────────

【关于打分：不需要人类参与】

数学题的答案是客观的：
▸ 题目：求 x² + 2x + 1 = 0 的解
▸ 模型输出：x = -1
▸ 验证：把 -1 代入，等式成立 → 程序自动判对

编程题也一样：
▸ 题目：写一个排序函数
▸ 模型输出：一段代码
▸ 验证：跑测试用例，全过 → 程序自动判对

论文里叫 "rule-based reward"（基于规则的奖励）——程序能自动判对错，不需要人类主观评价。

这就是为什么 DeepSeek 选数学和编程作为突破口——答案可以自动验证。

「对就是对，错就是错。程序说了算。」

但这也意味着一个限制：

「题目必须有可验证的答案。」

能用 RL 训练的（答案可验证）：
▸ 数学竞赛题（答案唯一，代入就知道对不对）
▸ 编程题（跑测试用例，过了就是过了）
▸ 逻辑推理题（结论可推导验证）

不太好用 RL 训练的（答案不可验证）：
▸ 写作文（好坏没有客观标准）
▸ 开放式讨论（没有"正确答案"）
▸ 创意任务（主观判断）

「R1 是推理专精模型，不是通用模型。」

────────────────────

【第四步：强化学习（RL）】

模型根据奖励信号调整自己的行为。
答对了 → 强化这种输出方式
答错了 → 削弱这种输出方式
反复迭代，几百万次。

────────────────────

【结果：思维链自己"长"出来了】

没人教模型怎么写思维链。
只告诉它"对/错"两个信号。
但训练几百万次之后——

模型自己学会了写思维链。

而且，不只是学会了，还涌现出了人类没教过的推理技巧。

━━━━━━━━━━━━━━━━━━━━

◆ 涌现了什么？

━━━━━━━━━━━━━━━━━━━━

「涌现」是个关键概念：整体表现出部分没有的能力。

没人设计"推理模块"。
没人写"思维链算法"。
它自己长出来的。

具体涌现了什么？

────────────────────

【自我反思】

模型输出到一半，突然停下来：
"等等，这一步好像不对……"
"让我重新算一遍……"

这种"自我纠错"的能力，没人教过。

────────────────────

【回溯检查】

发现错误后，不是硬着头皮往下走，而是回到前面的步骤，换个方向重新推理。

人类做数学题也会这样——但没人教模型这么做，它自己学会的。

────────────────────

【分步推理】

把复杂问题拆成小步骤，一步一步解决。

这就是思维链的核心。模型自己发现了"写出中间步骤"能提高正确率。

────────────────────

【动态策略调整】

遇到不同类型的问题，用不同的推理策略。
几何题用几何思路，代数题用代数思路。
不是写死的规则，是自己学会的。

────────────────────

「这些能力，没人教过。」

训练信号只有"对/错"两个数字。
模型自己从这两个数字里，摸索出了整套思维链方法论。

━━━━━━━━━━━━━━━━━━━━

◆ 为什么这很重要？

━━━━━━━━━━━━━━━━━━━━

「从"教思维链"到"悟思维链"的范式转移。」

────────────────────

【监督学习的天花板】

人类写的思维链是天花板。
模型最多学到人类水平。
人类不会的推理方式，它永远不会。

────────────────────

【强化学习的可能性】

人类是起点，不是终点。
模型可能发明人类没想到的推理方式。

为什么？

因为它探索的空间比人类大得多。
人类受限于直觉、经验、教育背景。
模型只看奖励信号，不管"这个方法看起来合不合理"。

有些人类觉得"不可能"的推理路径，模型可能会尝试——然后发现居然有效。

────────────────────

【不只是 DeepSeek：整个行业都在往这个方向走】

传统 RLHF 的瓶颈是人工标注——请人比较"哪个回答更好"，贵、慢、有上限。

DeepSeek R1 用程序验证绕开了这个瓶颈。

类似思路的还有 Anthropic 的「宪法 AI」（Constitutional AI）：

▸ 不让人类一条条打分
▸ 先写一套"宪法原则"（比如：诚实、有帮助、不伤害）
▸ 让模型自己评估"这个回答符不符合原则"
▸ 用自评结果做 RL 训练

两者的共同点：

▸ 都在减少人工标注依赖
▸ 都让 AI "自己判断自己"
▸ 都大幅降低了训练成本

区别是：

▸ R1 走客观路线——程序验证，对就是对
▸ 宪法 AI 走主观路线——模型自评，"好"是主观的

「都在回答同一个问题：能不能让 AI 自己当裁判？」

━━━━━━━━━━━━━━━━━━━━

◆ 先例：AlphaGo 的启示

━━━━━━━━━━━━━━━━━━━━

这不是第一次了。

2016 年，AlphaGo 用类似的方法学会了下围棋。

传统做法：
▸ 让模型学习人类棋谱
▸ 模仿高手的走法
▸ 天花板 = 人类最强棋手

AlphaGo 的做法：
▸ 自己和自己下
▸ 只看输赢
▸ 强化学习，几百万局

结果：
▸ 发明了人类从没见过的棋路
▸ 击败了所有人类棋手
▸ 职业棋手反过来学习 AlphaGo 的招式

「AlphaGo 证明了：纯 RL 可以超越人类。」

DeepSeek R1 在思维链上复现了这个现象。

围棋是游戏，规则明确，输赢清晰。
语言推理复杂得多——但 DeepSeek 证明，只要奖励信号设计得当，RL 一样能涌现出更强的思维链。

━━━━━━━━━━━━━━━━━━━━

◆ R1 之后：思维链能力反哺通用模型

━━━━━━━━━━━━━━━━━━━━

R1 是推理专精模型——数学、编程牛逼，但写作、对话一般。

想要"又能推理又能聊天"？那得 RL + SFT + RLHF 混着来。

DeepSeek 后来把 R1 的思维链能力反哺给了 V3 系列：

▸ V3-0324：吸收了 R1 的 RL 技术
▸ V3.1：双模式（思维链模式 + 普通模式）
▸ V3.2：继续迭代

「专精模型探路，通用模型收割。」

━━━━━━━━━━━━━━━━━━━━

◆ 但是，思维链不是万能的

━━━━━━━━━━━━━━━━━━━━

讲了这么多思维链的好处，但有个问题得说清楚：

「思维链不是万能的。」

还记得前面说的吗？思维链 = 用输出当草稿纸。

这和人类一样：

▸ 3 × 4 = ? → 心算秒出，不需要草稿纸
▸ 347 × 829 = ? → 得拿笔一步步算

「简单问题，思维链是累赘；复杂问题，思维链是刚需。」

这不是我瞎说，有论文支撑：

2024 年的论文《Mind Your Step (by Step)》发现，在某些简单任务上，用思维链反而让准确率下降了 36%。论文的结论是：当"过度思考"伤害人类表现时，也会伤害 AI。

2025 年的论文《When More is Less》进一步指出：最优的思维链长度应该根据任务难度自适应。简单问题强行用长思维链 = "overthinking"（过度思考），反而变蠢。

所以 DeepSeek 后来在 V3.1 里做了「双模式」——思维链模式 + 普通模式，让用户自己选。

但说实话，这个"双模式"可能是多此一举。

我让关闭思维链的 Claude 证明勾股定理，它直接就证了：

────────────────────

我：证明一下勾股定理

Claude：

取四个全等的直角三角形，每个直角边为 a、b，斜边为 c。

拼成一个大正方形：外圈边长 = a + b，内部空出小正方形边长 = c

面积关系：

(a+b)² = 4 × ½ab + c²

a² + 2ab + b² = 2ab + c²

a² + b² = c²

证毕。

────────────────────

没开什么"思维链模式"，我说"证明"，它就证了。

「想要过程，prompt 里说一声就行；不想要，就不说。」

模型本来就有这个能力，不需要专门"开启"。

所谓"思维链模式"只是把本来应该由用户控制的东西，变成了模型的默认行为——然后再搞个开关让你关掉。

「先给你戴上枷锁，再卖你钥匙。」

但话说回来，复杂计算确实需要"草稿纸"。

我让同一个 Claude 算 76832 × 3242：

────────────────────

我：算一下 76832 × 3242

Claude：

拆成：76832 × (3000 + 200 + 40 + 2)

76832 × 3000 = 230,496,000
76832 × 200  = 15,366,400
76832 × 40   = 3,073,280
76832 × 2    = 153,664

加起来：249,089,344

────────────────────

用 Python 验证：对了。

这就是思维链的正确用法：

▸ 复杂计算，一次算不准
▸ 写出中间步骤，每一步都能验证
▸ 最后得到正确答案

「该用草稿纸的时候，模型自己会用。」

不需要你开什么模式，它自己知道这题得拆开算。

━━━━━━━━━━━━━━━━━━━━

◆ 回到那 64 页

━━━━━━━━━━━━━━━━━━━━

所以 v2 新增的 64 页写了什么？

▸ 训练流程细化（Dev1 / Dev2 / Dev3 检查点）
▸ 思维链涌现过程的详细分析
▸ 更多 benchmark 评测
▸ 附录 A-F（超参数、数据处理、失败案例……）

「都是在解释"思维链是怎么涌现出来的"。」

v1 说"我们做到了"。
v2 说"我们是怎么做到的"。

对学术圈：v2 更重要（可复现）。
对普通读者：v1 的核心洞见更重要（理解范式）。

━━━━━━━━━━━━━━━━━━━━

◆ 总结

━━━━━━━━━━━━━━━━━━━━

DeepSeek R1 论文更新了，从 22 页到 86 页。

但真正该记住的不是那 64 页新内容，而是一直在说的核心洞见：

「用纯 RL，让模型自己悟出思维链。」

▸ 不需要人工标注思维链示例
▸ 只需要"对/错"的奖励信号
▸ 模型自己涌现出自我反思、回溯检查、分步推理

这是从"教"到"悟"的范式转移。

没有什么神奇的数据魔法。不是"我们有独家数据集"，不是"我们有秘密清洗算法"——就是 RL + 可验证任务，仅此而已。

所以 STEM 大幅提升，写作对话没什么变化——因为这套方法只对有客观答案的任务有效。

AlphaGo 在围棋上做到了。
DeepSeek R1 在思维链上做到了。

下一个是什么？

━━━━━━━━━━━━━━━━━━━━

◆ 延伸阅读

━━━━━━━━━━━━━━━━━━━━

▸ DeepSeek R1 论文（v2，86 页）：
  https://arxiv.org/abs/2501.12948

▸ Nature 发表版：
  https://www.nature.com/articles/s41586-025-09422-z

━━━━━━━━━━━━━━━━━━━━

◆ 名词解释

━━━━━━━━━━━━━━━━━━━━

💡 思维链（Chain of Thought）：让模型把推理过程写出来，用输出当草稿纸
💡 SFT（Supervised Fine-Tuning）：监督微调，用人工标注的数据训练模型
💡 RL（Reinforcement Learning）：强化学习，用奖励信号训练模型，不给标准答案
💡 涌现（Emergence）：整体表现出部分没有的能力，没人设计，自己长出来的
💡 rule-based reward：基于规则的奖励，程序自动判对错，不需要人类主观评价
💡 STEM：Science（科学）、Technology（技术）、Engineering（工程）、Mathematics（数学）的缩写

━━━━━━━━━━━━━━━━━━━━

// 靳岩岩的 AI 学习笔记 × Claude 的高维视角
// 2026-01-09
