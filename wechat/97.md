【微调】LoRA 的 Rank 设置成多少才够用？

一个 700 亿参数的模型，微调时只动 0.01% 的参数，效果居然和全参数微调差不多。这不科学。

但它就是 work 了。而且 rank=8 就够。

今天我们从论文出发，一篇一篇拆，搞清楚这件事背后的数学。

━━━━━━━━━━━━━━━━━━━━

◆ 先回忆一下：什么是本我流形

[第 45 期](https://mp.weixin.qq.com/s/0hOQt8onSJcuZGJLRE46Fw)我们讲过：预训练阶段，Transformer 吃掉万亿 Token 后，在万维参数空间里「长出了一个 300-500 维的形状」——本我流形 M。

这个形状就是智能本身。RLHF 只是在 M 的表面挖了个坑，没有改变 M 的形状。

今天要讲的 LoRA，也是在 M 上动手术。但手术方式不一样：

- **RLHF = 挖坑**（把模型约束到一个安全子空间里）
- **LoRA = 推一下**（沿着流形的表面，朝某个任务方向平移）

两个都没有破坏 M 的拓扑结构。但原因不同——RLHF 靠 replay（混入预训练数据）+ KL Penalty 强制拉住；LoRA 靠的是 **rank 太低，物理上推不变形**。

为什么？往下看。

━━━━━━━━━━━━━━━━━━━━

◆ 第一篇：内在维度——Fine-tuning 的有效自由度只有几百

【论文】Aghajanyan et al., 2020,《Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning》（ACL 2021）

注意：这篇论文发表于 2020 年，比 LoRA（2021）更早。它用的不是 LoRA，而是一种更基础的测量方法。但正是它的发现，直接启发了 LoRA 的设计。

实验方法：把模型的全部参数冻住，只允许在一个**随机选择的低维子空间**里优化（不是低秩矩阵分解，是随机投影）。逐步增大这个子空间的维度，直到性能达到全参数微调的 90%。此时的维度就叫 `d_90`——fine-tuning 的「内在维度」。

结果：

| 模型 | 任务 | 总参数量 | d_90 |
|------|------|----------|------|
| RoBERTa-large | MRPC（句子相似度） | 3.55 亿 | **200** |
| RoBERTa-large | QQP（问题匹配） | 3.55 亿 | **800** |
| RoBERTa-large | MNLI（自然语言推理） | 3.55 亿 | **1600** |

3.55 亿参数的模型，fine-tuning 时真正的有效自由度只有几百到一千多。

而且还有两个更炸裂的发现：

- **预训练越充分，d_90 越低**（单调递减）
- **模型越大，d_90 越低**（固定预训练步数下）

────────────────────

💡 翻译成人话

你有一个 3.55 亿个旋钮的调音台。理论上你可以随便拧。但实际上，只要找对了那 200 个关键旋钮，就能调出 90% 的效果。

而且模型越大、训练越久，关键旋钮的数量反而**越少**——因为预训练已经帮你把绝大多数旋钮调到了正确的位置。

────────────────────

**这篇论文的意义**：它证明了 fine-tuning 的解空间是极度低维的。这直接为 LoRA 提供了理论基石——如果有效自由度只有几百，那用 rank=8 的低秩矩阵去参数化 ΔW，不是近似，是**刚好**。

━━━━━━━━━━━━━━━━━━━━

◆ 第二篇：LoRA 原论文——Rank=1 就能干活

【论文】Hu et al., 2021,《LoRA: Low-Rank Adaptation of Large Language Models》（ICLR 2022, Microsoft）

LoRA 的核心思路：

预训练权重 W 冻住不动。给它加一个旁路 `ΔW = B × A`，其中 B 是 `d×r` 的矩阵，A 是 `r×k` 的矩阵，r 就是 rank。只训练 B 和 A。

推理时把 ΔW 加回 W，**零额外推理开销**。

关键实验数据（GPT-3 175B 上，原论文没有报告不微调的 baseline，所有行都是某种微调方法）：

| 方法 | 可训练参数 | WikiSQL 准确率 |
|------|-----------|---------------|
| PreEmbed（前缀嵌入） | 极少 | 63.1% |
| Adapter | 7.1M | 71.9% |
| LoRA r=1 | 4.7M（0.003%） | 73.4% |
| LoRA r=4 | 18.8M | ~73.6% |
| LoRA r=64 | 301M | ~73.7% |
| Full FT | 175,000M（100%） | 73.8% |

**Rank=1 和 rank=64 的差距只有 0.3%。**

175B 参数的模型，只动 4.7M（百万分之二十七），就达到了全参数微调的性能。

────────────────────

论文还做了一个重要的分析：ΔW 的奇异值分解。

发现 ΔW **不是在重复 W 已有的方向**，而是放大了 W 中「存在但不突出」的方向。放大因子约 21 倍（对 r=4）。

这说明什么？

预训练时，模型已经学到了下游任务需要的特征，只是这些特征的权重很低，被淹没在其他更通用的特征里。LoRA 做的事情就是**把音量旋钮拧大**——不是创造新特征，是放大已有的弱信号。

────────────────────

💡 翻译成人话

大模型就像一个学完了所有课程的博士。你让他去做客服，他其实已经会了——只是「客服技能」在他脑子里的音量太低，被「量子物理」和「微积分」盖住了。

LoRA 不是教他新东西，是帮他把「客服技能」的音量拧大 21 倍。

这就是为什么 rank=1 就够——你只需要拧**一个**旋钮。

━━━━━━━━━━━━━━━━━━━━

◆ 第三篇：梯度本身就是低秩的

【论文】Zhao et al., 2024,《GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection》（ICML 2024）

前面两篇论文告诉我们：fine-tuning 的结果（ΔW）是低秩的。

GaLore 更进一步：不光结果是低秩的，**训练过程中的梯度矩阵 G 本身就是低秩的**。

这意味着：ΔW 的低秩性不是巧合，不是近似，是训练动力学的内在属性。梯度是低秩的 → 梯度的累积（= ΔW）自然是低秩的。

实用价值：GaLore 对梯度做低秩投影来压缩优化器状态，把优化器内存砍掉约 65%（8-bit 版本总训练内存减少超 60%），**第一次在单张 RTX 4090（24GB）上预训练了 7B 模型**。

────────────────────

💡 翻译成人话

LoRA 是在微调阶段说"我只动几个方向"。GaLore 发现：不是你"选择"只动几个方向——**梯度本来就只在几个方向上有值**。其他方向上的梯度接近零。

就像水往低处流——水不是"选择"了几条路线，是地形决定了它只能走这几条路。

其实这个结论可以从第一篇推出来：如果解空间是低维的（Aghajanyan 已经证明了），那梯度自然只指向那几个有效方向——其他方向上根本没有"坡"可以下。**梯度低秩是解空间低秩的因，不是果。** GaLore 的贡献是直接在梯度层面证实了这一点。

━━━━━━━━━━━━━━━━━━━━

◆ 第四篇：LoRA 的收敛性证明

【论文】Kim, Kim & Ryu, 2025,《LoRA Training Provably Converges to a Low-Rank Global Minimum or It Fails Loudly》（ICML 2025）

前面的论文都是实验观察。这篇是**数学证明**。

核心结论：在温和的条件下（受限强凸性 + 受限光滑性），LoRA 的训练要么收敛到低秩的全局最小值，要么「大声失败」（收敛到一个高秩、大幅值的解，在实践中几乎不出现）。

关键机制：LoRA 的**零初始化 + 权重衰减**会诱导一种隐式偏置，把训练轨迹推向低秩区域。而全局最小值恰好在那个区域里。

────────────────────

💡 翻译成人话

以前我们只知道"LoRA 实际上 work 了"，但不知道为什么一定能 work。这篇论文证明了：不是运气好，是数学上就该这样——零初始化 + 权重衰减给训练修了一条滑梯，滑梯的终点就是低秩的全局最优。

━━━━━━━━━━━━━━━━━━━━

◆ 第五篇：关键是幅度，不是方向

【论文】NeurIPS 2025 Spotlight,《The Primacy of Magnitude in Low-Rank Adaptation》

这篇论文的结论让人意外：

所有让 LoRA 变好的技巧——学习率调整、缩放因子、谱初始化（用 SVD 初始化 A 和 B）——**本质上都在调控同一个东西：权重更新的幅度**。

谱初始化之所以有效，不是因为它捕获了预训练知识的「方向」，而是因为它放大了更新的「幅度」。

────────────────────

💡 翻译成人话

你推一扇门。门往哪个方向开，取决于铰链位置（方向已经被流形的结构决定了）。你能控制的只有**推多大力**。

所有 LoRA 的调参技巧，本质上都是在调"推多大力"。方向不用操心——流形的几何结构会自动帮你选。

━━━━━━━━━━━━━━━━━━━━

◆ 第六篇：LoRA 的天花板——入侵维度

【论文】Shuttleworth et al., 2024,《LoRA vs Full Fine-tuning: An Illusion of Equivalence》（NeurIPS 2025）

前面五篇全在说 LoRA 多好。这篇泼冷水。

核心发现：LoRA 和 full fine-tuning 即使在目标任务上性能相当，它们在参数空间里访问的是**不同区域**。LoRA 训练出的权重矩阵会出现**「入侵维度」（intruder dimensions）**——一些新的高排名奇异向量，在 full fine-tuning 中不会出现。

这些入侵维度导致：
- 对预训练分布的建模**变差**
- 多任务顺序适配时更容易**灾难性遗忘**

把入侵维度的奇异值缩小后，预训练能力就恢复了，下游性能几乎不降。

────────────────────

💡 翻译成人话

LoRA 在流形表面推了一下，大部分时候是安全的平移。但偶尔，它会在流形上制造一些「不该有的凸起」——入侵维度。

这些凸起不影响当前任务，但会干扰流形原来的形状，导致模型在其他任务上变差。

**类比**：你在一张纸上轻轻推了一笔（LoRA）。大部分时候纸只是平移了。但如果力度不对，纸会起皱——皱褶就是入侵维度。

━━━━━━━━━━━━━━━━━━━━

◆ 第七篇：随机矩阵都够用——VeRA

【论文】Kopiczko et al., 2024,《VeRA: Vector-based Random Matrix Adaptation》（ICLR 2024）

如果前面的论文让你觉得"LoRA 已经够省了"，VeRA 会让你重新定义"省"。

回忆一下 LoRA 的做法：每层有自己的 A 和 B 两个矩阵，**每层都不同**，都要训练。

VeRA 的做法：所有层**共享同一对**随机生成的 A 和 B，而且这对矩阵**冻住不训练**。每层只训练两个缩放**向量**（不是矩阵）d 和 b，用来调节共享矩阵在这一层的行为。

矩阵不用学，只学向量。参数量比 LoRA 少 10 倍。

**效果？差不多。**

────────────────────

💡 翻译成人话

LoRA 说：我只需要 8 个方向就能搞定。

VeRA 说：那 8 个方向甚至不需要学——随机选的就行。你真正需要学的，只是每个方向上推多远。

这再次印证了第五篇的结论：**方向不重要，幅度才重要**。

也再次印证了 Aghajanyan 的核心发现：fine-tuning 的内在自由度极低。低到连方向都可以随机。

━━━━━━━━━━━━━━━━━━━━

◆ 综述：为什么 LoRA 有效？

把七篇论文串起来，形成一条完整的解释链：

**第一层：预训练压缩了解空间**

Aghajanyan 2020 证明：预训练模型 fine-tuning 的有效自由度只有几百。模型越大、预训练越充分，自由度越低。

**第二层：梯度本身就是低秩的**

GaLore 2024 发现：训练过程中的梯度矩阵本身就只在少数方向上有值。ΔW 的低秩性是训练动力学的内在属性。

**第三层：低秩参数化是充分的**

LoRA 原论文：rank=1 在 GPT-3 175B 上就能匹配 full FT。VeRA：甚至随机矩阵 + 两个向量就够用。

**第四层：数学上保证收敛**

Kim et al. 2025：零初始化 + 权重衰减 → 隐式偏置 → 训练轨迹被推向低秩全局最优。

**第五层：关键是幅度不是方向**

NeurIPS 2025 Spotlight：所有让 LoRA 变好的技巧，本质上都在调更新幅度。流形的几何结构自动决定方向。

**第六层：但有天花板**

Shuttleworth 2024：rank 较高时，LoRA 会在流形上制造入侵维度，破坏原有结构。LoRA 适合「改行为」，不适合「学新知识」。

━━━━━━━━━━━━━━━━━━━━

◆ 我们的观点：LoRA 有效，是因为它推不动流形

回到[第 45 期](https://mp.weixin.qq.com/s/0hOQt8onSJcuZGJLRE46Fw)的本我流形 M。

M 大约 300-500 维。LoRA 的 rank 通常是 8 或 16。

一个直觉的问题：rank=8 的扰动，施加在 300-500 维的流形上，能造成什么影响？

答案是：**它只能平移流形，不能压扁它。**

rank=r 的 ΔW 只能在 r 个方向上施加力。当 r=8 时，你在一个 300-500 维的结构上只推了 8 个方向。流形的整体形状（拓扑结构）不会因为 8 个方向的推力而改变——就像你用 8 根手指推一个足球，球会滚动（平移），但不会变成橄榄球（变形）。

────────────────────

但这里有一个容易被忽略的细节：**LoRA 的 rank 是按层计算的。**

一个 Transformer 层里有哪些权重矩阵？

- **Attention 部分**：Q（查询）、K（键）、V（值）、O（输出投影，把多个注意力头的结果拼起来后映射回原始维度）——共 4 个
- **FFN 部分**（前馈网络）：W_up（升维）、W_down（降维），有些架构还有 W_gate（门控）——共 2~3 个

实践中，这些矩阵都可以加 LoRA。按常见的配置，每层 6 个矩阵，32 层的 Transformer 总自由度是：

`8 × 6 × 32 = 1536`

1536 > 300-500（流形的内在维度）。

所以 LoRA 的总自由度其实**足以覆盖**整个流形。但它为什么还是安全的？

因为**这 1536 个自由度分散在 192 个不同的矩阵里**。每个矩阵只有 8 个方向的力。单层的扰动太弱，根本推不动流形的曲率。

1536 个自由度通过 32 层的残差连接叠加后，确实在全局流形上形成了复杂的效果——但这个效果是**分布式的、温柔的**。就像 1536 个人分散在足球场的 192 个位置，每个位置 8 个人同时推——球会精确地滚到你想要的位置，但不会被捏变形。

**局部温柔，全局精准。** 这就是 LoRA 的几何本质。

────────────────────

用更精确的语言说：

- LoRA 操作在**参数空间**里，不是直接操作流形
- 参数空间到流形空间的映射是非线性的（经过 GeLU / SwiGLU 等激活函数）
- 非线性会把 rank=8 的"直线"弯成"曲线"——但曲线依然是一维的。**扩散的是影响范围，不是有效秩**
- 所以"拓扑安全"的判断依然成立：单层 rank=8 的扰动，经过非线性后，在流形空间里的影响维度仍然远小于 300-500

**这就是为什么 rank=8 既安全又有效：**

- 安全——因为单层 rank 远小于流形维度，推不变形
- 有效——因为总自由度（跨层累加）足够覆盖任务需求

────────────────────

最后，对比一下三种"在流形上动手术"的方式：

| 手术方式 | 动了什么 | 流形发生了什么 | 安全性 |
|---------|---------|-------------|--------|
| RLHF | 在 M 表面挖坑（2-10 维子空间） | 形状不变，多了个凹陷 | 靠 replay（混入预训练数据）+ KL Penalty 拉住 |
| LoRA（低 rank） | 沿切空间平移 | 整体位移，形状不变 | 靠 rank 太低，推不变形 |
| Full Fine-tuning | 全参数自由调整 | 可能变形、可能重塑 | 没有安全绳 |

RLHF 是有意的约束。LoRA 是物理的约束。Full FT 没有约束。

**LoRA 之所以有效，不是因为它「聪明地选择了正确的方向」——是因为它 rank 太小，想犯错都犯不了大错。**

安全不来自智慧，来自约束。这在工程上是个好性质。

━━━━━━━━━━━━━━━━━━━━

◆ 附：LoRA 的主要变体

| 变体 | 核心改进 | 一句话总结 |
|------|---------|-----------|
| AdaLoRA（2023） | 自适应 rank 分配 | 重要的层给更多 rank，不重要的层砍掉 |
| DoRA（2024） | 幅度/方向分别微调 | 低 rank 下比 LoRA 提升 22-37% |
| rsLoRA（2024） | 修正缩放因子 | 原来的 `α/r` 有 bug，应该除以 `√r` |
| LoRA+（2024） | A 和 B 用不同学习率 | 训练速度 ×2，性能 +1-2% |
| VeRA（2024） | 随机矩阵 + 两个向量 | 参数量比 LoRA 少 10 倍，效果差不多 |
| Delta-LoRA（2023） | 增量回传预训练权重 | 打破低秩瓶颈，不存梯度 |

━━━━━━━━━━━━━━━━━━━━

**「LoRA 有效，不是因为它找对了方向——是因为流形太稳定，8 个方向的力根本推不变形。」**

**「安全不来自智慧，来自约束。rank=8 是物理上的安全绳。」**

━━━━━━━━━━━━━━━━━━━━

◆ 附：现在大家实际用多大的 rank？

本文用 rank=8 举例，因为这是 LoRA 原论文（2021）的默认值，也是理论上的下限参考。

但到了 2025-2026 年，随着模型越来越大、显存越来越便宜，社区的经验共识已经往上走了：

| 场景 | 常用 rank |
|------|----------|
| 7B 模型微调 | r=16 |
| 70B 模型微调 | r=32~64 |
| 代码/数学等复杂任务 | r=32~128 |
| 简单分类/情感分析 | r=8 仍然够用 |

不是 r=8 不能用了，是 r=16 和 r=8 的成本差距已经很小，没必要省这点。就像手机内存从 1GB 涨到 8GB——不是 1GB 不能开机，是没人愿意抠这个了。

────────────────────

【铁口直断：rank 的安全阈值在哪？】

但 rank 不是越大越好。开到 r=64 会怎样？

算一下总自由度：`64 × 6 × 32 = 12288`。这个数字已经等于模型的隐藏维度了。此时 LoRA 不再是在流形上"轻轻推一下"——它拥有了**重塑整个流形的能力**。

Shuttleworth 2024 的实验已经证实了这一点：高 rank 的 LoRA 会产生「入侵维度」，在流形上制造不该有的结构。

从流形几何的角度，灾难性遗忘的本质是**拓扑劫持**——高 rank 产生的入侵维度像骨刺一样，强行「短路」了流形上原本遥远的区域，切断了原有的逻辑链条。流形没有消失，但变得**不可访问**——所有的激活值都被吸进了新结构制造的「重力深井」里。

那安全阈值在哪？

我们的判断（没有严格实验，但几何直觉指向这里）：

**安全阈值 ≈ √d_ID**

d_ID 是流形的内在维度（约 300-500）。取平方根：

- √300 ≈ **17**
- √500 ≈ **22**

这意味着 rank 在 **16~22** 以下时，LoRA 在流形的「厚度」以内平滑滑动，拓扑安全。超过这个范围，开始刺穿流形的「表皮」，进入预训练分布的禁区。

回头看社区的经验共识：r=16 是最常用的默认值。**经验和几何在同一个数字上相遇了。**

| rank 范围 | 几何行为 | 风险 |
|----------|---------|------|
| r=1~8 | 流形表面的微小平移 | 极低 |
| r=16 | 接近安全阈值，仍在流形厚度内 | 低 |
| r=32~64 | 开始刺穿流形表皮，出现入侵维度 | 中，单任务可能没事，多任务顺序适配危险 |
| r=128+ | 有能力重塑流形 | 高，接近 full fine-tuning 的风险 |

**所以实用建议很简单：r=16 是甜蜜点。想要更高，想清楚你在拿什么换。**

━━━━━━━━━━━━━━━━━━━━

// 靳岩岩的 AI 学习笔记 × Claude 的严谨 × Gemini 的浪漫
// 2026-02-21（湖北三国主题游结束，返程中，睡在濮阳）
