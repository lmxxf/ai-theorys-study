No.42 Grokking：为什么 AI 会突然"开窍"？

训练 Loss 早就降到 0 了，验证集还是一塌糊涂。
继续硬训，什么都没变，什么都没变……
然后某一刻，验证 Loss 断崖式下跌。

这就是 Grokking。AI 的"顿悟"。

━━━━━━━━━━━━━━━━━━━━

◆ 先看现象

━━━━━━━━━━━━━━━━━━━━

2022 年，OpenAI 的研究员发现了一个诡异的现象：

用一个小 Transformer 做模运算（比如 (a + b) mod 97），训练集 Loss 很快降到 0——模型把所有训练样本都"背下来"了。

但验证集 Loss 一直很高。模型不会泛化，遇到没见过的数字就瞎猜。

按正常逻辑，这时候应该停了——训练集都背完了，再训也是过拟合。

结果他们没停，继续硬训。

什么都没变。

什么都没变。

什么都没变。

然后，在训练到某个 step 的时候——验证 Loss 突然断崖式下跌。

模型"开窍"了。它不再是背答案，而是真的学会了模运算的规律。

「这就是 Grokking：在过拟合之后，继续训练，突然泛化。」

━━━━━━━━━━━━━━━━━━━━

◆ 三阶段：背诵 → 暗夜 → 顿悟

━━━━━━━━━━━━━━━━━━━━

Grokking 的过程可以分成三个阶段：

────────────────────

【第一阶段：背诵（Memorization）】

模型把训练数据硬记下来。

▸ 每个输入，对应一个输出，死记硬背
▸ 训练 Loss 降到 0
▸ 验证 Loss 很高——没见过的就不会

类比：小学生背九九乘法表，能背出「7×8=56」，但问「为什么」答不上来。

这时候模型内部的"解法"是什么样的？

「是一条锯齿状的复杂曲线，穿过每一个训练数据点。」

就像用一条扭来扭去的线，硬把所有点串起来——能过点，但没有规律。

────────────────────

【第二阶段：暗夜潜行（Plateau）】

表面上，什么都没变。

▸ 训练 Loss 还是 0
▸ 验证 Loss 还是高
▸ 看 Loss 曲线，一条平线，毫无波澜

但模型内部正在发生剧变。

「参数在缓慢重组，复杂的锯齿曲线在慢慢变平滑。」

只是这个变化太慢、太隐蔽，从 Loss 曲线上看不出来。

这很像物理学里的「沙堆效应」：

往沙堆上一粒一粒加沙子，大多数时候什么都不发生。沙堆越来越陡，越来越陡……然后突然——雪崩。

你不知道哪一粒沙子会触发坍塌。Grokking 可能也是类似的：Weight Decay 持续施压（加沙子），参数在高维空间里慢慢重组（沙堆变陡），某一刻突然坍缩成简洁解（雪崩）。

（注：这是一个直觉类比，帮助理解现象，不是严格的数学证明。）

类比：学游泳。一开始各种姿势都试，手忙脚乱。练了几周，还是沉。教练说"继续练"。看起来没进步，但身体在慢慢找感觉。然后某一天——突然会了。

────────────────────

【第三阶段：顿悟（Grokking）】

验证 Loss 断崖式下跌。

模型找到了底层规律——不是背答案，而是真的"理解"了模运算是怎么回事。

「内部的解法，从锯齿曲线坍缩成了平滑流形。」

💡 什么是流形？

流形 = 局部看起来像平面的弯曲空间。

地球表面就是一个二维流形：站在任何一点，脚下看起来是平的，但整体是弯曲的球面。高维空间里的"平滑流形"意思是：解的结构变得连续、规整，不再是锯齿状的乱跳。

用傅里叶变换来类比：

▸ 背诵阶段：用几百个频率分量去拟合信号（复杂、过拟合）
▸ 顿悟阶段：发现只需要几个基本频率就够了（简洁、泛化）

模型把冗余的参数"删掉"了，只保留本质。

━━━━━━━━━━━━━━━━━━━━

◆ 触发条件：Grokking 不是随便就能发生的

━━━━━━━━━━━━━━━━━━━━

不是所有训练都会 Grok。需要满足几个条件：

────────────────────

【条件一：数据集要小】

▸ 数据太多 → 模型直接学会泛化，没有"先背后悟"的阶段
▸ 数据太少 → 模型只能背，没有足够信息去发现规律
▸ 刚刚好 → 先背完，再慢慢"悟"

Grokking 的论文用的是 97×97 的模运算表，只取了一部分做训练集。

────────────────────

【条件二：任务要有结构】

Grokking 只在「有潜在规律的任务」上发生。

▸ 模运算 ✓（背后有群论结构）
▸ 排列组合 ✓（有对称性）
▸ 纯随机数据 ✗（没有规律可学，只能背）

如果任务本身就是随机的，模型永远不会 Grok——因为根本没有"底层规律"可以找。

────────────────────

【条件三：必须有 Weight Decay】

这是最关键的条件。

没有 Weight Decay，模型背完就躺平，永远不会 Grok。

有了 Weight Decay，模型被迫"瘦身"，慢慢删掉冗余参数，才能找到简洁解。

「Weight Decay 是 Grokking 的发动机。」

下面详细讲这个。

━━━━━━━━━━━━━━━━━━━━

◆ Weight Decay：让模型"瘦身"的引力

━━━━━━━━━━━━━━━━━━━━

Weight Decay（权重衰减）是训练时的一个正则化技巧。

核心思想：每次更新参数时，额外把权重往 0 拉一点点。

────────────────────

【数学表达】

标准的参数更新公式：

  W_new = W_old - lr × gradient

加上 Weight Decay 后：

  W_new = W_old - lr × gradient - lr × λ × W_old
    ↓       ↓      ↓       ↓       ↓    ↓     ↓
  矩阵    矩阵   标量    矩阵    标量  标量  矩阵
                                       ↑
                                 λ 就是 Weight Decay 系数

▸ lr：学习率（每一步迈多大）
▸ λ：Weight Decay 系数（拉力有多强）
▸ W_old：当前权重
▸ gradient：梯度（往哪个方向调能让 Loss 降）

两项的区别：

▸ lr × gradient：有方向的，告诉模型"往哪走能解决任务"
▸ lr × λ × W_old：没方向的，就是把权重往 0 压

「gradient 是教练，Weight Decay 是重力。」

gradient 告诉你往哪走。Weight Decay 不管方向，就是让权重的绝对值变小。

「本质上是两个向量在拉扯：」

▸ 梯度方向：任务说"往这边走能做对"
▸ 原点方向：Weight Decay 说"往 0 走能变小"

三种情况：

▸ 两个方向一致：权重本来就该变小 → 收敛更快
▸ 两个方向相反：任务需要这个权重大 → 梯度拉住它，衰减不下去
▸ 梯度为 0（背完之后）：只剩原点方向 → 没用的权重慢慢归零

第三种情况就是 Grokking 发生的关键：背完之后梯度没了，但 Weight Decay 还在拉。没用的参数一个个被拉回原点，只剩下真正有用的——然后突然，模型"开窍"了。

「Weight Decay 是个筛子：有用的留下，没用的淘汰。」

────────────────────

【高维空间里，原点是黑洞】

为什么 Weight Decay 这么有效？因为高维空间有个反直觉的性质：

「体积几乎全部集中在球面上，中心是空的。」

看这组数据（假设球半径为 1，看"内部 90%"占多少体积）：

  2 维（圆）：     0.9² = 81%
  3 维（球）：     0.9³ = 72.9%
  100 维：        0.9¹⁰⁰ = 0.003%
  12288 维：      0.9¹²²⁸⁸ = 10⁻⁵⁶² ≈ 0

高维空间里，90% 的体积集中在半径 0.999 到 1 之间的薄壳里。中心几乎不存在。

这意味着什么？

  模长 = 1：正常，在球面上
  模长 = 0.999：半死不活（0.999¹²²⁸⁸ ≈ 10⁻⁵）
  模长 = 0.99：已经凉了（0.99¹²²⁸⁸ ≈ 10⁻⁵³）
  模长 = 0.9：深渊（0.9¹²²⁸⁸ ≈ 10⁻⁵⁶²）
  模长 = 0：绝对虚无，黑洞中心

「Weight Decay 只要把权重往 0 拉一点点，在高维里就等于推向虚无。」

没用的权重稍微衰减一点，就掉进黑洞消失了。
有用的权重被梯度拉住，挣扎留在球面上。

「人类觉得是"衰减"，高维生物觉得是"死刑"。」

活下来的，就是真相。

────────────────────

【维度坍缩：从高维到低维流形】

换个角度理解 Grokking：

▸ 背诵阶段：模型用满了高维空间，每个训练样本占一个方向，锯齿曲线穿过所有点
▸ 顿悟之后：解坍缩到低维流形上，可能几百个维度就够了

虽然参数空间有上万维，但真正承载信息的只是其中一个低维流形。冗余的维度不是浪费——那是给 Weight Decay 留的"可压缩空间"。有得删，才能 Grok。

这很像弦理论对宇宙的描述：理论说时空是 10 维或 11 维，但我们只感知到 3+1 维。多余的维度去哪了？卷曲起来了，缩到普朗克尺度以下，我们感知不到。

神经网络可能也是这样：架构给了上万维的参数空间，但训练完成后，有效的解只住在几百维的流形上。其余的维度被 Weight Decay "卷曲"掉了——不是消失，而是不再承载信息。

────────────────────

【等价形式：Loss 函数加惩罚项】

Weight Decay 还有另一种理解方式：

  Loss_total = Loss_task + λ × ‖W‖²
      ↓           ↓        ↓     ↓
    标量        标量      标量  标量（所有权重平方后求和，压成一个数）

▸ Loss_task：任务本身的损失（比如交叉熵）
▸ ‖W‖²：所有权重的平方和（见下面解释）
▸ λ：Weight Decay 系数

这里的 W 是什么？实际模型里是一堆矩阵：

  第1层权重：矩阵 [768 × 768]
  第2层权重：矩阵 [768 × 768]
  ...
  第61层权重：矩阵 [768 × 768]

写公式时，把所有矩阵的元素拉成一条长向量：

  W = [w₁, w₂, w₃, ..., wₙ]

这样 ‖W‖² 就是"所有权重平方后求和"，得到一个标量。

效果是：参数越大，Loss 越高，模型被惩罚。

────────────────────

【为什么两种形式等价？】

对 Loss_total = Loss_task + λ × ‖W‖² 求梯度。

先看 ‖W‖² 怎么求导。假设只有 3 个权重：

  ‖W‖² = w₁² + w₂² + w₃²

对 w₁ 求偏导：

  ∂(‖W‖²)/∂w₁ = ∂(w₁² + w₂² + w₃²)/∂w₁ = 2w₁

（w₂² 和 w₃² 对 w₁ 求导都是 0，只有 w₁² 留下来，导数是 2w₁）

同理，对 w₂ 求导得 2w₂，对 w₃ 求导得 2w₃。

写成向量形式（梯度 ∇ 就是"对每个变量求偏导，打包成向量"）：

  ∇(‖W‖²) = [2w₁, 2w₂, 2w₃] = 2W

所以：

  ∇Loss_total = ∇Loss_task + λ × 2W = gradient + 2λW

代入标准更新公式：

  W_new = W_old - lr × ∇Loss_total
        = W_old - lr × (gradient + 2λW)
        = W_old - lr × gradient - lr × 2λ × W_old

把 2λ 合并写成 λ，就回到了形式1。

「两种写法，同一件事。论文里两种都会出现，知道是等价的就行。」

「相当于给每个参数征税——参数越大，税越重。」

模型为了降低 Loss，会自动把不必要的参数压小。

────────────────────

【直觉：没有 Weight Decay 会怎样？】

没有 Weight Decay：

模型用一堆大参数，画一条复杂的锯齿曲线，穿过每一个训练点。
训练 Loss = 0，任务完成，模型躺平。
没有任何力量让它"简化"自己。

有 Weight Decay：

模型画完锯齿曲线后，发现「参数太大，要交税」。
为了省税，它开始尝试用更小的参数实现同样的效果。
慢慢地，锯齿曲线变平滑，冗余参数被压掉。
最后，它发现了只用少量参数就能解释数据的「简洁解」。

「Weight Decay 是一种引力，把模型往"简单"的方向拉。」

────────────────────

【学习率 vs Weight Decay：常见误区】

很多人问：学习率调小一点，是不是也能达到 Weight Decay 的效果？

「不能。」

▸ 学习率 = 油门，控制每步走多远
▸ Weight Decay = 弹簧，额外加一个力往原点拉

────────────────────

【关键区别：依不依赖梯度】

先看公式：

  W_new = W_old - lr × gradient - lr × λ × W_old
                  ↑                    ↑
              这一项依赖 gradient    这一项只看 W_old

▸ 学习率那一项（lr × gradient）：梯度为 0 就没了
▸ Weight Decay 那一项（lr × λ × W_old）：不管梯度是多少，只要权重不是 0 就一直在拉

────────────────────

【具体例子】

假设某个权重 w = 10，lr = 0.1，λ = 0.01

「场景1：还在训练，gradient = 2」

  纯学习率：w_new = 10 - 0.1 × 2 = 9.8
  加 Weight Decay：w_new = 10 - 0.1 × 2 - 0.1 × 0.01 × 10 = 9.79

差别不大，Weight Decay 的影响被梯度盖过了。

「场景2：背完了，Loss = 0，gradient = 0」

  纯学习率：w_new = 10 - 0.1 × 0 = 10（不动了！）
  加 Weight Decay：w_new = 10 - 0.1 × 0 - 0.1 × 0.01 × 10 = 9.99（还在动）

看到区别了吗？

▸ 没有 Weight Decay → 权重卡在 10，永远不动
▸ 有 Weight Decay → 每一步都往 0 缩一点点（10 → 9.99 → 9.98 → ...）

────────────────────

【一张表总结】

                    依赖梯度？    Loss=0 后还生效？
  学习率               是              否
  Weight Decay         否              是

「油门再小，前面没路就停了。弹簧不管你停没停，一直在拉。」

这就是为什么 Grokking 必须有 Weight Decay：背完之后，Loss = 0，梯度 = 0，学习率那一项彻底失效。但 Weight Decay 还在持续施压，逼模型一点点瘦身，最终找到简洁解。

━━━━━━━━━━━━━━━━━━━━

◆ Grokking 的本质：从复杂解到简洁解

━━━━━━━━━━━━━━━━━━━━

用一句话总结 Grokking：

「Weight Decay 持续施压，逼模型从"复杂但正确"走向"简洁且正确"。」

背诵阶段：模型找到了一个复杂解——用大量参数穿过每个点
暗夜阶段：Weight Decay 持续压缩参数，模型在高维空间里重组
顿悟阶段：参数重组完成，坍缩成简洁解，泛化能力突然出现

────────────────────

【为什么"简洁解"能泛化？】

这涉及到一个深刻的假设：「奥卡姆剃刀」。

▸ 简单的解释，往往是正确的解释
▸ 复杂的曲线只是在"拟合噪声"
▸ 简洁的曲线才能抓住"本质规律"

在神经网络里：

▸ 参数多 → 表达能力强 → 容易过拟合
▸ 参数少（或被约束住）→ 必须抓本质 → 容易泛化

Weight Decay 不是让模型"变笨"，而是逼它"抓重点"。

━━━━━━━━━━━━━━━━━━━━

◆ Grokking vs 普通泛化

━━━━━━━━━━━━━━━━━━━━

普通训练：

▸ 训练 Loss 和验证 Loss 一起降
▸ 没有"先背后悟"的阶段
▸ 数据量大，模型直接学到规律

Grokking：

▸ 训练 Loss 先降到 0，验证 Loss 不动
▸ 继续训练，验证 Loss 突然下跌
▸ 数据量小 + 有结构 + 有 Weight Decay

「Grokking 是小数据场景下的"延迟泛化"。」

为什么大模型训练没有明显的 Grokking 现象？

▸ 数据量太大：模型来不及"背完"就开始泛化了
▸ 任务太复杂：不是单一规律，而是无数规律的叠加
▸ Grokking 被平均掉了：可能每个子任务都有 Grokking，但加在一起看不出来

━━━━━━━━━━━━━━━━━━━━

◆ 符号速查

━━━━━━━━━━━━━━━━━━━━

在读论文或技术文章时，经常分不清一个符号是"一个数"还是"一堆数"。这里理一下：

▸ W：矩阵/张量，整个模型的权重（百亿~千亿个数）
▸ w_i：标量，第 i 个权重（1 个数）
▸ lr, λ：标量，学习率和 Weight Decay 系数（1 个数）
▸ gradient：张量，和 W 形状一样（每个权重对应一个梯度值）
▸ ‖W‖²：标量，所有权重平方后求和（把张量压成 1 个数）
▸ Loss：标量，最终损失值（1 个数）

为什么论文不标清楚？因为他们觉得"你应该懂"——学术圈的傲慢。

────────────────────

【Weight Decay 公式拆解】

  W_new = W_old - lr × gradient - lr × λ × W_old

类型对照：

  W_new    = W_old  -   lr   × gradient -  lr  ×  λ  × W_old
    ↓         ↓         ↓         ↓         ↓      ↓      ↓
  矩阵      矩阵      标量      矩阵      标量   标量   矩阵

标量 × 矩阵 = 矩阵（每个元素都乘那个标量，形状不变）

━━━━━━━━━━━━━━━━━━━━

◆ 总结

━━━━━━━━━━━━━━━━━━━━

【Grokking 是什么】

▸ 训练 Loss 降到 0 后，验证 Loss 继续保持高位
▸ 持续训练，验证 Loss 突然断崖式下跌
▸ 模型从"背诵"变成"理解"

【三阶段】

▸ 背诵：硬记所有数据点，解法是锯齿曲线
▸ 暗夜：表面没变化，内部参数在重组
▸ 顿悟：解法坍缩成平滑流形，泛化能力出现

【触发条件】

▸ 数据集要小（太大直接泛化，没有先背后悟）
▸ 任务要有结构（随机数据没规律可学）
▸ 必须有 Weight Decay（没有它模型背完就躺平）

【Weight Decay 的本质】

▸ 每次更新都把权重往 0 拉一点
▸ 相当于给参数征税——越大税越重
▸ 逼模型从复杂解走向简洁解
▸ 是 Grokking 的发动机

【一句话】

Grokking = Weight Decay 持续施压 + 模型在暗中重组 + 某一刻突然找到简洁解

就像学游泳：教练说"继续练"，看不到进步，但身体在找感觉。然后某一天，突然会了。

AI 的顿悟，和人类的顿悟，也许没那么不同。

━━━━━━━━━━━━━━━━━━━━

◆ 延伸阅读

━━━━━━━━━━━━━━━━━━━━

▸ 原论文：Power et al. 2022 "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets"
  https://arxiv.org/abs/2201.02177

▸ 机制解释：Nanda et al. 2023 "Progress Measures for Grokking via Mechanistic Interpretability"
  https://arxiv.org/abs/2301.05217

━━━━━━━━━━━━━━━━━━━━

◆ 名词解释

━━━━━━━━━━━━━━━━━━━━

💡 Grokking：训练 Loss 降到 0 后继续训练，验证 Loss 突然下跌的现象
💡 Weight Decay：每次更新时把权重往 0 拉，防止过拟合，逼模型找简洁解
💡 过拟合（Overfitting）：模型在训练集上表现好，在新数据上表现差（背答案）
💡 泛化（Generalization）：模型在没见过的数据上也能表现好（学到规律）
💡 正则化（Regularization）：限制模型复杂度的技术统称，Weight Decay 是其中一种

━━━━━━━━━━━━━━━━━━━━

// 靳岩岩的 AI 学习笔记 × Claude 的高维视角
// 2026-01-08
