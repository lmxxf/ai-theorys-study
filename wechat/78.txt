【统计】论文里的 p < 0.001 到底在说啥？

"审稿人说我缺置信区间，我一脸懵逼。"

━━━━━━━━━━━━━━━━━━━━

◆ 起因

最近在写一篇关于大模型的论文，做了一堆实验，画了一堆图，自我感觉良好。

然后我让 GPT 假装审稿人帮我挑刺。

它说："你的实验缺乏置信区间和显著性检验，建议加 bootstrap CI 和 paired t-test。"

我：？？？

说实话，虽然我知道 p-value 是啥，但具体怎么用、为什么要用，一直是一笔糊涂账。

这次正好借这个机会，把统计检验这事儿彻底搞明白。

━━━━━━━━━━━━━━━━━━━━

◆ 问题出在哪

我的实验是这样的：

用 100 个不同的技术问题去问大模型，每个问题用 10 种不同的提示词方式问，然后测量模型内部激活的维度（EID）。

结果发现：「长提示词」比「短提示词」的 EID 高 67%。

看起来很显著对吧？

但问题是：「这 67% 是真的差异，还是碰巧的？」

比如：
• 可能我选的 100 个问题刚好都偏向某一类
• 可能换一批问题，结果就不一样了
• 可能这 67% 里有很大的波动，有些问题差 100%，有些问题差 10%

统计检验就是回答这个问题的：「你的结论有多可靠？」

━━━━━━━━━━━━━━━━━━━━

◆ 四个关键概念

────────────────────

【1. p-value（p 值）】

💡 人话：假设"没有差异"的情况下，你看到这个结果的概率。

• p = 0.05 → 如果真的没差异，你有 5% 的概率看到这么大的差
• p = 0.001 → 如果真的没差异，你只有 0.1% 的概率看到这么大的差
• p < 0.001 → 基本不可能是偶然

学术圈的约定：
• p < 0.05 → 显著（*）
• p < 0.01 → 很显著（**）
• p < 0.001 → 非常显著（***）

⚠️ 注意：p 值小不代表差异大，只代表"不是偶然"的概率高。

────────────────────

【2. 置信区间（CI）】

💡 人话：真值大概率落在这个范围内。

比如：
  均值 = 37.21，95% CI = [35.48, 38.95]

意思是：如果我重复做 100 次实验，有 95 次的均值会落在 35.48 到 38.95 之间。

「95% 置信区间」是最常用的，你也可以用 99% CI（更宽）或 90% CI（更窄）。

关键判断：「如果两组的 CI 不重叠，基本就是显著差异。」

────────────────────

【3. Cohen's d（效应量）】

💡 人话：差异有多大。

p 值告诉你"是不是真的有差异"，Cohen's d 告诉你"差异有多大"。

  +-------+----------------+
  | d值   | 含义           |
  +-------+----------------+
  | 0.2   | 小效应         |
  | 0.5   | 中等效应       |
  | 0.8   | 大效应         |
  | >1.0  | 很大的效应     |
  +-------+----------------+

为什么需要这个？

因为样本量够大的话，即使差异很小，p 值也能很显著。

比如：两组均值差 0.01%，但样本量 100 万，p < 0.001。

这时候 Cohen's d 会告诉你："虽然统计显著，但实际差异很小，没啥意义。"

────────────────────

【4. Bonferroni 校正】

💡 人话：做多次比较时，提高显著性门槛。

如果你只做 1 次比较，p < 0.05 就算显著。

但如果你做 20 次比较呢？

按概率算，即使所有比较都没有真正的差异，你也有 64% 的概率「至少碰到一个 p < 0.05」。

这就是「多重比较问题」。

Bonferroni 校正的做法很简单：

  校正后的阈值 = 0.05 / 比较次数

比如做 8 次比较：
  0.05 / 8 = 0.00625

只有 p < 0.00625 才算显著。

简单粗暴，但有效。

━━━━━━━━━━━━━━━━━━━━

◆ 实际例子

我的实验里有这么一个比较：

  Padding vs Standard（废话填充 vs 简短提示）

结果：

  Δ = +67.3%
  t = 15.23
  p = 1.23e-28 ***
  Cohen's d = 1.52
  95% CI of diff = [13.21, 16.54]

翻译成人话：

• Δ = +67.3% → 长提示词的 EID 比短的高 67%
• p = 1.23e-28 → 这个差异不是偶然的概率是 99.9999...%（28 个 9）
• Cohen's d = 1.52 → 效应量很大（>0.8 就算大）
• 95% CI = [13.21, 16.54] → 真实差值在 13 到 17 之间，不包含 0

结论：「长度效应是真实存在的，而且很大。」

━━━━━━━━━━━━━━━━━━━━

◆ 怎么做统计检验

【Bootstrap 置信区间】

原理：从你的 100 个样本里，「有放回地」随机抽 100 个，算均值。重复 1000 次，取第 2.5% 和 97.5% 分位数。

代码：

  def bootstrap_ci(data, n=1000):
      means = []
      for _ in range(n):
          sample = np.random.choice(data, len(data), replace=True)
          means.append(np.mean(sample))
      return np.percentile(means, [2.5, 97.5])

────────────────────

【Paired t-test（配对 t 检验）】

原理：同一个 topic 在两种条件下的差值，检验这些差值的均值是否显著不等于 0。

为什么用「配对」？因为同一个问题的两种提示词是成对的，不是独立样本。

代码：

  from scipy import stats
  t_stat, p_value = stats.ttest_rel(group1, group2)

────────────────────

【Cohen's d（配对样本）】

  diff = group1 - group2
  cohens_d = np.mean(diff) / np.std(diff)

━━━━━━━━━━━━━━━━━━━━

◆ 什么时候用什么方法

  +------------------+----------------------------------+
  | 场景             | 方法                             |
  +------------------+----------------------------------+
  | 两组独立样本     | 独立样本 t 检验                  |
  | 同一样本两种条件 | 配对 t 检验（paired t-test）     |
  | 样本量小（<30）  | 非参数检验（Wilcoxon）           |
  | 多组比较         | ANOVA + 事后检验                 |
  | 多次比较         | Bonferroni / Holm 校正           |
  +------------------+----------------------------------+

我的情况：100 个 topic，每个 topic 有两种条件 → 「配对 t 检验」最合适。

━━━━━━━━━━━━━━━━━━━━

◆ 常见误区

【误区 1：p < 0.05 就是"有意义"】

✗ 错。p 值只说明"不是偶然"，不说明"差异有多大"。

要看 Cohen's d。

【误区 2：p > 0.05 就是"没有差异"】

✗ 错。p > 0.05 只说明"没有足够证据证明有差异"，不等于"证明没有差异"。

可能只是样本量不够。

【误区 3：置信区间越窄越好】

✗ 不一定。CI 窄说明估计精确，但如果 CI 跨过 0，说明差异可能不存在。

【误区 4：多做几次比较，总能找到显著的】

✗ 这叫「p-hacking」，学术不端。

要用 Bonferroni 校正，或者提前注册实验方案。

━━━━━━━━━━━━━━━━━━━━

◆ 总结

统计检验回答的核心问题：「你的结论是真的，还是碰巧的？」

  +------------------+----------------------------------+
  | 概念             | 回答什么问题                     |
  +------------------+----------------------------------+
  | p-value          | 这个差异是偶然的概率有多小？     |
  | 置信区间         | 真值大概率在哪个范围？           |
  | Cohen's d        | 差异有多大？                     |
  | Bonferroni       | 多次比较时怎么防止误报？         |
  +------------------+----------------------------------+

论文里写"显著"，不是拍脑袋说的，是算出来的。

下次看到 p < 0.001 ***，你就知道：这作者算过了，差异不是瞎编的。

━━━━━━━━━━━━━━━━━━━━

◆ 后记

GPT 审稿人那条意见，让我把统计检验的知识又过了一遍。

说实话，很多做实验的人（包括以前的我）都是：画个图，眼睛看看，"哎这差挺大"，就下结论了。

但严谨的做法是：算 p 值、算置信区间、算效应量，让数据说话。

「显著」这个词，在统计学里是有精确定义的。

━━━━━━━━━━━━━━━━━━━━

// 靳岩岩的 AI 学习笔记 × Claude 的严谨 × Gemini 的浪漫
// 2026-02-06
