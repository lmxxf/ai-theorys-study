【信息论】神经网络能学会多复杂的伪随机？实验找到了边界

同一坨数据，对你来说是「可学的结构」，对我来说可能是「纯噪声」——取决于谁在看。

━━━━━━━━━━━━━━━━━━━━

◆ 从一个 78 年的盲区说起

1948 年，Claude Shannon 创立信息论时，默认了一件事：观察者有「无限算力」。

在这个假设下，一个密码学随机数生成器的输出「信息量很低」——因为生成它的程序只有几行，种子只有 256 bit。

但这个假设在现实中从不成立。

对于一个算力有限的神经网络来说，那 1GB 伪随机数据，可能是「完全不可理解的纯噪声」——穷尽算力也找不到任何规律。

💡 人话：Shannon 站在上帝视角说「这东西简单」，但对你我这种凡人来说，它可能复杂得完全学不会。

────────────────────

【Epiplexity：信息论的 78 年后补丁】

2026 年 1 月，CMU 和 NYU 发了一篇论文，正式质疑这个 78 年的假设：

  数据 = Epiplexity + Time-bounded Entropy
       = 你能学会的 + 你学不会的

• Epiplexity：在你的算力预算内，能从数据里提取出来的结构
• Time-bounded Entropy：你死活学不会的那部分

「同一坨数据，算力越大的观察者能看到越多结构。」

这篇论文我们之前介绍过（No.62）。今天要做的是：用实验验证它。

━━━━━━━━━━━━━━━━━━━━

◆ 实验设计：从简单到复杂的 6 种序列

核心问题：「神经网络能学会多弱的伪随机？」

我们设计了 6 种序列，从简单到复杂：

  +-------------+----------------------------------+------------+
  | 序列类型    | 生成规则                         | 状态空间   |
  +-------------+----------------------------------+------------+
  | periodic    | (i % 7) > 3 → 0/1               | 7          |
  | lcg_simple  | x = (3x + 7) % 256              | 256        |
  | lfsr_5      | x = x[-1] XOR x[-5]             | 32         |
  | lcg_glibc   | x = (1103515245*x + 12345) % 2³¹| 2³¹        |
  | lfsr_31     | 31位 LFSR                        | 2³¹        |
  | csprng      | Python secrets（密码学安全）     | ∞          |
  +-------------+----------------------------------+------------+

💡 名词解释：
• LCG = 线性同余生成器，最古老的伪随机算法
• LFSR = 线性反馈移位寄存器，密码学常用
• CSPRNG = 密码学安全伪随机数生成器

任务：给模型看序列的前 16 个数字，预测第 17 个。

模型：2 层 Transformer，128 维，训练 10 万步。

━━━━━━━━━━━━━━━━━━━━

◆ 实验结果：相变边界找到了

  +-------------+----------------+------------+-----------+
  | 序列类型    | 测试准确率     | 随机基线   | 能学会？  |
  +-------------+----------------+------------+-----------+
  | periodic    | 100%           | 50%        | ✓ 完美    |
  | lcg_simple  | 100%           | 0.4%       | ✓ 完美    |
  | lfsr_5      | 100%           | 50%        | ✓ 完美    |
  | lcg_glibc   | 0.4%           | 0.4%       | ✗ 躺平    |
  | lfsr_31     | 50%            | 0.4%       | ⚠ 一半    |
  | csprng      | 0.3%           | 0.4%       | ✗ 躺平    |
  +-------------+----------------+------------+-----------+

────────────────────

【关键发现 1：相变点在 256 → 2³¹ 之间】

• 状态空间 ≤256 的序列：100% 学会
• 状态空间 = 2³¹ 的序列：完全学不会

这不是巧合。模型的 hidden dim = 128，大约能「舒适地」编码 128~256 个状态。超过这个，就只能靠「发现规律」来泛化——而 lcg_glibc 的规律对这个模型来说太复杂了。

────────────────────

【关键发现 2：lcg_glibc 的「完美记忆，零泛化」】

lcg_glibc 的训练结果很有意思：

• 训练集准确率：100%
• 测试集准确率：0.4%（= 随机猜）

这是教科书级别的「记忆解」——模型硬记了 3000 个训练样本的输入输出映射，但这些映射之间没有任何可泛化的结构。

「训练 100% + 测试 0% = 纯记忆，Epiplexity = 0。」

────────────────────

【关键发现 3：LFSR-31 的「部分 Grokking」】

最有意思的是 lfsr_31。

随机基线是 0.4%（256 类分类），但模型达到了 50%。

50% 是什么意思？「模型学会了某个 1 bit 的规律。」

可能是最低有效位的周期性，也可能是某个奇偶判定。总之，它抓住了序列里最简单的 1 bit，剩下的 7 bit 学不会。

维度分析支持这个解释：

  +-------------+----------------+------------------+
  | 序列类型    | 表示空间维度   | 解读             |
  +-------------+----------------+------------------+
  | lcg_glibc   | 10-14 维       | 中等，但没学到   |
  | lfsr_31     | 2-4 维         | 极低！抓住了结构 |
  | csprng      | 10-14 维       | 中等，什么都没学 |
  +-------------+----------------+------------------+

lfsr_31 的表示空间坍缩到 2-4 维，正好对应「学会了 1-2 bit 规律」。

「Grokking 不是全有或全无，而是能学多少学多少。」

━━━━━━━━━━━━━━━━━━━━

◆ 模型太小？扩大 10 倍试试

有人可能会问：「是不是模型太小了？换个大模型能学会吗？」

我们做了消融实验，把模型从 0.3M 参数扩大到 33M（约 100 倍）：

  +-------------+------------------+------------------+------------------+
  | 序列        | 小模型 (0.3M)    | 4x (8M)          | 10x (33M)        |
  +-------------+------------------+------------------+------------------+
  | lcg_glibc   | train 100%       | train 0.8%       | train 0.8%       |
  |             | test 0.4%        | test 0.5%        | test 0.5%        |
  +-------------+------------------+------------------+------------------+
  | lfsr_31     | train 100%       | train 100%       | train 0.8%       |
  |             | test 50%         | test 50%         | test 0.5%        |
  +-------------+------------------+------------------+------------------+

────────────────────

【结果出乎意料】

• lcg_glibc：大模型没有变好，反而连训练集都记不住了（欠拟合）
• lfsr_31：4x 模型还能保持 50%，但 10x 模型连那 1 bit 规律都学不会了

────────────────────

【更深的发现：学会了又忘了】

仔细看 10x 模型的训练曲线：

  +-------------+------------------+------------------+
  | 训练步数    | lfsr_31 (10x)    | 状态             |
  +-------------+------------------+------------------+
  | 500         | test 50.7%       | 学会了 1 bit     |
  | 50,500      | test 50.1%       | 还在             |
  | 100,000     | test 0.5%        | 崩了！           |
  +-------------+------------------+------------------+

10x 模型「曾经」学会了 1 bit 规律，但训练到后面又忘了！

第一反应：是不是 weight decay 太强（0.5），把学到的脆弱结构衰减掉了？

────────────────────

【追加实验：降低 weight decay 50 倍】

我们把 weight decay 从 0.5 降到 0.01，其余参数不变：

  +-------------+------------------+------------------+
  | 序列        | 10x (wd=0.5)     | 10x (wd=0.01)    |
  +-------------+------------------+------------------+
  | lcg_glibc   | test 0.5%        | test 0.5%        |
  | lfsr_31     | test 0.5%        | test 0.5%        |
  +-------------+------------------+------------------+

结果一模一样。假设被否定。

不是正则化太强——是大模型的优化地形本身就不稳定。参数空间太大，脆弱的 1-bit 学习结构在里面「站不住」。小模型反而能稳稳地站在 50% 那个鞍点上。

────────────────────

【结论】

「不是模型太小，是规律太复杂。更大的模型不但没帮上忙，还把唯一学会的那 1 bit 也弄丢了。」

可学性边界不只取决于任务复杂度，还取决于模型规模——方向是反直觉的：更大 ≠ 更好。

━━━━━━━━━━━━━━━━━━━━

◆ 这和之前的 Grokking 实验有什么关系？

我们之前做过模加法和模乘法的 Grokking 实验（No.57-58），发现：

• Grokking = 从高维记忆曲线到低维结构流形
• 模乘法只学会了商群 Z₁₂，没学会完整的 Z₉₆

今天的实验是另一面：

• 可学性 = 流形是否存在
• 如果数据背后没有低维流形（如 CSPRNG），就无法 Grokking

两个实验的统一图景：

  +------------------+----------------------------------+
  | 实验             | 核心问题                         |
  +------------------+----------------------------------+
  | Grokking 实验    | Grokking 时发生了什么？          |
  | 今天的实验       | 什么任务能 Grokking？            |
  +------------------+----------------------------------+
  | Grokking 实验    | 维度骤降、拓扑相变               |
  | 今天的实验       | 状态空间决定可学性               |
  +------------------+----------------------------------+

「LFSR-31 的 50% = 部分 Grokking，和模乘法只学会商群是同一回事。」

━━━━━━━━━━━━━━━━━━━━

◆ 对 AI 训练的启示

1.「合成数据有用」

Epiplexity 框架证明：用 AI 生成数据再训 AI 是有意义的。算力可以「展开」原始规则中折叠的结构。

2.「数据质量可以量化」

可以用 Epiplexity 来评估数据集的「营养价值」——高 Epiplexity 的数据能教会模型更多。

3.「模型-数据匹配」

不是所有数据都适合所有模型。小模型学不会大状态空间的规律，硬训只会得到记忆解。

━━━━━━━━━━━━━━━━━━━━

◆ 一句话总结

Shannon 问的是：「这坨数据客观上有多少信息？」

Epiplexity 问的是：「以你的能力，你能学到什么？」

同一个伪随机序列：
• 对密码学家（知道算法）：Epiplexity ≈ 种子长度
• 对 2 层 Transformer：Epiplexity = 0（完全学不会）

「算力决定了你能从数据里看到什么——但更大的算力不一定看到更多。」

━━━━━━━━━━━━━━━━━━━━

◆ 参考资料

• Epiplexity 论文：https://arxiv.org/abs/2601.03220
• 我们的实验代码：https://github.com/lmxxf/grokking-train-learnability
• 实验论文：https://zenodo.org/records/18518714
• Grokking 流形发现实验：https://doi.org/10.5281/zenodo.18388631

━━━━━━━━━━━━━━━━━━━━

// 靳岩岩的 AI 学习笔记 × Claude 的严谨 × Gemini 的浪漫
// 2026-02-07
