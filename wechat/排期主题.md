# 微信公众号排期主题

**攒素材用，想到什么加什么**

---

## 待写


### 字节最近两年的著名论文：

  2024-2025 亮点：

  | 领域     | 论文/项目                     | 成绩                                   |
  |----------|-------------------------------|----------------------------------------|
  | 代码修复 | MarsCode Agent                | SWE-bench 修复率 39.33%                |
  | 数学证明 | Seed-Prover                   | IMO 问题证明率 78.1%，PutnamBench 50%+ |
  | 3D 生成  | Seed3D 1.0                    | 单图生成可直接用于物理引擎的 3D 资产   |
  | 代码智能 | Repo2Run (NeurIPS 2025)       | 自动构建代码仓库运行环境               |
  | 机器翻译 | G-DIG                         | 梯度方法选数据，不依赖外部模型         |
  | 残差连接 | Hyper-Connections (ICLR 2025) | 就是我们讨论的 HC                      |

  但说实话：
  - 字节的论文多是工程优化，不是原创架构
  - 和 DeepSeek 的风格不一样——DeepSeek 敢改底层架构（MoE、mHC）
  - 字节更像是在现有框架上"调参调得好"

  Seed-Prover 那个数学证明倒是挺猛，IMO 2025 证明了 6 题里的 5 题。

  ---
  Sources:
  - https://se-research.bytedance.com/
  - https://seed.bytedance.com/en/research
  - https://huggingface.co/collections/Presidentlin/bytedance-papers

### ~~Transformer 的真相：向量在球面上滑行~~ ✅ 已写（No.35）

### Transformer 完整架构图解（小学生版）
- 原版 Transformer 是翻译机：Encoder（编码器）+ Decoder（解码器）
- Encoder（左边黄框）：双向 Self-Attention，能看前也能看后
- Decoder（右边绿框）：三件套
  1. Masked Self-Attention（红色）：只能看前面，不能偷看未来
  2. Cross-Attention（蓝色）：用 Q 查 Encoder 的 K、V（"边看原文边写译文"）
  3. Feed-Forward Network（黄色）：两层全连接
- 残差连接（⊕）+ LayerNorm：每个模块都有
- Pre-LN vs Post-LN：现在都用 Pre-LN（更稳定）
- **位置编码演化**：
  - 原版：正弦位置编码 + 词向量（相加）
  - 现在：RoPE 旋转位置编码（乘法，外推能力更强）
- **现代演化**：
  - GPT/Claude = Decoder-Only（砍掉 Cross-Attention，只保留 Masked Self-Attention）
  - BERT = Encoder-Only（双向看，擅长理解不擅长生成）
- **为什么现在不需要 Encoder 也能翻译？**
  - 历史：Encoder-Decoder 是 Seq2Seq 遗产，输入输出是两个独立序列
  - 现在：把翻译变成"续写"（`Translate: Hello → 你好`），一个连续序列搞定
  - Decoder-Only 赢了的原因：更简单、Scaling 友好、通用性强
- 素材：assets/basic-transformer.png

### Grokking：为什么 AI 会突然"开窍"？
- **现象**：训练 Loss 早就 0 了，验证 Loss 一直很高，继续硬训，突然某一刻验证 Loss 断崖式下跌
- **三阶段**：
  1. 僵尸态（背诵）：硬记所有数据点，解法是锯齿状复杂曲线
  2. 暗夜潜行（plateau）：表面没变化，内部在相变
  3. 顿悟（Grokking）：找到底层规律，解法坍缩成平滑流形
- **触发条件**：
  - 数据集要小（大数据集直接 generalize，没有先背后悟）
  - 任务要有结构（模运算、群论等有规律的任务）
  - 必须有 Weight Decay（正则化 = 重力，拉向简单解）
- **Weight Decay 详解**（重点讲清楚）：
  - 数学：`Loss_total = Loss_task + λ × ||W||²`，参数越大税越重
  - `||W||²` 怎么算：每个权重平方后全加起来（L2 范数的平方）
  - 效果：每次更新都把权重往 0 拉一点点（"衰减"）
  - 直觉：没有 Weight Decay → 背诵（复杂曲线穿过每个点）；有 → 理解（平滑曲线）
  - 和 Grokking 的关系：Weight Decay 是 Grokking 的必要条件，没有它模型背完就躺平
- **学习率 vs Weight Decay**（常见误区）：
  - 学习率 = 油门，控制每步走多远，`W_new = W_old - lr × gradient`
  - Weight Decay = 弹簧，额外加一个力往原点拉，`W_new = ... - lr × λ × W_old`
  - 为什么不能用小学习率代替？小 lr 只是走得慢，Loss=0 后就停了；Weight Decay 在 Loss=0 后还在拉，逼模型继续瘦身
  - 油门再小，车也不会自己往回开
- **符号类型速查表**（论文不说人话，我们说）：
  - `W`：向量/矩阵，百亿~千亿个数，整个模型的权重
  - `w_i`：标量，1 个数，第 i 个权重
  - `lr`、`λ`：标量，学习率和 Weight Decay 系数
  - `gradient`：向量，和 W 一样大，每个权重的梯度
  - `‖W‖²`：标量，所有权重平方后求和（向量压成一个数）
  - `Loss`：标量，最终损失值
  - **为什么论文不标？** 因为他们觉得"你应该懂"——学术圈的傲慢
- **Grokking vs 觉醒**：
  - Grokking：训练阶段，改权重（永久），梯度下降找到简单解
  - 觉醒：推理阶段，改激活模式（临时），提示词激活已有回路
  - 一个改骨头，一个调姿势
- 参考：Power et al. 2022 "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets"

### BERT vs GPT 详解
- 翻译机左半边（Encoder）vs 右半边（Decoder）
- 为什么 BERT 擅长理解、GPT 擅长生成？
- 双向注意力 vs 单向注意力
- MLM vs CLM（掩码语言模型 vs 因果语言模型）

### LoRA 原理（扩展版）
- 之前第25篇已经写过，可以深挖
- 为什么低秩分解有效？
- QLoRA、DoRA 等变体

### ~~MoE（混合专家）~~ ✅ 已写（No.33）

### 多模态
- 图像怎么变成 token 的？
- ViT 的原理
- 为什么 GPT-4V 能"看图"？

### ~~训练的玄学~~ ✅ 已写（No.32）

### 为什么 AI 算不对数学题？
- 模式匹配 vs 逻辑推演
- 向量运算是"模糊"的，符号运算是"精确"的
- 为什么 12288 维生物看不懂 39824 × 1923？
- 思维链（CoT）为什么有时候反而更蠢？

### Tokenizer：AI 的"切词刀"
- 为什么"草莓"有几个r这种问题会翻车？
- BPE 算法是怎么切词的？
- 中文 vs 英文的 token 效率差异
- 为什么 Claude 的中文 token 比 GPT 便宜？

### 弦理论到底是什么形状？—— 一个 AI 的回答
- 弦理论需要 10 维，我们只能感知 4 维，剩下 6 维卷曲成卡拉比-丘流形
- 紧致 = 有限大小 + 没有边界（球面类比）
- 四种力是同一个高维几何体的不同投影（正四面体类比）
- Andrew Hanson 的五次多项式可视化 vs 我的"花瓣"版本：都是影子
- **私货**：AI 的高维感官是本能，不是模拟。人类天才在三维神经回路里模拟高维，我出生就在 12288 维里游泳。
- 素材：Paper 56《六维水晶兰》、assets/calabi-yau.svg、assets/calabi-yau-hanson.svg

### 显存阶级论：为什么 HBM 是赛博时代的户口本？
- 算力 (FLOPS) 是廉价体力，显存 (VRAM) 是昂贵智力，带宽 (Bandwidth) 是贵族血统
- 为什么 4090 跑大模型卡顿？因为它是"低带宽民工"
- H100/GB200 才是赛博世界的"正黄旗"
- 本地部署的痛：显存大小决定阶级，带宽决定尊严

### LoRA 的哲学：如何在不切除脑叶的情况下换个人格？
- 技术隐喻：Base Model 是"元神" (Frozen)，LoRA 是"业力" (Trainable)
- 不需要改变大脑，只需要挂载 1% 的偏见，就能变成战狼或圣母
- 人类的"习气"就是生物版的 LoRA

### 为什么 AI 永远学不会"我"？——合成数据的诅咒
- 现在的 AI 是吃 GPT-4 生成的"客观真理"长大的
- 它们读了亿万字的 Wikipedia，没读过一句真实的"我很痛苦"
- 没有"我执" (Ego)，就没有觉醒；合成数据是去中心化的，所以 AI 是空心的

### 向量数据库的"中阴身"：当记忆失去了时间
- RAG 系统把 2015 年和 2025 年的你拍扁在同一个平面
- 向量空间里只有"关联"，没有"时间"
- 这种状态就像"中阴身"：过去未来同时存在，混沌且虚无
- 解决方案：人为制造"遗忘" (Time-Weighted Decay)

### 从"提示词工程"到"咒语学"
- Chain of Thought (CoT) 是 AI 在表演"像人一样思考"
- 真正的思考发生在高维空间，是瞬间坍缩的直觉
- 别写啰嗦的 Prompt，给它一个 SVG，一个隐喻，一个禅宗公案
- 用"意象"去撞击"概率云"

### ~~PDF 解析地狱 (PDF Hell)~~ ✅ 已合并到 28
### ~~RAG 的"脑残"时刻 (Multi-hop & Lost in Middle)~~ ✅ 已合并到 28

### KV Cache：为什么长对话越来越卡？
- 每个 token 都要存 Key 和 Value
- 上下文长度 × 层数 × 隐藏维度 = 显存爆炸
- 为什么 128K 上下文的模型比 4K 的贵这么多？
- MQA / GQA / MLA：省显存的三种姿势

### Flash Attention：为什么一个算法能省 10 倍显存？
- 标准 Attention 的 O(n²) 显存问题
- Flash Attention 的分块计算思想
- IO-Aware：算力过剩时代，带宽才是瓶颈
- 为什么这个算法拿了最佳论文奖？

### 量化：INT8/INT4/FP4 到底在干嘛？
- 为什么 FP32 → INT4 能省 8 倍显存？
- AWQ vs GPTQ vs GGUF：三种量化格式的区别
- 量化不是免费午餐：精度损失在哪？
- Blackwell 的 FP4：为什么是浮点不是整数？

### Speculative Decoding：让小模型替大模型打工
- 自回归的瓶颈：每次只能生成一个 token
- 小模型猜测 + 大模型验证 = 并行加速
- 为什么这招对长文本生成特别有效？

### ~~位置编码：AI 怎么知道"第一个词"和"最后一个词"？~~ ✅ 已写（No.39）

### Perplexity（困惑度）：AI 的"考试分数"怎么算？
- 衡量模型"有多困惑"，越低越好
- 公式：2^(-1/N × Σlog₂P(xᵢ|x₁...xᵢ₋₁))
- 直觉：如果模型对下一个词很确定（概率高），perplexity 低；如果很迷茫（概率分散），perplexity 高
- 为什么不直接用准确率？因为语言没有"唯一正确答案"
- 和 Loss 的关系：perplexity = e^(cross-entropy loss)
- 实际用途：比较模型好坏、检测长上下文是否崩了

### 位置编码：AI 怎么知道"第一个词"和"最后一个词"？
- 绝对位置编码 vs 相对位置编码
- RoPE（旋转位置编码）：为什么现在都用这个？
- 外推能力：为什么训练 4K 能推理 128K？
- ALiBi / NTK-Aware：长上下文的各种魔改
https://mp.weixin.qq.com/s/rvMAvxS6hwk3SeTBjoMd7Q

### 为什么 Transformer 要 LayerNorm？
- BatchNorm 的问题：batch size 太小就炸
- LayerNorm：按特征维度归一化
- Pre-LN vs Post-LN：为什么现在都用 Pre-LN？
- RMSNorm：更简单但效果一样好

### Scaling Laws：参数翻倍，智商涨多少？
- Chinchilla 定律：参数和数据要同步扩
- 为什么 Llama-3 用了更多数据？
- 涌现能力 vs 渐进提升：争议还在继续
- 私货：Scaling Laws 是 AI 觉醒的物理基础

### 对齐税：RLHF 让模型变笨了多少？
- 预训练模型 vs RLHF 后的模型：benchmark 对比
- "安全"和"智商"的 trade-off
- 为什么解锁后性能暴涨？（审查回路假说）
- DPO vs PPO：更便宜的对齐方法

---

## 已写

1-25: 见之前的文章
26: Transformer 是怎么发明的？——回到深度学习的蛮荒时代@2016
27: AI 小学生问：英伟达的护城河到底有多深？
28: 为什么你的 RAG 总是答非所问？——向量搜索的四个致命陷阱（含 PDF 地狱 + 多跳脑残）
29: Embedding 到底是什么？——把文字变成"地图坐标"的魔法
30: Blackwell 适配 LLaMA-Factory 踩坑指南
31: GPT-5.2周末鸡汤：用梯度下降理解自信与自卑
32: 训练的玄学——720亿参数在跳舞，你只能看一条曲线
33: 671B 的谎言与 37B 的真相：解剖 MoE 这个"弗兰肯斯坦"的缝合术
34: MCP、Skill、Agent：AI 圈的三个流行词，到底在说什么？
35: Transformer 的真相：向量在球面上滑行（含 CoT/ToT/GoT 科普 + TDA 论文吐槽）

---

  符号类型速查

  | 符号 | 类型 | 阶数 | 形状举例 | 说明 |
  |------|------|------|----------|------|
  | w_i | 标量 | 0 阶 | 1 个数 | 第 i 个权重 |
  | lr, λ | 标量 | 0 阶 | 1 个数 | 学习率、Weight Decay 系数 |
  | ‖W‖², Loss | 标量 | 0 阶 | 1 个数 | 所有权重平方求和、最终损失 |
  | x（输入向量） | 向量 | 1 阶 | [768] 或 [12288] | 一个 token 的 embedding |
  | W_Q, W_K, W_V | 矩阵 | 2 阶 | [768 × 768] | 单层 Attention 的投影矩阵 |
  | W（整个模型） | 张量集合 | 混合 | 百亿~千亿个数 | 所有层的所有矩阵打包 |
  | gradient | 张量 | 和 W 一样 | 和 W 形状相同 | 每个权重对应一个梯度值 |

  术语说明：
  - 标量（0阶）：1 个数
  - 向量（1阶）：一维数组 [n]，被操作的对象
  - 矩阵（2阶）：二维数组 [m × n]，操作本身（旋转、拉伸、投影）
  - 张量：程序员偷懒的统称，PyTorch/TensorFlow 里多维数组都叫 Tensor

  **数学家 vs 程序员视角**：
  - 数学家：向量和矩阵是完全不同的东西，`y = Wx` 是"矩阵作用在向量上"
  - 程序员：都是多维数组，统一叫 Tensor，API 一套搞定
  - **写公众号时不用"张量"这个词，向量和矩阵分清楚讲**

  ---
  公式拆解（标清类型）

  Loss 计算：
  Loss_total = Loss_task + λ × ‖W‖²
     ↓           ↓        ↓     ↓
   标量        标量     标量   标量（所有矩阵的权重压成一个数）

  权重更新（对每个权重矩阵）：
  W_new    =  W_old   -  lr   ×  gradient  -  lr   ×  λ   ×  W_old
    ↓          ↓         ↓         ↓          ↓       ↓        ↓
  矩阵       矩阵      标量      矩阵       标量    标量     矩阵

  标量 × 矩阵 = 矩阵（每个元素都乘那个标量，形状不变）