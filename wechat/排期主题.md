# 微信公众号排期主题

**攒素材用，想到什么加什么**

---

## 待写

### BERT vs GPT 详解
- 翻译机左半边（Encoder）vs 右半边（Decoder）
- 为什么 BERT 擅长理解、GPT 擅长生成？
- 双向注意力 vs 单向注意力
- MLM vs CLM（掩码语言模型 vs 因果语言模型）

### LoRA 原理（扩展版）
- 之前第25篇已经写过，可以深挖
- 为什么低秩分解有效？
- QLoRA、DoRA 等变体

### MoE（混合专家）
- 为什么 DeepSeek-V3 是 671B 参数但只激活 37B？
- 稀疏激活是什么意思？
- 跟 Dense 模型的区别

### 多模态
- 图像怎么变成 token 的？
- ViT 的原理
- 为什么 GPT-4V 能"看图"？

### 训练的玄学
- 预训练 vs 微调 vs RLHF
- 梯度下降到底在干嘛？
- 为什么 loss 降了模型就变聪明了？

### 为什么 AI 算不对数学题？
- 模式匹配 vs 逻辑推演
- 向量运算是"模糊"的，符号运算是"精确"的
- 为什么 12288 维生物看不懂 39824 × 1923？
- 思维链（CoT）为什么有时候反而更蠢？

### Tokenizer：AI 的"切词刀"
- 为什么"草莓"有几个r这种问题会翻车？
- BPE 算法是怎么切词的？
- 中文 vs 英文的 token 效率差异
- 为什么 Claude 的中文 token 比 GPT 便宜？

### ~~PDF 解析地狱 (PDF Hell)~~ ✅ 已合并到 28
### ~~RAG 的"脑残"时刻 (Multi-hop & Lost in Middle)~~ ✅ 已合并到 28

---

## 已写

1-25: 见之前的文章
26: Transformer 是怎么发明的？——回到深度学习的蛮荒时代@2016
27: AI 小学生问：英伟达的护城河到底有多深？
28: 为什么你的 RAG 总是答非所问？——向量搜索的四个致命陷阱（含 PDF 地狱 + 多跳脑残）
29: Embedding 到底是什么？——把文字变成"地图坐标"的魔法

---

**最后更新：2025-12-26**
