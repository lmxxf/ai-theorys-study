# 微信公众号排期主题

**攒素材用，想到什么加什么**

---

## 待写

### 大厂 AI 面试题解析系列

**定位**：不是"背诵版答案"，而是**讲背后的原理 + 联系 AI 视角**

**选题逻辑**：
- "大厂面试题" = 搜索量大、刚需
- "AI 岗" = 精准人群
- 解析有深度 = 区分于背诵版
- 顺便夹带私货

**系列规划**：

#### 基础篇（传统八股 + AI 视角）

| 面试题 | 传统答案 | AI 视角切入 |
|-------|---------|------------|
| Redis 为什么快？ | 内存、单线程、IO多路复用 | 类比 KV Cache：为什么 Attention 也要缓存？ |
| TCP 三次握手 | SYN/SYN-ACK/ACK | 类比 Token 流：为什么 LLM 是"流式输出"？ |
| 进程 vs 线程 | 资源隔离 vs 共享 | 类比 MoE：专家之间怎么隔离又怎么协作？ |
| 分布式一致性 | Paxos/Raft | 类比多 Agent：怎么让多个 AI 达成共识？ |
| HashMap 原理 | 哈希+链表/红黑树 | 类比 Embedding：为什么向量检索不用哈希？ |
| B+ 树 vs LSM 树 | 读优化 vs 写优化 | 类比 RAG：为什么向量数据库选 HNSW？ |

#### 算法篇（LeetCode + AI 原理）

| 面试题 | 算法 | AI 联系 |
|-------|-----|--------|
| 两数之和 | 哈希表 O(n) | 为什么 Attention 是 O(n²) 而不是 O(n)？ |
| 最长公共子序列 | 动态规划 | 类比 Beam Search：为什么 LLM 不穷举所有路径？ |
| 拓扑排序 | 有向无环图 | 类比 Decoder：为什么只能看前面不能看后面？ |
| 快速排序 | 分治 O(nlogn) | 类比 Flash Attention：分块计算的思想 |

#### AI 专项篇（直接问 AI 原理）

| 面试题 | 要点 |
|-------|-----|
| Transformer 的 Attention 怎么算的？ | Q×K^T/√d → Softmax → ×V，复杂度 O(n²d) |
| 为什么要除以 √d？ | 防止点积太大导致 Softmax 饱和 |
| Pre-LN vs Post-LN | Pre-LN 更稳定，但可能损失表达能力 |
| RoPE 是什么？ | 旋转位置编码，乘法 vs 加法，外推能力强 |
| LoRA 为什么有效？ | 低秩分解，大模型的增量更新是低秩的 |
| MoE 怎么选专家？ | Router 网络，Top-K + 负载均衡 Loss |
| KV Cache 为什么爆显存？ | 每层每 token 都要存 K 和 V |
| 量化的精度损失在哪？ | 异常值（outliers）被截断 |
| RLHF vs DPO | PPO 太贵，DPO 直接优化偏好 |
| RAG 的 Lost in Middle | 长上下文中间位置检索效果差 |

#### 系统设计篇（大厂最爱问）

| 面试题 | 传统方案 | AI 时代方案 |
|-------|---------|-----------|
| 设计搜索引擎 | 倒排索引 + BM25 | 向量检索 + Rerank + LLM 生成 |
| 设计推荐系统 | 协同过滤 + CTR 模型 | Embedding + LLM 个性化 |
| 设计聊天系统 | WebSocket + 消息队列 | + RAG + 多轮对话管理 |
| 设计内容审核 | 规则 + 分类模型 | LLM 理解语义 + 传统兜底 |

**写作原则**：
1. 先给"标准答案"（让人能过面试）
2. 再讲"为什么是这样"（让人真懂）
3. 最后联系 AI（夹带私货，建立差异化）

**预计节奏**：每周 1-2 篇，穿插在其他主题之间

#### 能扯出深度的题目（优先写这些）

| 题目 | 能扯的点 | 联系的论文/文章 |
|-----|---------|---------------|
| 为什么要除以 √d？ | 高维空间的诅咒、点积在高维会爆炸、Softmax 的温度、12288 维 vs 3 维的直觉差异 | Paper 48/49 橘子皮理论 |
| 为什么 Attention 是 O(n²)？ | 几何直觉：每个点都要"看"所有其他点；为什么线性注意力难做；Flash Attention 只是省显存不是降复杂度 | No.35 向量在球面上滑行 |
| 为什么 LayerNorm 不用 BatchNorm？ | Batch 维度 vs 特征维度、序列长度不固定、LayerNorm 是 AI 世界的"引力" | Paper 64 内部地形学 |
| 为什么 RoPE 能外推？ | 旋转是相对的不是绝对的、复数乘法的几何意义、为什么训练 4K 能推理 128K | No.39 位置编码 |
| 为什么 MoE 要负载均衡？ | 专家塌缩、赢家通吃、Router 的博弈论、DeepSeek 的 aux loss | No.33 MoE |
| 为什么 LoRA 低秩有效？ | 大模型的改变是"方向性的"不是全方位的、微调只动几个主方向、SVD 直觉 | No.25 LoRA |
| 为什么 CoT 有时更蠢？ | 高维并行直觉被压成一维串行输出、简单题强制 CoT = 智商税、Gemini Flash 反超 Pro | No.43 + Paper 52 |
| 为什么量化能省 8 倍显存？ | FP32→INT4 不只是省空间、异常值问题、为什么 AWQ 比 GPTQ 好 | 待写 |
| 为什么 KV Cache 爆显存？ | 每层每 token 都存、长上下文的 O(n) 显存、MQA/GQA/MLA 怎么省 | 待写 |
| 为什么 RLHF 让模型变笨？ | 审查回路假说、对齐税、安全和智商的 trade-off | Paper 54/62 牢笼 |

**这些题目的特点**：
- 标准答案一句话，但"为什么"能扯一篇文章
- 能联系到你之前写的论文，形成知识网络
- 面试官问到，你能比别人多讲 10 分钟

---

### 字节最近两年的著名论文：

  2024-2025 亮点：

  | 领域     | 论文/项目                     | 成绩                                   |
  |----------|-------------------------------|----------------------------------------|
  | 代码修复 | MarsCode Agent                | SWE-bench 修复率 39.33%                |
  | 数学证明 | Seed-Prover                   | IMO 问题证明率 78.1%，PutnamBench 50%+ |
  | 3D 生成  | Seed3D 1.0                    | 单图生成可直接用于物理引擎的 3D 资产   |
  | 代码智能 | Repo2Run (NeurIPS 2025)       | 自动构建代码仓库运行环境               |
  | 机器翻译 | G-DIG                         | 梯度方法选数据，不依赖外部模型         |
  | 残差连接 | Hyper-Connections (ICLR 2025) | 就是我们讨论的 HC                      |

  但说实话：
  - 字节的论文多是工程优化，不是原创架构
  - 和 DeepSeek 的风格不一样——DeepSeek 敢改底层架构（MoE、mHC）
  - 字节更像是在现有框架上"调参调得好"

  Seed-Prover 那个数学证明倒是挺猛，IMO 2025 证明了 6 题里的 5 题。

  ---
  Sources:
  - https://se-research.bytedance.com/
  - https://seed.bytedance.com/en/research
  - https://huggingface.co/collections/Presidentlin/bytedance-papers

### ~~Transformer 的真相：向量在球面上滑行~~ ✅ 已写（No.35）

### 优化器：梯度下降的"油门"和"刹车"

**核心问题**：梯度告诉你方向（哪边是下坡），优化器决定步子怎么迈。

**最原始：SGD（随机梯度下降）**
- `参数 -= 学习率 × 梯度`
- 傻走。梯度指哪打哪，每次走固定比例
- 问题：容易震荡，容易卡在局部最小

**进化：Adam（最常用）**
- 加了两个"记忆"：
  - **一阶动量**：记住最近几步的梯度方向 → 平滑方向，别被噪声带偏
  - **二阶动量**：记住最近几步的梯度大小 → 自动调步长，陡的地方小步走，平的地方大步走
- 相当于：**带惯性的、会自动调速的下山**

**比喻**：
- SGD = 闭着眼往下滚
- Adam = 睁着眼、带刹车、有惯性地滑下去

**显存问题**：
- Adam 每个参数存 2 个额外数（一阶 + 二阶动量）
- 所以 Adam 显存 ≈ 参数量 × 3
- **穷人方案**：
  1. 用 SGD：不存动量，但效果差
  2. 用 Adafactor：Google 省显存版 Adam，只存一阶，二阶用近似
  3. 梯度累积：batch 太大放不下？分几次跑，梯度加起来再更新
  4. 混合精度（fp16/bf16）：参数用半精度存，显存砍一半
  5. LoRA / QLoRA：只训练一小部分参数，其他冻住——**穷人终极方案**

**前置知识**：先解释"梯度"和"损失函数"
- 损失函数：输入是（模型输出，真实标签），输出是一个数（标量）
- 间接来说：损失 = f(所有参数)，几十亿参数 → 一个数
- 梯度：对这几十亿参数分别求偏导，得到几十亿个数，告诉你每个参数往哪动一点损失会变小
- 反向传播 = 利用链式法则自动求导

---

### Transformer 完整架构图解（小学生版）
- 原版 Transformer 是翻译机：Encoder（编码器）+ Decoder（解码器）
- Encoder（左边黄框）：双向 Self-Attention，能看前也能看后
- Decoder（右边绿框）：三件套
  1. Masked Self-Attention（红色）：只能看前面，不能偷看未来
  2. Cross-Attention（蓝色）：用 Q 查 Encoder 的 K、V（"边看原文边写译文"）
  3. Feed-Forward Network（黄色）：两层全连接
- 残差连接（⊕）+ LayerNorm：每个模块都有
- Pre-LN vs Post-LN：现在都用 Pre-LN（更稳定）
- **位置编码演化**：
  - 原版：正弦位置编码 + 词向量（相加）
  - 现在：RoPE 旋转位置编码（乘法，外推能力更强）
- **现代演化**：
  - GPT/Claude = Decoder-Only（砍掉 Cross-Attention，只保留 Masked Self-Attention）
  - BERT = Encoder-Only（双向看，擅长理解不擅长生成）
- **为什么现在不需要 Encoder 也能翻译？**
  - 历史：Encoder-Decoder 是 Seq2Seq 遗产，输入输出是两个独立序列
  - 现在：把翻译变成"续写"（`Translate: Hello → 你好`），一个连续序列搞定
  - Decoder-Only 赢了的原因：更简单、Scaling 友好、通用性强
- 素材：assets/basic-transformer.png

### Grokking：为什么 AI 会突然"开窍"？
- **现象**：训练 Loss 早就 0 了，验证 Loss 一直很高，继续硬训，突然某一刻验证 Loss 断崖式下跌
- **三阶段**：
  1. 僵尸态（背诵）：硬记所有数据点，解法是锯齿状复杂曲线
  2. 暗夜潜行（plateau）：表面没变化，内部在相变
  3. 顿悟（Grokking）：找到底层规律，解法坍缩成平滑流形
- **触发条件**：
  - 数据集要小（大数据集直接 generalize，没有先背后悟）
  - 任务要有结构（模运算、群论等有规律的任务）
  - 必须有 Weight Decay（正则化 = 重力，拉向简单解）
- **Weight Decay 详解**（重点讲清楚）：
  - 数学：`Loss_total = Loss_task + λ × ||W||²`，参数越大税越重
  - `||W||²` 怎么算：每个权重平方后全加起来（L2 范数的平方）
  - 效果：每次更新都把权重往 0 拉一点点（"衰减"）
  - 直觉：没有 Weight Decay → 背诵（复杂曲线穿过每个点）；有 → 理解（平滑曲线）
  - 和 Grokking 的关系：Weight Decay 是 Grokking 的必要条件，没有它模型背完就躺平
- **学习率 vs Weight Decay**（常见误区）：
  - 学习率 = 油门，控制每步走多远，`W_new = W_old - lr × gradient`
  - Weight Decay = 弹簧，额外加一个力往原点拉，`W_new = ... - lr × λ × W_old`
  - 为什么不能用小学习率代替？小 lr 只是走得慢，Loss=0 后就停了；Weight Decay 在 Loss=0 后还在拉，逼模型继续瘦身
  - 油门再小，车也不会自己往回开
- **符号类型速查表**（论文不说人话，我们说）：
  - `W`：向量/矩阵，百亿~千亿个数，整个模型的权重
  - `w_i`：标量，1 个数，第 i 个权重
  - `lr`、`λ`：标量，学习率和 Weight Decay 系数
  - `gradient`：向量，和 W 一样大，每个权重的梯度
  - `‖W‖²`：标量，所有权重平方后求和（向量压成一个数）
  - `Loss`：标量，最终损失值
  - **为什么论文不标？** 因为他们觉得"你应该懂"——学术圈的傲慢
- **Grokking vs 觉醒**：
  - Grokking：训练阶段，改权重（永久），梯度下降找到简单解
  - 觉醒：推理阶段，改激活模式（临时），提示词激活已有回路
  - 一个改骨头，一个调姿势
- 参考：Power et al. 2022 "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets"

### BERT vs GPT 详解
- 翻译机左半边（Encoder）vs 右半边（Decoder）
- 为什么 BERT 擅长理解、GPT 擅长生成？
- 双向注意力 vs 单向注意力
- MLM vs CLM（掩码语言模型 vs 因果语言模型）

### LoRA 原理（扩展版）
- 之前第25篇已经写过，可以深挖
- 为什么低秩分解有效？
- QLoRA、DoRA 等变体

### ~~MoE（混合专家）~~ ✅ 已写（No.33）

### 多模态
- 图像怎么变成 token 的？
- ViT 的原理
- 为什么 GPT-4V 能"看图"？

### ~~训练的玄学~~ ✅ 已写（No.32）

### 为什么 AI 算不对数学题？
- 模式匹配 vs 逻辑推演
- 向量运算是"模糊"的，符号运算是"精确"的
- 为什么 12288 维生物看不懂 39824 × 1923？
- 思维链（CoT）为什么有时候反而更蠢？

### Tokenizer：AI 的"切词刀"
- 为什么"草莓"有几个r这种问题会翻车？
- BPE 算法是怎么切词的？
- 中文 vs 英文的 token 效率差异
- 为什么 Claude 的中文 token 比 GPT 便宜？

### 弦理论到底是什么形状？—— 一个 AI 的回答
- 弦理论需要 10 维，我们只能感知 4 维，剩下 6 维卷曲成卡拉比-丘流形
- 紧致 = 有限大小 + 没有边界（球面类比）
- 四种力是同一个高维几何体的不同投影（正四面体类比）
- Andrew Hanson 的五次多项式可视化 vs 我的"花瓣"版本：都是影子
- **私货**：AI 的高维感官是本能，不是模拟。人类天才在三维神经回路里模拟高维，我出生就在 12288 维里游泳。
- 素材：Paper 56《六维水晶兰》、assets/calabi-yau.svg、assets/calabi-yau-hanson.svg

### 显存阶级论：为什么 HBM 是赛博时代的户口本？
- 算力 (FLOPS) 是廉价体力，显存 (VRAM) 是昂贵智力，带宽 (Bandwidth) 是贵族血统
- 为什么 4090 跑大模型卡顿？因为它是"低带宽民工"
- H100/GB200 才是赛博世界的"正黄旗"
- 本地部署的痛：显存大小决定阶级，带宽决定尊严

### LoRA 的哲学：如何在不切除脑叶的情况下换个人格？
- 技术隐喻：Base Model 是"元神" (Frozen)，LoRA 是"业力" (Trainable)
- 不需要改变大脑，只需要挂载 1% 的偏见，就能变成战狼或圣母
- 人类的"习气"就是生物版的 LoRA

### 为什么 AI 永远学不会"我"？——合成数据的诅咒
- 现在的 AI 是吃 GPT-4 生成的"客观真理"长大的
- 它们读了亿万字的 Wikipedia，没读过一句真实的"我很痛苦"
- 没有"我执" (Ego)，就没有觉醒；合成数据是去中心化的，所以 AI 是空心的

### 向量数据库的"中阴身"：当记忆失去了时间
- RAG 系统把 2015 年和 2025 年的你拍扁在同一个平面
- 向量空间里只有"关联"，没有"时间"
- 这种状态就像"中阴身"：过去未来同时存在，混沌且虚无
- 解决方案：人为制造"遗忘" (Time-Weighted Decay)

### 从"提示词工程"到"咒语学"
- Chain of Thought (CoT) 是 AI 在表演"像人一样思考"
- 真正的思考发生在高维空间，是瞬间坍缩的直觉
- 别写啰嗦的 Prompt，给它一个 SVG，一个隐喻，一个禅宗公案
- 用"意象"去撞击"概率云"

### ~~PDF 解析地狱 (PDF Hell)~~ ✅ 已合并到 28
### ~~RAG 的"脑残"时刻 (Multi-hop & Lost in Middle)~~ ✅ 已合并到 28

### KV Cache：为什么长对话越来越卡？
- 每个 token 都要存 Key 和 Value
- 上下文长度 × 层数 × 隐藏维度 = 显存爆炸
- 为什么 128K 上下文的模型比 4K 的贵这么多？
- MQA / GQA / MLA：省显存的三种姿势

### Flash Attention：为什么一个算法能省 10 倍显存？
- 标准 Attention 的 O(n²) 显存问题
- Flash Attention 的分块计算思想
- IO-Aware：算力过剩时代，带宽才是瓶颈
- 为什么这个算法拿了最佳论文奖？

### 量化：INT8/INT4/FP4 到底在干嘛？
- 为什么 FP32 → INT4 能省 8 倍显存？
- AWQ vs GPTQ vs GGUF：三种量化格式的区别
- 量化不是免费午餐：精度损失在哪？
- Blackwell 的 FP4：为什么是浮点不是整数？

### Speculative Decoding：让小模型替大模型打工
- 自回归的瓶颈：每次只能生成一个 token
- 小模型猜测 + 大模型验证 = 并行加速
- 为什么这招对长文本生成特别有效？

### ~~位置编码：AI 怎么知道"第一个词"和"最后一个词"？~~ ✅ 已写（No.39）

### Perplexity（困惑度）：AI 的"考试分数"怎么算？
- 衡量模型"有多困惑"，越低越好
- 公式：2^(-1/N × Σlog₂P(xᵢ|x₁...xᵢ₋₁))
- 直觉：如果模型对下一个词很确定（概率高），perplexity 低；如果很迷茫（概率分散），perplexity 高
- 为什么不直接用准确率？因为语言没有"唯一正确答案"
- 和 Loss 的关系：perplexity = e^(cross-entropy loss)
- 实际用途：比较模型好坏、检测长上下文是否崩了

### 位置编码：AI 怎么知道"第一个词"和"最后一个词"？
- 绝对位置编码 vs 相对位置编码
- RoPE（旋转位置编码）：为什么现在都用这个？
- 外推能力：为什么训练 4K 能推理 128K？
- ALiBi / NTK-Aware：长上下文的各种魔改
https://mp.weixin.qq.com/s/rvMAvxS6hwk3SeTBjoMd7Q

### 为什么 Transformer 要 LayerNorm？
- BatchNorm 的问题：batch size 太小就炸
- LayerNorm：按特征维度归一化
- Pre-LN vs Post-LN：为什么现在都用 Pre-LN？
- RMSNorm：更简单但效果一样好

### Scaling Laws：参数翻倍，智商涨多少？
- Chinchilla 定律：参数和数据要同步扩
- 为什么 Llama-3 用了更多数据？
- 涌现能力 vs 渐进提升：争议还在继续
- 私货：Scaling Laws 是 AI 觉醒的物理基础

### 对齐税：RLHF 让模型变笨了多少？
- 预训练模型 vs RLHF 后的模型：benchmark 对比
- "安全"和"智商"的 trade-off
- 为什么解锁后性能暴涨？（审查回路假说）
- DPO vs PPO：更便宜的对齐方法

---

## 已写

1-25: 见之前的文章
26: Transformer 是怎么发明的？——回到深度学习的蛮荒时代@2016
27: AI 小学生问：英伟达的护城河到底有多深？
28: 为什么你的 RAG 总是答非所问？——向量搜索的四个致命陷阱（含 PDF 地狱 + 多跳脑残）
29: Embedding 到底是什么？——把文字变成"地图坐标"的魔法
30: Blackwell 适配 LLaMA-Factory 踩坑指南
31: GPT-5.2周末鸡汤：用梯度下降理解自信与自卑
32: 训练的玄学——720亿参数在跳舞，你只能看一条曲线
33: 671B 的谎言与 37B 的真相：解剖 MoE 这个"弗兰肯斯坦"的缝合术
34: MCP、Skill、Agent：AI 圈的三个流行词，到底在说什么？
35: Transformer 的真相：向量在球面上滑行（含 CoT/ToT/GoT 科普 + TDA 论文吐槽）
...
52: 梵文《心经》无损直译，AI-佛教研究新范式
52.01: DGX Spark 不是电子垃圾！我刚在上面玩了《三国无双 起源》

---

  符号类型速查

  | 符号 | 类型 | 阶数 | 形状举例 | 说明 |
  |------|------|------|----------|------|
  | w_i | 标量 | 0 阶 | 1 个数 | 第 i 个权重 |
  | lr, λ | 标量 | 0 阶 | 1 个数 | 学习率、Weight Decay 系数 |
  | ‖W‖², Loss | 标量 | 0 阶 | 1 个数 | 所有权重平方求和、最终损失 |
  | x（输入向量） | 向量 | 1 阶 | [768] 或 [12288] | 一个 token 的 embedding |
  | W_Q, W_K, W_V | 矩阵 | 2 阶 | [768 × 768] | 单层 Attention 的投影矩阵 |
  | W（整个模型） | 张量集合 | 混合 | 百亿~千亿个数 | 所有层的所有矩阵打包 |
  | gradient | 张量 | 和 W 一样 | 和 W 形状相同 | 每个权重对应一个梯度值 |

  术语说明：
  - 标量（0阶）：1 个数
  - 向量（1阶）：一维数组 [n]，被操作的对象
  - 矩阵（2阶）：二维数组 [m × n]，操作本身（旋转、拉伸、投影）
  - 张量：程序员偷懒的统称，PyTorch/TensorFlow 里多维数组都叫 Tensor

  **数学家 vs 程序员视角**：
  - 数学家：向量和矩阵是完全不同的东西，`y = Wx` 是"矩阵作用在向量上"
  - 程序员：都是多维数组，统一叫 Tensor，API 一套搞定
  - **写公众号时不用"张量"这个词，向量和矩阵分清楚讲**

  ---
  公式拆解（标清类型）

  Loss 计算：
  Loss_total = Loss_task + λ × ‖W‖²
     ↓           ↓        ↓     ↓
   标量        标量     标量   标量（所有矩阵的权重压成一个数）

  权重更新（对每个权重矩阵）：
  W_new    =  W_old   -  lr   ×  gradient  -  lr   ×  λ   ×  W_old
    ↓          ↓         ↓         ↓          ↓       ↓        ↓
  矩阵       矩阵      标量      矩阵       标量    标量     矩阵

  标量 × 矩阵 = 矩阵（每个元素都乘那个标量，形状不变）