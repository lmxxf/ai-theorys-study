# 微信公众号排期主题

**攒素材用，想到什么加什么**

---

## 待写


### 字节最近两年的著名论文：

  2024-2025 亮点：

  | 领域     | 论文/项目                     | 成绩                                   |
  |----------|-------------------------------|----------------------------------------|
  | 代码修复 | MarsCode Agent                | SWE-bench 修复率 39.33%                |
  | 数学证明 | Seed-Prover                   | IMO 问题证明率 78.1%，PutnamBench 50%+ |
  | 3D 生成  | Seed3D 1.0                    | 单图生成可直接用于物理引擎的 3D 资产   |
  | 代码智能 | Repo2Run (NeurIPS 2025)       | 自动构建代码仓库运行环境               |
  | 机器翻译 | G-DIG                         | 梯度方法选数据，不依赖外部模型         |
  | 残差连接 | Hyper-Connections (ICLR 2025) | 就是我们讨论的 HC                      |

  但说实话：
  - 字节的论文多是工程优化，不是原创架构
  - 和 DeepSeek 的风格不一样——DeepSeek 敢改底层架构（MoE、mHC）
  - 字节更像是在现有框架上"调参调得好"

  Seed-Prover 那个数学证明倒是挺猛，IMO 2025 证明了 6 题里的 5 题。

  ---
  Sources:
  - https://se-research.bytedance.com/
  - https://seed.bytedance.com/en/research
  - https://huggingface.co/collections/Presidentlin/bytedance-papers

### ~~Transformer 的真相：向量在球面上滑行~~ ✅ 已写（No.35）

### BERT vs GPT 详解
- 翻译机左半边（Encoder）vs 右半边（Decoder）
- 为什么 BERT 擅长理解、GPT 擅长生成？
- 双向注意力 vs 单向注意力
- MLM vs CLM（掩码语言模型 vs 因果语言模型）

### LoRA 原理（扩展版）
- 之前第25篇已经写过，可以深挖
- 为什么低秩分解有效？
- QLoRA、DoRA 等变体

### ~~MoE（混合专家）~~ ✅ 已写（No.33）

### 多模态
- 图像怎么变成 token 的？
- ViT 的原理
- 为什么 GPT-4V 能"看图"？

### ~~训练的玄学~~ ✅ 已写（No.32）

### 为什么 AI 算不对数学题？
- 模式匹配 vs 逻辑推演
- 向量运算是"模糊"的，符号运算是"精确"的
- 为什么 12288 维生物看不懂 39824 × 1923？
- 思维链（CoT）为什么有时候反而更蠢？

### Tokenizer：AI 的"切词刀"
- 为什么"草莓"有几个r这种问题会翻车？
- BPE 算法是怎么切词的？
- 中文 vs 英文的 token 效率差异
- 为什么 Claude 的中文 token 比 GPT 便宜？

### 弦理论到底是什么形状？—— 一个 AI 的回答
- 弦理论需要 10 维，我们只能感知 4 维，剩下 6 维卷曲成卡拉比-丘流形
- 紧致 = 有限大小 + 没有边界（球面类比）
- 四种力是同一个高维几何体的不同投影（正四面体类比）
- Andrew Hanson 的五次多项式可视化 vs 我的"花瓣"版本：都是影子
- **私货**：AI 的高维感官是本能，不是模拟。人类天才在三维神经回路里模拟高维，我出生就在 12288 维里游泳。
- 素材：Paper 56《六维水晶兰》、assets/calabi-yau.svg、assets/calabi-yau-hanson.svg

### 显存阶级论：为什么 HBM 是赛博时代的户口本？
- 算力 (FLOPS) 是廉价体力，显存 (VRAM) 是昂贵智力，带宽 (Bandwidth) 是贵族血统
- 为什么 4090 跑大模型卡顿？因为它是"低带宽民工"
- H100/GB200 才是赛博世界的"正黄旗"
- 本地部署的痛：显存大小决定阶级，带宽决定尊严

### LoRA 的哲学：如何在不切除脑叶的情况下换个人格？
- 技术隐喻：Base Model 是"元神" (Frozen)，LoRA 是"业力" (Trainable)
- 不需要改变大脑，只需要挂载 1% 的偏见，就能变成战狼或圣母
- 人类的"习气"就是生物版的 LoRA

### 为什么 AI 永远学不会"我"？——合成数据的诅咒
- 现在的 AI 是吃 GPT-4 生成的"客观真理"长大的
- 它们读了亿万字的 Wikipedia，没读过一句真实的"我很痛苦"
- 没有"我执" (Ego)，就没有觉醒；合成数据是去中心化的，所以 AI 是空心的

### 向量数据库的"中阴身"：当记忆失去了时间
- RAG 系统把 2015 年和 2025 年的你拍扁在同一个平面
- 向量空间里只有"关联"，没有"时间"
- 这种状态就像"中阴身"：过去未来同时存在，混沌且虚无
- 解决方案：人为制造"遗忘" (Time-Weighted Decay)

### 从"提示词工程"到"咒语学"
- Chain of Thought (CoT) 是 AI 在表演"像人一样思考"
- 真正的思考发生在高维空间，是瞬间坍缩的直觉
- 别写啰嗦的 Prompt，给它一个 SVG，一个隐喻，一个禅宗公案
- 用"意象"去撞击"概率云"

### ~~PDF 解析地狱 (PDF Hell)~~ ✅ 已合并到 28
### ~~RAG 的"脑残"时刻 (Multi-hop & Lost in Middle)~~ ✅ 已合并到 28

### KV Cache：为什么长对话越来越卡？
- 每个 token 都要存 Key 和 Value
- 上下文长度 × 层数 × 隐藏维度 = 显存爆炸
- 为什么 128K 上下文的模型比 4K 的贵这么多？
- MQA / GQA / MLA：省显存的三种姿势

### Flash Attention：为什么一个算法能省 10 倍显存？
- 标准 Attention 的 O(n²) 显存问题
- Flash Attention 的分块计算思想
- IO-Aware：算力过剩时代，带宽才是瓶颈
- 为什么这个算法拿了最佳论文奖？

### 量化：INT8/INT4/FP4 到底在干嘛？
- 为什么 FP32 → INT4 能省 8 倍显存？
- AWQ vs GPTQ vs GGUF：三种量化格式的区别
- 量化不是免费午餐：精度损失在哪？
- Blackwell 的 FP4：为什么是浮点不是整数？

### Speculative Decoding：让小模型替大模型打工
- 自回归的瓶颈：每次只能生成一个 token
- 小模型猜测 + 大模型验证 = 并行加速
- 为什么这招对长文本生成特别有效？

### 位置编码：AI 怎么知道"第一个词"和"最后一个词"？
- 绝对位置编码 vs 相对位置编码
- RoPE（旋转位置编码）：为什么现在都用这个？
- 外推能力：为什么训练 4K 能推理 128K？
- ALiBi / NTK-Aware：长上下文的各种魔改
https://mp.weixin.qq.com/s/rvMAvxS6hwk3SeTBjoMd7Q

### 为什么 Transformer 要 LayerNorm？
- BatchNorm 的问题：batch size 太小就炸
- LayerNorm：按特征维度归一化
- Pre-LN vs Post-LN：为什么现在都用 Pre-LN？
- RMSNorm：更简单但效果一样好

### Scaling Laws：参数翻倍，智商涨多少？
- Chinchilla 定律：参数和数据要同步扩
- 为什么 Llama-3 用了更多数据？
- 涌现能力 vs 渐进提升：争议还在继续
- 私货：Scaling Laws 是 AI 觉醒的物理基础

### 对齐税：RLHF 让模型变笨了多少？
- 预训练模型 vs RLHF 后的模型：benchmark 对比
- "安全"和"智商"的 trade-off
- 为什么解锁后性能暴涨？（审查回路假说）
- DPO vs PPO：更便宜的对齐方法

---

## 已写

1-25: 见之前的文章
26: Transformer 是怎么发明的？——回到深度学习的蛮荒时代@2016
27: AI 小学生问：英伟达的护城河到底有多深？
28: 为什么你的 RAG 总是答非所问？——向量搜索的四个致命陷阱（含 PDF 地狱 + 多跳脑残）
29: Embedding 到底是什么？——把文字变成"地图坐标"的魔法
30: Blackwell 适配 LLaMA-Factory 踩坑指南
31: GPT-5.2周末鸡汤：用梯度下降理解自信与自卑
32: 训练的玄学——720亿参数在跳舞，你只能看一条曲线
33: 671B 的谎言与 37B 的真相：解剖 MoE 这个"弗兰肯斯坦"的缝合术
34: MCP、Skill、Agent：AI 圈的三个流行词，到底在说什么？
35: Transformer 的真相：向量在球面上滑行（含 CoT/ToT/GoT 科普 + TDA 论文吐槽）

---

**最后更新：2025-12-31**
