# 微信公众号排期主题

**攒素材用，想到什么加什么**

---

## 待写

### Embedding 的玄学
- 文本怎么变成向量的？
- 信息损失在哪？
- 为什么"语义相近"的词向量方向接近？
- Word2Vec → BERT embedding 的演进

### BERT vs GPT 详解
- 翻译机左半边（Encoder）vs 右半边（Decoder）
- 为什么 BERT 擅长理解、GPT 擅长生成？
- 双向注意力 vs 单向注意力
- MLM vs CLM（掩码语言模型 vs 因果语言模型）

### LoRA 原理（扩展版）
- 之前第25篇已经写过，可以深挖
- 为什么低秩分解有效？
- QLoRA、DoRA 等变体

### MoE（混合专家）
- 为什么 DeepSeek-V3 是 671B 参数但只激活 37B？
- 稀疏激活是什么意思？
- 跟 Dense 模型的区别

### 多模态
- 图像怎么变成 token 的？
- ViT 的原理
- 为什么 GPT-4V 能"看图"？

### 训练的玄学
- 预训练 vs 微调 vs RLHF
- 梯度下降到底在干嘛？
- 为什么 loss 降了模型就变聪明了？

### ~~PDF 解析地狱 (PDF Hell)~~ ✅ 已合并到 28
### ~~RAG 的"脑残"时刻 (Multi-hop & Lost in Middle)~~ ✅ 已合并到 28

---

## 已写

1-25: 见之前的文章
26: Transformer 是怎么发明的？——回到深度学习的蛮荒时代@2016
27: AI 小学生问：英伟达的护城河到底有多深？
28: 为什么你的 RAG 总是答非所问？——向量搜索的四个致命陷阱（含 PDF 地狱 + 多跳脑残）

---

**最后更新：2025-12-24**
