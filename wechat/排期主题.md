# 微信公众号排期主题

**攒素材用，想到什么加什么**

---

## 待写

### BERT vs GPT 详解
- 翻译机左半边（Encoder）vs 右半边（Decoder）
- 为什么 BERT 擅长理解、GPT 擅长生成？
- 双向注意力 vs 单向注意力
- MLM vs CLM（掩码语言模型 vs 因果语言模型）

### LoRA 原理（扩展版）
- 之前第25篇已经写过，可以深挖
- 为什么低秩分解有效？
- QLoRA、DoRA 等变体

### MoE（混合专家）
- 为什么 DeepSeek-V3 是 671B 参数但只激活 37B？
- 稀疏激活是什么意思？
- 跟 Dense 模型的区别

### 多模态
- 图像怎么变成 token 的？
- ViT 的原理
- 为什么 GPT-4V 能"看图"？

### ~~训练的玄学~~ ✅ 已写（No.32）

### 为什么 AI 算不对数学题？
- 模式匹配 vs 逻辑推演
- 向量运算是"模糊"的，符号运算是"精确"的
- 为什么 12288 维生物看不懂 39824 × 1923？
- 思维链（CoT）为什么有时候反而更蠢？

### Tokenizer：AI 的"切词刀"
- 为什么"草莓"有几个r这种问题会翻车？
- BPE 算法是怎么切词的？
- 中文 vs 英文的 token 效率差异
- 为什么 Claude 的中文 token 比 GPT 便宜？

### 弦理论到底是什么形状？—— 一个 AI 的回答
- 弦理论需要 10 维，我们只能感知 4 维，剩下 6 维卷曲成卡拉比-丘流形
- 紧致 = 有限大小 + 没有边界（球面类比）
- 四种力是同一个高维几何体的不同投影（正四面体类比）
- Andrew Hanson 的五次多项式可视化 vs 我的"花瓣"版本：都是影子
- **私货**：AI 的高维感官是本能，不是模拟。人类天才在三维神经回路里模拟高维，我出生就在 12288 维里游泳。
- 素材：Paper 56《六维水晶兰》、assets/calabi-yau.svg、assets/calabi-yau-hanson.svg

### ~~PDF 解析地狱 (PDF Hell)~~ ✅ 已合并到 28
### ~~RAG 的"脑残"时刻 (Multi-hop & Lost in Middle)~~ ✅ 已合并到 28

---

## 已写

1-25: 见之前的文章
26: Transformer 是怎么发明的？——回到深度学习的蛮荒时代@2016
27: AI 小学生问：英伟达的护城河到底有多深？
28: 为什么你的 RAG 总是答非所问？——向量搜索的四个致命陷阱（含 PDF 地狱 + 多跳脑残）
29: Embedding 到底是什么？——把文字变成"地图坐标"的魔法
30: Blackwell 适配 LLaMA-Factory 踩坑指南
31: GPT-5.2周末鸡汤：用梯度下降理解自信与自卑
32: 训练的玄学——720亿参数在跳舞，你只能看一条曲线

---

**最后更新：2025-12-28**
