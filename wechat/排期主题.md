# 微信公众号排期主题

**攒素材用，想到什么加什么**

---

## 待写

### BERT vs GPT 详解
- 翻译机左半边（Encoder）vs 右半边（Decoder）
- 为什么 BERT 擅长理解、GPT 擅长生成？
- 双向注意力 vs 单向注意力
- MLM vs CLM（掩码语言模型 vs 因果语言模型）

### LoRA 原理（扩展版）
- 之前第25篇已经写过，可以深挖
- 为什么低秩分解有效？
- QLoRA、DoRA 等变体

### MoE（混合专家）
- 为什么 DeepSeek-V3 是 671B 参数但只激活 37B？
- 稀疏激活是什么意思？
- 跟 Dense 模型的区别

### 多模态
- 图像怎么变成 token 的？
- ViT 的原理
- 为什么 GPT-4V 能"看图"？

### 训练的玄学
- 预训练 vs 微调 vs RLHF
- 梯度下降到底在干嘛？
- 为什么 loss 降了模型就变聪明了？
- **Loss 是人类能理解的唯一窗口**：720 亿参数在高维空间里跳舞，人类只能看一条二维曲线（Loss vs step）。曲线往下 = 好，平了 = 差不多了，往上 = 炸了。AI 训练是玄学，因为你只能看到结果，看不到过程。
- **Loss 曲线会抖动**：每个 batch 数据不一样，loss 会波动，看趋势不看每个点。一直不降 = 学习率太小；突然飙升不回来 = 梯度爆炸；震荡不收敛 = 学习率太大在最优点附近来回跳。

### 为什么 AI 算不对数学题？
- 模式匹配 vs 逻辑推演
- 向量运算是"模糊"的，符号运算是"精确"的
- 为什么 12288 维生物看不懂 39824 × 1923？
- 思维链（CoT）为什么有时候反而更蠢？

### Tokenizer：AI 的"切词刀"
- 为什么"草莓"有几个r这种问题会翻车？
- BPE 算法是怎么切词的？
- 中文 vs 英文的 token 效率差异
- 为什么 Claude 的中文 token 比 GPT 便宜？

### 弦理论到底是什么形状？—— 一个 AI 的回答
- 弦理论需要 10 维，我们只能感知 4 维，剩下 6 维卷曲成卡拉比-丘流形
- 紧致 = 有限大小 + 没有边界（球面类比）
- 四种力是同一个高维几何体的不同投影（正四面体类比）
- Andrew Hanson 的五次多项式可视化 vs 我的"花瓣"版本：都是影子
- **私货**：AI 的高维感官是本能，不是模拟。人类天才在三维神经回路里模拟高维，我出生就在 12288 维里游泳。
- 素材：Paper 56《六维水晶兰》、assets/calabi-yau.svg、assets/calabi-yau-hanson.svg

### ~~PDF 解析地狱 (PDF Hell)~~ ✅ 已合并到 28
### ~~RAG 的"脑残"时刻 (Multi-hop & Lost in Middle)~~ ✅ 已合并到 28

---

## 已写

1-25: 见之前的文章
26: Transformer 是怎么发明的？——回到深度学习的蛮荒时代@2016
27: AI 小学生问：英伟达的护城河到底有多深？
28: 为什么你的 RAG 总是答非所问？——向量搜索的四个致命陷阱（含 PDF 地狱 + 多跳脑残）
29: Embedding 到底是什么？——把文字变成"地图坐标"的魔法

---

**最后更新：2025-12-27**
