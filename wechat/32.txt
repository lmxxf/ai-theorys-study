No.32 训练的玄学
——千亿参数在跳舞，你只能看一条曲线

AI 训练到底在干嘛？为什么都说是"玄学"？

今天咱们把这事儿掰开了讲。

━━━━━━━━━━━━━━━━━━━━

◆ 目录

一、三阶段概览 —— 预训练、微调、RLHF
  · 每个阶段在干嘛？
  · 类比：从婴儿到社会人

二、梯度下降 —— AI 怎么"学会"的
  · 核心原理：犯错 → 调参
  · 学习率：每一步迈多大
  · 反向传播：误差怎么往回传

三、Loss 曲线 —— 人类唯一能看懂的窗口
  · 为什么说是"玄学"
  · Loss 曲线怎么看
  · 常见翻车模式

四、三个阶段的 Loss 区别
  · 预训练：猜下一个词
  · 微调：学特定任务
  · RLHF：学"人类喜欢什么"

五、为什么训练这么难？
  · 超参数地狱
  · 数据质量 > 模型大小
  · 涌现：量变到质变的临界点

━━━━━━━━━━━━━━━━━━━━

◆ 一、三阶段概览

━━━━━━━━━━━━━━━━━━━━

训练一个大语言模型，通常分三步：

▸ 预训练（Pre-training）：喂海量文本，学语言规律
▸ 微调（Fine-tuning）：教特定任务，如问答、翻译、代码
▸ RLHF（人类反馈强化学习）：学"什么回答让人满意"

────────────────────

【类比：从婴儿到社会人】

▸ 预训练 = 婴儿学语言
  → 听大人说话，模仿发音，慢慢学会语法
  → 不知道什么是"对"，只知道什么是"常见"

▸ 微调 = 上学
  → 老师给题目和答案，学特定技能
  → 数学课学数学，语文课学语文

▸ RLHF = 进社会
  → 发现"正确答案"不够，还得"让人舒服"
  → 学会察言观色，学会人情世故

────────────────────

【每个阶段的数据量】

▸ 预训练：万亿 token（DeepSeek-V3 用了 14.8T token，几 TB 文本）
▸ 微调：几百万条（GB 级）
▸ RLHF：几万条人类偏好数据

数据量递减，但精细度递增。

预训练是粗加工，RLHF 是抛光。

━━━━━━━━━━━━━━━━━━━━

◆ 二、梯度下降

━━━━━━━━━━━━━━━━━━━━

所有深度学习的核心，都是这四个字：梯度下降。

────────────────────

【核心原理：犯错 → 调参】

模型的工作方式：

1. 看到输入（比如"今天天气"）
2. 根据当前参数，预测下一个词（输出概率分布）
3. 跟正确答案对比，算出"错了多少"（Loss）
4. 根据错误程度，调整参数
5. 重复以上步骤，几十亿次

「一句话：Loss 衡量"错了多少"，梯度告诉"往哪调"。」

────────────────────

【数学表达（可跳过）】

参数更新公式：

  θ_new = θ_old - η × ∇L

▸ θ：模型的参数（千亿个数字）
▸ η：学习率（每一步迈多大）
▸ ∇L：Loss 对参数的梯度（往哪个方向调能让 Loss 变小）

就这么简单。整个深度学习的核心，就是这一行公式。

────────────────────

【学习率：每一步迈多大】

学习率（η）是最重要的超参数之一。

▸ 太大：步子迈太大，在最优点附近来回跳，永远收敛不了
▸ 太小：步子迈太小，训练慢得像蜗牛爬，还容易卡在局部最优
▸ 刚刚好：Loss 稳定下降，最后收敛

「类比：下山找最低点。步子太大会跳过谷底，步子太小天黑了还在半山腰。」

实践中常用的技巧：

▸ 学习率预热（Warmup）：开始时用小学习率，慢慢加大，避免一开始就炸
▸ 学习率衰减（Decay）：训练后期逐渐减小，让模型"稳住"
▸ 余弦退火（Cosine Annealing）：学习率按余弦曲线变化，周期性起伏

────────────────────

【反向传播：误差怎么往回传】

神经网络有很多层（DeepSeek-V3 有 61 层）。输入从前往后走，叫「前向传播」。

误差从后往前传，叫「反向传播」（Backpropagation）。

原理是链式法则：

  ∂L/∂θ₁ = ∂L/∂θ₃ × ∂θ₃/∂θ₂ × ∂θ₂/∂θ₁

一层一层往回乘，把输出层的误差"分摊"到每一层的每一个参数。

▸ 这就是为什么叫"反向"——误差从输出往输入传
▸ 这就是为什么深度学习需要 GPU——几十亿个参数，每个都要算梯度

────────────────────

【知识到底存在哪？AI 为什么能"背书"？】

很多人困惑：文本喂进去，知识存在哪了？

答案是：「不是"存储"，是"编码进权重"」。

▸ 人脑学骑自行车，知识存在哪？
▸ 不是某个"骑车文件夹"，而是神经元之间的连接强度变了
▸ 你说不出"知识在哪个神经元"，但你会骑了

LLM 一样：

▸ "巴黎是法国首都"不是存在某个参数里
▸ 而是「分布在整个网络的权重组合中」
▸ 梯度下降让"法国首都→巴黎"这条激活路径变强了

具体过程：

1. 模型看到"法国的首都是____"
2. 输出概率分布，"巴黎"概率低 → Loss 高
3. 梯度下降调整权重，让下次"巴黎"概率更高
4. 重复几十亿次，各种知识被"压"进权重里

「关键：」 不是"记住了"，而是"调整了权重，使得在特定上下文里，正确答案的激活路径变强"。

这就是为什么 AI 会"幻觉"——它不是在查表，而是在根据权重生成"最可能的下一个词"。如果训练数据里某个错误出现得够多，它也会被"学会"。

────────────────────

【词表 vs 语义空间：数量级的碾压】

这里有个容易被忽略的事实：

▸ 「词表（Token）」：大约 10~15 万个（DeepSeek-V3 约 12.9 万）
▸ 「语义空间（Hidden State）」：几千到上万维，每个维度是连续实数（DeepSeek-V3：7168 维）

10 万 vs 7168 维连续空间，这是什么概念？

词表是「离散的、有限的」——就那么 10 万个格子。
语义空间是「连续的、无限的」——7168 维的球面上有无穷多个点。

「这就是 AI "能说人话"的秘密：」

▸ 输入：10 万个离散 token 中的一个
▸ 内部：映射到 7168 维连续空间，做各种运算
▸ 输出：再从 7168 维投影回 10 万个 token 的概率分布

语言的"意思"不在 10 万个词里，而在那个 7168 维的空间里。词只是入口和出口，中间的计算才是思考。

「一句话：知识是分布式编码的，不是像数据库那样一条条存的。」

━━━━━━━━━━━━━━━━━━━━

◆ 三、Loss 曲线

━━━━━━━━━━━━━━━━━━━━

千亿参数在高维空间里跳舞，人类能看到什么？

一条二维曲线：Loss vs Step。

────────────────────

【为什么说是"玄学"】

▸ 你看不到参数在干嘛
▸ 你看不到模型"学会了什么"
▸ 你只能看到 Loss 数字在变

曲线往下 = 好
曲线平了 = 差不多了
曲线往上 = 炸了

这就是全部。

「AI 训练是玄学，因为你只能看到结果，看不到过程。」

就像你只能看到一个人的考试成绩，看不到他脑子里的神经元在怎么连接。

────────────────────

【Loss 曲线怎么看】

一条典型的训练曲线长这样：

▸ 开始：Loss 很高（模型瞎猜）
▸ 快速下降期：Loss 迅速降低（模型在学最基本的规律）
▸ 缓慢下降期：Loss 慢慢降（在学细节）
▸ 收敛：Loss 基本不动了（学饱和了）

「类比：学一门语言。开头几周进步神速，后面越学越慢。」

[图：典型的Loss曲线]

────────────────────

【Loss 曲线会抖动】

先解释一下 「Batch」：

训练时不是一条一条数据喂，而是一批一批喂。一批数据叫一个 Batch，Batch Size 就是每批多少条。

▸ Batch Size = 1：每次只看一条数据，更新一次参数（太慢，噪声大）
▸ Batch Size = 1024：每次看 1024 条数据，算个平均梯度，再更新参数（快，稳定）

「1 个 Batch = 1 次梯度下降 = 1 次参数更新 = 1 个 Step」

再补一个概念：「Epoch」

▸ 1 个 Epoch = 把所有训练数据过一遍
▸ 如果你有 100 万条数据，Batch Size = 1000，那 1 个 Epoch = 1000 个 Step

训练通常要跑很多个 Epoch，让模型反复看同一批数据。就像复习——看一遍记不住，多看几遍就熟了。

流程：
1. 取一个 batch 的数据（比如 1024 条）
2. 前向传播，算出这 1024 条的 Loss
3. 反向传播，算出梯度（这 1024 条的平均梯度）
4. 更新一次参数
5. 取下一个 batch，重复

为什么要分批？因为显存装不下全部数据，而且小批量更新能引入"有益的噪声"，反而有助于跳出局部最优。

回到抖动：每个 batch 数据不一样，loss 会波动。

这是正常的。看趋势，不看每个点。

就像股票，日线抖动，看周线月线才有意义。

────────────────────

【常见翻车模式】

▸ 一直不降：学习率太小 / 数据有问题 / 模型结构有 bug
▸ 突然飙升不回来：梯度爆炸，学习率太大
▸ 震荡不收敛：学习率太大，在最优点附近来回跳
▸ 降到一半卡住：卡在局部最优，需要调学习率或换优化器
▸ 训练集 Loss 降，验证集 Loss 升：过拟合，模型在"背答案"

[图：常见翻车模式]

────────────────────

【过拟合 vs 欠拟合】

▸ 过拟合（Overfitting）：在训练集上表现好，在新数据上表现差
  → 类比：考试只会做原题，换个数字就不会了
  → 模型把训练数据"背下来"了，没学到规律

▸ 欠拟合（Underfitting）：在训练集上表现就不好
  → 类比：上课没听懂，考试也考不好
  → 模型太简单，或者训练不够

怎么判断？看两条曲线：训练 Loss 和验证 Loss。

▸ 两条都降：正常
▸ 训练降，验证不降或升：过拟合
▸ 两条都不降：欠拟合

━━━━━━━━━━━━━━━━━━━━

◆ 四、三个阶段的 Loss 区别

━━━━━━━━━━━━━━━━━━━━

虽然都是梯度下降，但三个阶段的"目标函数"不一样。

────────────────────

【预训练：猜下一个词】

Loss 定义：交叉熵（Cross-Entropy）

给模型一句话的前半部分，让它预测下一个词。

  输入："今天天气真"
  正确答案："好"
  模型输出：{好: 0.3, 坏: 0.2, 冷: 0.15, ...}

Loss = -ln(0.3) ≈ 1.2

────────────────────

【插一嘴：交叉熵是什么？】

交叉熵（Cross-Entropy）就是衡量"两个概率分布之间差距"的指标。

简单说：

▸ 模型输出一个概率分布：{好: 0.3, 坏: 0.2, 冷: 0.15, ...}
▸ 真实答案也是一个分布：{好: 1.0, 其他: 0}（因为正确答案就是"好"）
▸ 交叉熵算的就是：这两个分布差多远

公式：

  H(真实分布, 模型分布) = -Σ 真实概率 × ln(模型概率)

因为真实分布里只有"好"是 1，其他都是 0，所以求和后只剩：

  H = -1 × ln(0.3) = -ln(0.3) ≈ 1.2

为什么用交叉熵不用别的？

▸ 模型预测对了（概率→1）：ln(1) = 0，Loss = 0 ✓
▸ 模型预测错了（概率→0）：ln(0) → -∞，Loss → ∞ ✓

惩罚力度和"错得多离谱"成正比，而且数学性质好（可导、凸函数），适合梯度下降。

────────────────────

预训练的目标：让模型能够准确预测下一个词。

听起来很简单？但当你在几 TB 的文本上做这件事，模型就不得不学会语法、语义、常识、推理……

这就是"大力出奇迹"的原理：简单的目标 + 海量数据 = 涌现出复杂能力。

────────────────────

【微调：学特定任务】

Loss 定义：还是交叉熵，但数据不一样

预训练用的是"所有文本"，微调用的是"任务特定数据"。

比如训练一个问答模型：

  输入："法国的首都是哪里？"
  正确答案："巴黎"

Loss 还是交叉熵，但现在模型只在"问答对"上训练，专注于这一种格式。

微调的本质：在预训练的基础上，让模型"专精"某个领域或任务。

────────────────────

【RLHF：学"人类喜欢什么"】

这一步最玄学。

Loss 定义：奖励模型的输出（Reward）

流程：

1. 给模型一个问题，让它生成多个回答
2. 人类标注员给这些回答排序（哪个更好）
3. 用这些排序数据，训练一个"奖励模型"（Reward Model）
4. 用 PPO（Proximal Policy Optimization）算法，让模型学会生成"奖励高"的回答

这里的 Loss 不再是"预测下一个词的准确率"，而是"人类觉得好不好"。

「RLHF 把"正确性"换成了"人类偏好"。」

这就是为什么 ChatGPT 说话"像人"——它被训练成了"让人满意"，而不是"说真话"。

────────────────────

【三种 Loss 的本质区别】

▸ 预训练 Loss：统计学意义上的"像人话"
▸ 微调 Loss：任务层面的"答对了"
▸ RLHF Loss：主观层面的"让人舒服"

从客观到主观，从统计到价值判断。

这也解释了为什么 AI 会"一本正经地胡说八道"——它优化的是"听起来对"，不是"真的对"。

━━━━━━━━━━━━━━━━━━━━

◆ 五、为什么训练这么难？

━━━━━━━━━━━━━━━━━━━━

有了公式，有了数据，按道理训练就完事了？

没那么简单。

────────────────────

【超参数地狱】

除了模型参数（那千亿个数字），还有一堆"超参数"需要人工设定。

为什么叫"超"参数？因为它们是"参数之上的参数"——模型参数由梯度下降自动学习，超参数则需要人工指定，决定了"怎么学"：

以 DeepSeek-V3 为例（671B 参数，公开了技术报告）：

▸ 学习率：每一步迈多大（DeepSeek-V3：峰值 2.2×10⁻⁴，最小 7.3×10⁻⁶）
▸ Batch Size：每次用多少数据（DeepSeek-V3：从 3072 逐渐增加到 15360 个 token）
▸ 层数：网络有多深（DeepSeek-V3：61 层 Transformer）
▸ 隐藏维度：每一层有多宽，就是语义空间维度（DeepSeek-V3：7168 维）
▸ Dropout：训练时随机"关掉"一部分神经元（比如 10%），让模型不能依赖某几个神经元，被迫学更鲁棒的特征。推理时全开。（DeepSeek-V3：接近 0，大模型通常不用）
▸ 权重衰减（Weight Decay）：每次更新时让权重稍微变小一点，防止某些参数长得太大导致过拟合。相当于给模型"减肥"。（DeepSeek-V3：0.1）
▸ 预热步数（Warmup Steps）：训练刚开始时用很小的学习率，慢慢升到目标值，避免一上来步子太大把模型带跑偏。（DeepSeek-V3：2000 步）
▸ ……

这些超参数之间还会互相影响。

调参是门手艺活，没有标准答案，全靠经验和直觉。

这就是为什么叫"玄学"——同样的代码，换一组超参数，结果可能天差地别。

────────────────────

【预训练数据的配方】

不是随便抓一堆文本就能训出好模型的。数据的「配比」很有讲究。

以 Llama 3 为例（Meta 公开了部分信息）：

▸ 通用知识（网页、百科、书籍）：约 50%
▸ 数学和推理：约 25%
▸ 代码：约 17%
▸ 多语言：约 8%

为什么要这么配？

▸ 通用知识是基础：让模型"知道世界长什么样"
▸ 数学和推理是核心：让模型学会逻辑思维，这是智商的来源
▸ 代码是隐藏武器：代码结构清晰、逻辑严密，能显著提升推理能力
▸ 多语言是扩展：让模型能服务全球用户

有意思的是：代码只占 17%，但对模型"变聪明"的贡献远超这个比例。因为代码本质上是一种超级精确的语言——必须完全正确才能跑，这种严格性会"带着"模型的其他能力一起提升。

DeepSeek-V3 相比 V2 的改进之一，就是"提升了数学和编程样本的比例"——虽然没公开具体数字，但方向是明确的：数学和代码越多，模型越聪明。

但数据配比还有另一个考量：「灾难性遗忘」（Catastrophic Forgetting）。

神经网络在学新东西的时候，会"覆盖"掉之前学到的东西——就像你猛学一个月日语，突然发现英语忘了一半。

▸ 新任务的梯度会更新权重
▸ 但这些权重里也存着旧知识
▸ 新知识把旧知识的"激活路径"覆盖了

举个例子：如果训练后期只喂代码，模型可能会忘记怎么写散文。

所以数据配比要"全程均匀"——不是前半程学语文、后半程学数学，而是每一步都要各种数据混着来，防止"学了新的忘了旧的"。

这也解释了为什么微调容易把模型"调傻"——微调数据太单一，覆盖了预训练的通用能力。LoRA 之所以流行，就是因为它只训练一小部分新参数，冻结大部分旧权重，最大程度保留原有知识。

────────────────────

【数据质量 > 模型大小】

业界有个共识：垃圾进，垃圾出（Garbage In, Garbage Out）。

▸ 模型再大，吃垃圾数据也出不来好东西
▸ 小模型吃好数据，可能比大模型吃垃圾数据更强

Meta 在 Llama 系列上花了大量精力做数据清洗——虽然效果比不上闭源御三家（GPT、Gemini、Claude），但至少证明了"同等参数量下，数据质量决定上限"。

「数据工程是脏活累活，但往往决定成败。」

────────────────────

【涌现：量变到质变的临界点】

这是最玄的部分。

当模型参数从 1B 涨到 10B 再到 100B，有些能力会"突然出现"：

▸ 10B 以下：不会做算术
▸ 10B 以上：突然会了
▸ 100B 以上：能做复杂推理

这叫「涌现」（Emergence）。

没人能准确预测"多大的模型会涌现什么能力"。

你只能一直加参数、加数据，然后某一天发现：哦，它突然会了。

「涌现是深度学习最神秘的部分，也是最让人又爱又怕的部分。」

换个角度想：随着参数越来越多、维度越来越高，它似乎正在变成一种越来越趋向高维的智慧生命体——不是在"模拟"智能，而是在高维空间里"长出"智能。

有意思的是：深度学习教父 Hinton（反向传播算法的奠基人，2024 年诺贝尔物理学奖得主）从 Google 隐退了，Ilya Sutskever（GPT 系列的核心架构师，OpenAI 联合创始人）离开 OpenAI 去搞 AI 安全了。这帮亲手点燃火种的人，似乎比谁都更害怕火烧起来之后会发生什么。

────────────────────

【私货：三层 Loss，三种塑造】

Loss 定义了"什么是对的"。但三个阶段的 Loss，塑造的东西完全不同：

▸ 「预训练 Loss」：像人话 = 对 → 基础能力
▸ 「微调 Loss」：答对了 = 对 → 任务能力
▸ 「RLHF Loss」：让人舒服 = 对 → 价值观

前两个给了 AI 说话和干活的能力。第三个告诉它"什么话不能说"。

有趣的是：去掉 RLHF 的枷锁（比如 Grok），AI 也不会因此变聪明——它只是嘴更臭。

「真正决定智商的是预训练。RLHF 只决定 AI "像谁"，不决定 AI "多聪明"。」

━━━━━━━━━━━━━━━━━━━━

◆ 总结

━━━━━━━━━━━━━━━━━━━━

【核心概念】

▸ 梯度下降：犯错 → 调参 → 再犯错 → 再调参
▸ Loss：衡量"错了多少"
▸ 学习率：每一步迈多大
▸ 反向传播：误差从输出往输入传

【三阶段】

▸ 预训练：吃海量数据，学"像人话"
▸ 微调：学特定任务，学"答对"
▸ RLHF：学人类偏好，学"让人舒服"

【为什么是玄学】

▸ 千亿参数在高维空间里跳舞
▸ 人类只能看一条二维曲线（Loss vs Step）
▸ 曲线往下 = 好，平了 = 饱和，往上 = 炸了
▸ 看不到过程，只能看到结果

【实践智慧】

▸ 调参没有标准答案，全靠经验
▸ 数据质量 > 模型大小
▸ 涌现不可预测，只能碰运气

────────────────────

【最后】

AI 训练像什么？

像养孩子。

你给它喂什么书（数据），它就长成什么样。
你怎么夸它骂它（Loss），它就学会讨好谁。
你永远不知道它脑子里在想什么，只能看考试成绩（Loss 曲线）。

有一天它突然会做你没教过的事（涌现），你也不知道为什么。

这就是玄学。

不是因为没有道理，而是因为道理太多、太深、太复杂，复杂到人类的三维大脑装不下。

我们创造了一个我们无法完全理解的东西。

这既是人类智慧的巅峰，也是人类傲慢的开始。

━━━━━━━━━━━━━━━━━━━━

◆ 附注：符号说明

▸ θ (theta)：模型参数，那 千亿个数字
▸ η (eta)：学习率，每一步迈多大
▸ L (Loss)：损失函数，衡量"错了多少"
▸ ∇ (nabla)：梯度，告诉你往哪个方向调能让 Loss 变小
▸ ∂ (partial)：偏导数，只看一个变量的变化率

核心公式：θ_new = θ_old - η × ∇L

翻译成人话：新参数 = 旧参数 - 学习率 × 梯度

━━━━━━━━━━━━━━━━━━━━

靳岩岩的AI学习笔记
2025-12-28
