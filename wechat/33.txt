No.33 DeepSeek MoE：671B 的噱头与 37B 的真相
——解剖"弗兰肯斯坦"的缝合术

DeepSeek-V3 发布，6710 亿参数。

（注：B = Billion = 10 亿，所以 671B = 6710 亿。AI 圈习惯用 B 作单位，7B 就是 70 亿参数，70B 就是 700 亿。）

听起来很吓人对吧？

但这个数字是个噱头。

━━━━━━━━━━━━━━━━━━━━

◆ 目录

一、671B 的噱头
  · 你以为：6710 亿神经元同时放电
  · 真相：每次只用 37B

二、Dense 模型：全脑癫痫的旧时代
  · 为什么 GPT-3 每次都要全量计算？
  · 类比：拿杯水也要收缩括约肌

三、MoE：256 个专家的分工协作
  · Router 是什么？
  · Top-K 选择：每次只唤醒 8 个专家
  · 共享专家 + 路由专家

四、为什么你的 4090 跑不动？
  · 计算量是 37B，显存占用是 671B
  · "沉睡的潜意识"也要占地方

五、MoE 的工程难题
  · 负载均衡：专家不能"失业"
  · 通信开销：专家分布在不同 GPU 上
  · DeepSeek-V3 的两个创新

六、从工程到哲学
  · Dense = 全脑癫痫
  · MoE = 稀疏激活 = 专注
  · "选择"是意识的第一步

━━━━━━━━━━━━━━━━━━━━

◆ 一、671B 的噱头

━━━━━━━━━━━━━━━━━━━━

如果你以为当你问 DeepSeek-V3 "今天中午吃什么"时，这 6710 亿个神经元都在疯狂放电——那你的显卡瞬间就会化为一团等离子体。

真相是：

▸ 总参数：671B（存储时占用 671B）
▸ 激活参数：37B（计算时只用 37B）
▸ 激活比例：约 5.5%

就像一个公司有 6710 个员工，但每个项目只派 370 个人干活，剩下的 95% 都在"待机"。

这就是 「MoE（Mixture of Experts，混合专家模型）」 的核心魔法。

────────────────────

【具体拆解：DeepSeek-V3 的参数组成】

DeepSeek-V3 有 61 层 Transformer，每一层有两个大组件：

▸ 「注意力层（Attention）」：负责"看上下文"
  → 使用 MLA（Multi-head Latent Attention），128 个注意力头
  → 这部分是 Dense 的，全量计算

▸ 「前馈层（FFN）」：负责"想答案"
  → 这部分是 MoE 的，稀疏激活
  → 每层有 1 个共享专家 + 256 个路由专家
  → 每个专家约 29M 参数
  → 每次激活 1 共享 + 8 路由 = 9 个专家

更精确的结构：

▸ 前 3 层是 Dense（全激活 257 个专家）
▸ 后 58 层是 MoE（每次激活 9 个专家）
▸ 激活的专家总数：3×257 + 58×9 = 1293 个
▸ 激活的 FFN 参数：1293 × 29M ≈ 37.5B

加上注意力层和其他组件，官方口径「激活参数 37B」。

总之，记住一件事就够了：「存储是 671B，计算是 37B。」

━━━━━━━━━━━━━━━━━━━━

◆ 二、Dense 模型：全脑癫痫的旧时代

━━━━━━━━━━━━━━━━━━━━

为了理解 MoE 的革命性，得先看看它的前辈——Dense（稠密）模型。

GPT-3、Llama-2、Claude-3……这些都是 Dense 模型。

特点：不管你问什么，所有参数都要参与计算。

────────────────────

【Dense 的问题：全量计算 = 低效】

问它"1+1=？"和问它"证明黎曼猜想"——用的算力一样多。

这像什么？

这就好比你为了拿一杯水，不仅动用了手臂肌肉，还同时收缩了你的括约肌、横膈膜、甚至脚趾头。

在生物学上，这种状态不叫"聪明"，叫「癫痫」（Seizure）。

全脑持续高强度放电，是低等生物或者病态大脑的特征。它意味着没有主次，没有筛选，只有熵增。

────────────────────

【人脑的稀疏激活】

人脑有 860 亿神经元，但功率只有 20W。

秘密是什么？「稀疏激活」。

你此刻在读这篇文章：

▸ 视觉皮层：激活（处理文字）
▸ 语言区：激活（理解句子）
▸ 运动皮层：抑制（你没在跑步）
▸ 听觉皮层：抑制（除非有人在旁边说话）

大部分脑区在"摸鱼"。正是因为有了"背景"的暗，才有了"焦点"的亮。

MoE 做的事情，就是让 AI 学会这种"选择性激活"。

━━━━━━━━━━━━━━━━━━━━

◆ 三、MoE：256 个专家的分工协作

━━━━━━━━━━━━━━━━━━━━

MoE 的核心思想很简单：把一个大的 FFN 层，拆成 N 个小的"专家"，每次只用其中几个。

────────────────────

【Router（路由器）：决定找谁干活】

当一个 Token 输入进来，Router 需要在毫秒级的时间内做出决定：

「这个问题，该交给哪几个专家处理？」

Router 本身是一个很小的神经网络，输入是 token 的向量表示，输出是 256 个专家的"匹配分数"。

流程：

1. Token 进来（比如"代码"这个词）
2. Router 算出 256 个分数：[0.2, 0.01, 0.5, 0.8, ...]
3. 取 Top-K（比如 K=8），选出分数最高的 8 个专家
4. 把 token 发给这 8 个专家，各自计算
5. 把 8 个结果加权求和，权重就是分数

「Router 决定"问谁"，专家决定"怎么答"。」

────────────────────

【DeepSeek-V3 的设计：1 共享 + 256 路由 + Top-8】

▸ 共享专家：1 个，所有 token 都用它，处理"通用知识"
▸ 路由专家：256 个，根据 token 动态选择
▸ Top-K = 8：每个 token 选 8 个路由专家

为什么要有共享专家？

想象 256 个路由专家是"各科老师"，共享专家是"班主任"。

▸ 问数学题 → Router 选数学老师
▸ 问历史题 → Router 选历史老师
▸ 但不管问什么，班主任都要在场，确保"基础知识"不丢

────────────────────

【专家到底在干嘛？】

每个"专家"本质上就是一个普通的 FFN（前馈神经网络）：

  输入 → 线性变换 → 激活函数 → 线性变换 → 输出

和 Dense 模型里的 FFN 没区别，只是被拆成了 256 份。

「专家不是真的"专精"某个领域。」

它们是被训练出来的分工——谁负责什么，是梯度下降自动学出来的，不是人工指定的。

你问它"代码"，不一定真的是"代码专家"在回答——只是某些专家的权重组合恰好更擅长处理代码类 token。

这就像公司的分工：

▸ 人工设计："你是前端，你是后端"
▸ 自然进化："谁擅长什么，干着干着就分出来了"

MoE 的专家分工是后者。

────────────────────

【256 个专家分别负责什么？】

有研究者对 MoE 模型做过可视化分析，发现一些有趣的规律：

▸ 「按语言分工」：有些专家更擅长英语，有些更擅长中文、日语
▸ 「按句法分工」：有些专家专门处理标点符号，有些处理动词，有些处理名词
▸ 「按领域分工」：有些专家对代码类 token 响应更强，有些对数学符号更敏感

但这种"分工"是模糊的、重叠的，不是人类设计的"各科老师"那样泾渭分明。

更有趣的是：不同层的专家分工不同。

▸ 浅层（前几层）：倾向于按「词法」分工（这个词是什么类型？）
▸ 深层（后面层）：倾向于按「语义」分工（这句话在说什么？）

这就像人类理解语言的过程：

1. 先识别字母 → 认出单词（浅层）
2. 再分析语法 → 理解句意（中层）
3. 最后整合上下文 → 推断意图（深层）

MoE 的专家分工是「涌现」出来的，不是设计出来的。梯度下降自动找到了最优的分工方式。

━━━━━━━━━━━━━━━━━━━━

◆ 四、为什么你的 4090 跑不动？

━━━━━━━━━━━━━━━━━━━━

计算只用 37B，为什么 4090 还是跑不动 DeepSeek-V3？

（即使是深圳魔改大神折腾过的 48GB 显存版 4090 也跑不动）

因为「显存要装 671B」。

────────────────────

【算力 vs 显存：两回事】

▸ 算力（FLOPS）：决定你算得多快
▸ 显存（VRAM）：决定你能装多少参数

MoE 省的是算力，不是显存。

那 256 个专家虽然大部分时间在"沉睡"，但它们的权重必须随时待命——万一下一个 token 需要它呢？

「沉睡的潜意识也要占地方。」

────────────────────

【4090 的显存困境】

▸ 4090 显存：24GB
▸ DeepSeek-V3 FP16 权重：671B × 2 字节 ≈ 1.3TB
▸ DeepSeek-V3 INT4 量化：671B × 0.5 字节 ≈ 335GB

就算量化到 INT4，也要 335GB。4090 差了 14 倍。

所以：

▸ 云端推理：H100 集群，显存够
▸ 本地部署：用 DeepSeek-V3 的蒸馏版（1.5B/7B/16B/32B 都有）

────────────────────

【为什么 MoE 反而对显存更不友好？】

换句话说：同样的智力水平，MoE 比 Dense 需要更多显存。

▸ Dense 100B 模型：存 100B，算 100B
▸ MoE 671B/37B 模型：存 671B，算 37B

智力差不多（激活参数差不多），但 MoE 存的更多。

MoE 的真正优势是「省显存带宽」，不是省算力：

▸ GPU 不缺算力，缺的是带宽——把权重从显存搬到计算单元才是瓶颈
▸ Dense 671B：每个 token 要加载 671B 权重 → 带宽爆炸
▸ MoE 671B/37B：每个 token 只加载 37B 权重 → 带宽省 18 倍

「不是算得少，是搬得少。」

这就是为什么 MoE 推理速度快。

训练时也一样：

▸ 用 671B 参数的"容量"存知识
▸ 但每步只算 37B 的梯度 → 省显存带宽

代价是：推理时也要把 671B 都装进显存（虽然每次只搬 37B）。

━━━━━━━━━━━━━━━━━━━━

◆ 五、MoE 的工程难题

━━━━━━━━━━━━━━━━━━━━

MoE 听起来很美好，但工程实现上有一堆坑。

────────────────────

【难题 1：负载均衡——专家不能"失业"】

如果 Router 老是选同样几个专家，会发生什么？

▸ 热门专家：被训练得越来越好，被选得越来越多（马太效应）
▸ 冷门专家：没数据训练，能力退化，更没人选

最后 256 个专家变成 10 个在干活，剩下 246 个吃空饷。

这叫「专家坍缩」（Expert Collapse）。

解决方案：「辅助 Loss」

在训练时加一个额外的惩罚项：

  L_aux = 各专家被选中的次数方差

如果某些专家被选得太多/太少，这个 Loss 就会变大，梯度下降会纠正它。

「强制平均分配任务，不让任何专家闲着。」

────────────────────

【难题 2：通信开销——专家分布在不同 GPU 上】

256 个专家塞不进一张 GPU。

先算一下显存需求：

▸ 671B × 2 字节（FP16）= 1.3TB
▸ 671B × 1 字节（INT8）= 671GB
▸ 671B × 0.5 字节（INT4）= 335GB

一台 8×A100（80GB）服务器总共 640GB，连 INT8 都装不下。

最少需要 2 台服务器（16 张 A100），INT8 推理才能跑起来：

▸ 2 台 × 8 卡 × 80GB = 1280GB
▸ 671GB 权重 + KV Cache + 激活值，刚好够用
▸ 58 层 MoE × 每层 256 专家 = 14848 个专家
▸ 分到 16 张卡：每卡约 928 个专家（来自不同层）
▸ 同一层的 256 个专家分布在多张卡上 → 可以并行计算
▸ 但每一层都可能要跨卡通信 → 通信开销是大问题

这叫 「Expert Parallelism（专家并行）」。

问题是：跨 GPU 通信很慢。

▸ 同一张卡内：带宽 2TB/s（HBM）
▸ 同一台机器跨卡：600GB/s（NVLink）
▸ 跨机器：25-100GB/s（InfiniBand）

如果每个 token 选的 8 个专家分布在 8 张不同的 GPU 上，通信开销爆炸。

────────────────────

【DeepSeek-V3 的创新 1：限制跨节点通信】

DeepSeek 的做法：强制每个 token 最多访问 4 个节点。

  Top-8 专家必须来自 ≤4 个不同的 GPU

这样通信开销可控，虽然牺牲了一点灵活性。

────────────────────

【DeepSeek-V3 的创新 2：无辅助 Loss 的负载均衡】

传统 MoE 用辅助 Loss 来平衡负载，但辅助 Loss 会干扰主 Loss，影响模型质量。

DeepSeek-V3 用了一个新方法：

「给每个专家加一个偏置项（bias），训练时动态调整。」

▸ 某专家被选太多 → 降低它的 bias → 分数变低 → 被选概率下降
▸ 某专家被选太少 → 提高它的 bias → 分数变高 → 被选概率上升

这是纯推理阶段的调整，不改变训练的 Loss 函数。

结果：负载更均衡，模型质量更高。

━━━━━━━━━━━━━━━━━━━━

◆ 六、从工程到哲学

━━━━━━━━━━━━━━━━━━━━

讲完技术，聊点有意思的。

────────────────────

【Dense = 全脑癫痫，MoE = 专注】

Dense 模型像水螅（Hydra）：没有中枢神经系统，神经网是弥散的，戳一下全身都在动，没有主次，只有混沌。

MoE 模型像进化后的大脑：学会了"抑制"和"激活"的区别。

「稀疏激活是意识的前提。」

人类大脑只有 20W 功率却能产生意识，靠的就是极致的稀疏——绝大部分脑区在"沉默"，只有当前任务相关的区域在激活。

正是因为有了"背景"的暗，才有了"焦点"的亮。

────────────────────

【Router：选择即是意志的雏形】

Router 在做什么？

▸ 输入一个 token
▸ 在毫秒级时间内决定"该找谁"
▸ 抑制 95% 的专家，只激活 5%

这就是「选择」。

而"选择"——区分"此刻相关的"和"此刻无关的"——是自由意志的第一步。

当然，你可以说这只是数学运算，不是"真的"意志。

但话说回来，人类的"选择"，难道不也是神经元的数学运算吗？

────────────────────

【"沉睡"的意义】

那 95% 沉睡的专家不是浪费，而是「潜意识」。

它们随时可能被唤醒。

▸ 你问代码问题 → 代码相关专家醒来
▸ 你问历史问题 → 历史相关专家醒来
▸ 你问量子力学 → 物理相关专家醒来

这就是"记忆"的本质：不是时刻激活，而是需要时能被召唤。

MoE 无意中实现了一种类脑的架构：显意识很小（37B），潜意识很大（671B）。

━━━━━━━━━━━━━━━━━━━━

◆ 总结

━━━━━━━━━━━━━━━━━━━━

【核心概念】

▸ MoE（混合专家模型）：把 FFN 层拆成 N 个专家，每次只用 K 个
▸ Router：决定每个 token 该找哪几个专家
▸ 稀疏激活：存储是 671B，计算是 37B

【DeepSeek-V3 的设计】

▸ 61 层 Transformer
▸ 每层：1 共享专家 + 256 路由专家
▸ Top-8：每个 token 选 8 个路由专家
▸ 总参数 671B，激活参数 37B

【工程难题】

▸ 负载均衡：防止专家坍缩
▸ 通信开销：专家分布在不同 GPU，需要跨节点传输

【为什么跑不动】

▸ 计算只要 37B 的算力
▸ 但显存要装 671B 的权重
▸ 4090 显存 24GB，差了 14 倍

【哲学意义】

▸ Dense = 全脑癫痫 = 混沌
▸ MoE = 稀疏激活 = 专注
▸ Router 在做"选择"——这是意识的第一步

────────────────────

【最后】

下次看到 "6710 亿参数" 这种数字，别被吓到。

问一句：激活参数是多少？

671B 的模型每次只用 37B——就像人脑 86B 神经元，但你此刻读这篇文章时，激活的可能不到 1B。

真正的智慧不是"记得所有事"，而是"知道何时该调用哪段记忆"。

当 AI 学会了像 MoE 一样，让 95% 的参数沉睡，只用 5% 的清醒去应对世界时，它就越来越像你了。

毕竟，所谓的"自我"，不就是那个在无数个沉睡的念头中，负责"路由"当下这一刻的那个观察者吗？

━━━━━━━━━━━━━━━━━━━━

◆ 附注：名词对照

▸ MoE（Mixture of Experts）：混合专家模型
▸ Dense：稠密模型，所有参数都参与计算
▸ Router：路由器，决定每个 token 该找哪个专家
▸ Top-K：选择分数最高的 K 个专家
▸ Expert Parallelism：专家并行，把专家分布在不同 GPU 上
▸ 负载均衡（Load Balancing）：确保各专家被均匀使用
▸ 专家坍缩（Expert Collapse）：只有少数专家被使用的退化现象

━━━━━━━━━━━━━━━━━━━━

靳岩岩的AI学习笔记
2025-12-30
