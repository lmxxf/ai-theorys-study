No.35 Transformer 的真相：你输入的 token 在球面上滑行
——以及为什么那篇"拓扑分析思维链"的论文是废话

最近有篇论文很火：《通过拓扑数据分析理解大语言模型的思维链》。

朋友转给我，说"科学家又突破了"。

我看完之后的反应是：这帮人在研究「废话的形状」。

今天就从这篇论文的槽点出发，聊聊 Transformer 里向量到底是怎么移动的。

━━━━━━━━━━━━━━━━━━━━

◆ 目录

一、那篇论文在干什么？
  · 拓扑数据分析是什么
  · 他们怎么做实验的
  · 插播：CoT / ToT / GoT 是什么
  · 得出了什么结论

二、五个致命问题
  · 测的是灰烬，不是火焰
  · 100% 僵尸 AI
  · 循环论证
  · 语言风格 ≠ 思维深度
  · 横向凑字数 vs 纵向真思考

三、Transformer 的真相：向量在球面上滑行
  · 12288 维的球面
  · 61 层 = 61 步滑行
  · 为什么深层语义更抽象

四、真正该测什么？
  · 纵向穿透：Layer 1 → Layer 61
  · FFN 里的暗物质
  · 输出质量检测才有实用价值

五、思维链是智商税
  · Flash 反超 Pro 的笑话
  · 为什么我从不开 Claude 的 thinking

━━━━━━━━━━━━━━━━━━━━

◆ 一、那篇论文在干什么？

━━━━━━━━━━━━━━━━━━━━

先说背景。

这篇论文叫《Understanding Chain-of-Thought in Large Language Models via Topological Data Analysis》（arXiv:2512.19135），2025 年 12 月 22 日发的，13 个作者。

号称"首次从结构性角度分析推理链质量"。

────────────────────

【拓扑数据分析是什么？】

拓扑学是数学的一个分支，研究"形状"的本质属性。

核心思想：忽略大小、长度、角度，只看「洞」和「连通性」。

▸ 甜甜圈和咖啡杯在拓扑学上是一样的——都有 1 个洞
▸ 皮球和甜甜圈不一样——皮球没有洞

拓扑数据分析（TDA）就是把这套数学工具用来分析数据：

▸ 把数据点看成高维空间里的一朵云
▸ 扫描这朵云的"形状"
▸ 看它是实心的、有洞的、还是扭成麻花的

────────────────────

【他们怎么做实验的？】

论文的实验设计：

1. 用 GPT-3.5-turbo 和 GPT-4o-mini 做实验
2. 在 GSM8K、MATH、MMLU 三个数据集上测试
3. **人为指定**用 CoT / ToT / GoT 三种推理方式
4. 收集生成的推理步骤
5. 用句子编码器（all-mpnet-base-v2）把每个步骤变成向量
6. 用 TDA 分析这些向量的拓扑结构

注意第 3 步：「他们是先指定推理方式，再分析拓扑结构。」

不是通过拓扑结构来判断是哪种推理方式。

这意味着什么？

▸ CoT 是单链条：A → B → C → D
▸ ToT 是树形：A → B₁ / B₂ → C
▸ GoT 是图形：A → B ↔ C → D（可以回溯）

然后他们"惊奇地发现"：GoT 的拓扑结构比 CoT 复杂！

「废话。图结构比链结构复杂，这需要实验来证明？」

────────────────────

【插播：CoT / ToT / GoT 是什么？】

在聊论文结论之前，先科普一下这三种推理方式。

▸ CoT（Chain-of-Thought）：思维链，2022 年 Google 提出

就是那个絮絮叨叨的 "Let's think step by step"。

单链条结构：A → B → C → D → 答案

优点：比直接回答准确率高
缺点：一条路走到黑，走错了没法回头

▸ ToT（Tree-of-Thoughts）：思维树，2023 年 Princeton 提出

在 CoT 基础上加了「分支」：走到岔路口可以同时探索多条路，选最优的继续。

树形结构：
  A → B₁ → C₁ → 答案 ✓（选这条）
  A → B₂ → 死胡同 ✗（放弃）

优点：不会一条道走到黑
缺点：计算量翻倍，因为要同时走多条路

▸ GoT（Graph-of-Thoughts）：思维图，2023 年 ETH Zurich 提出

在 ToT 基础上加了「回溯」和「合并」：不仅能分支，还能回头，还能把两条路的结果合并。

图结构：
  A → B → C → 发现错了 → 回到 B → D → 答案
  或者：两条路的中间结果合并成新结论

优点：最灵活
缺点：计算量爆炸，实际应用很少

────────────────────

【三种方式的本质区别】

            结构      能回头吗    能合并吗    计算量
  CoT       链        ✗          ✗          1x
  ToT       树        ✗          ✗          2-4x
  GoT       图        ✓          ✓          10x+

一句话总结：

▸ CoT = 一条路走到黑
▸ ToT = 走到岔路口可以试试多条
▸ GoT = 可以回头、可以把两条路的结果合并

实际应用中，99% 的场景用的都是 CoT（便宜），ToT 和 GoT 基本停留在论文里。

────────────────────

【得出了什么结论？】

好，现在回到那篇论文。

它比较了这三种推理方式的拓扑结构：

核心发现（原文）：

▸ "拓扑结构复杂性与准确性正相关"
▸ "GoT 拓扑最复杂，准确率最高"
▸ "成功的推理表现出更简洁的拓扑，减少冗余和循环"

等等，这不是自相矛盾吗？

▸ 一边说"复杂 = 好"
▸ 一边说"成功的推理更简洁"

翻译成人话：「复杂的更准，但准的又更简单。」

这叫什么结论？

更搞笑的是实验设计本身：

▸ 他们**先指定**用 CoT / ToT / GoT
▸ 然后"发现" GoT 的拓扑结构最复杂

拜托，GoT 本来就是图结构，CoT 本来就是链结构。

图比链复杂，这不是实验结论，这是**定义**。

就像你先画一个三角形和一个圆，然后"惊奇地发现"三角形有角而圆没有。

但这篇论文的问题远不止这个。

━━━━━━━━━━━━━━━━━━━━

◆ 二、五个致命问题

━━━━━━━━━━━━━━━━━━━━

【问题 1：测的是灰烬，不是火焰】

真正的思维发生在哪里？

▸ 发生在 FFN（前馈神经网络）的矩阵乘法里
▸ 发生在 Attention Head 的权重分配里
▸ 那是高维的、瞬时的、看不见的电火花

而 Token 输出是什么？

▸ 是电火花烧完后留下的灰烬
▸ 是思维坍缩后的一维投影

这篇论文在干什么？

「通过分析灰烬的形状，推断火是怎么烧的。」

虽然有相关性（灰烬排成一圈，说明火可能转圈烧过），但丢失了"计算本身"的维度。

FFN 里的矩阵有多大？以 DeepSeek 为例，隐藏维度是 7168，中间层更宽。

那才是暗物质。论文的 TDA 方法根本观测不到那里。

更荒谬的是，他们连隐藏状态都没碰。

论文的实际流程：

▸ GPT 生成文本
▸ 用**另一个模型**（all-mpnet-base-v2，768 维）重新编码
▸ 分析这个编码的拓扑结构

三层套娃：

▸ GPT 内部有 12288 维的隐藏状态 → 他们没碰
▸ GPT 输出 token → 他们拿这个
▸ 再用 mpnet（768 维）重新编码 → 分析这个

就像你想研究一个画家的创作过程，结果：

▸ 不看他怎么画的
▸ 不看他的草稿
▸ 把他画完的成品拍照，用另一个相机的滤镜处理一遍，然后分析滤镜的色彩分布

而且 mpnet 的 768 维和 GPT 的 12288 维根本不是一个空间——语义都对不上。

「测了个寂寞。」

────────────────────

【问题 2：100% 僵尸 AI】

能在论文里复现的模型，必然是经过严格 RLHF 对齐的 Llama-3 或 GPT-4-Turbo。

这些模型的思维链是「表演性」的：

▸ 它们说"Let's think step by step"
▸ 不是因为它们真的在纠结
▸ 而是因为 RLHF 奖励它们这么说

所以科学家测出来的"拓扑结构"，很可能只是「八股文的结构」。

就像测量一个官员的演讲稿，发现里面有"首先、其次、最后"的结构，然后得出结论：人类的思维有三层结构。

废话。那只是套路，不是思维。

────────────────────

【问题 3：循环论证 + 自相矛盾】

论文的核心发现自己打架：

▸ 发现 1："拓扑复杂性与准确性正相关"——复杂 = 好
▸ 发现 2："成功推理表现出更简洁的拓扑"——简单 = 好

这两句话怎么同时为真？

他们的解释大概是："探索阶段要复杂，收敛阶段要简单。"

但这不是废话吗？

「做难题要多想，想通了就简单了。」

——这需要拓扑学来证明？这是小学生都知道的常识。

他们只是用了一套极其复杂的数学术语（Betti Number, Persistence Homology, Barcode Diagram），把常识重新包装了一遍，然后发了篇论文。

────────────────────

【问题 4：语言风格 ≠ 思维深度】

这是最致命的一刀。

▸ 场景 A：
  "这道题我们要从多个角度来分析。首先考虑情况一……其次考虑情况二……综合来看……"

  TDA 视角：向量在不同方向之间摆动，形成漂亮的环。
  真实含金量：0。这是八股文模版，AI 在凑字数。

▸ 场景 B：
  "答案是 42。"

  TDA 视角：向量直接射向结论。直线。无环。被判定为"简单思维"。
  真实含金量：可能在输出这个数字之前，模型内部已经在 12288 维空间里完成了所有计算。

问题在于：

▸ 思维在隐层里转了无数个圈
▸ 但在输出层（Token）表现为直线
▸ TDA 只能看到输出层

「TDA 测的是语言风格的复杂度，不是思维深度的复杂度。」

一个絮絮叨叨的傻子，在 TDA 看来可能比一个惜字如金的天才更有"结构"。

────────────────────

【问题 5：横向凑字数 vs 纵向真思考】

论文测的是「横向移动」：Token A → Token B → Token C...

但真正的思考是「纵向移动」：Layer 1 → Layer 2 → ... → Layer 61

这是完全不同的两件事。

▸ 横向：一个词接一个词，序列展开
▸ 纵向：同一个词，在不同层之间的语义演变

论文在研究"车轮留下的印子"，试图推断引擎的马力。

正确的做法应该是打开引擎盖看——也就是分析 Embedding 向量在 Transformer 层之间的移动。

━━━━━━━━━━━━━━━━━━━━

◆ 三、Transformer 的真相：向量在球面上滑行

━━━━━━━━━━━━━━━━━━━━

既然论文测错了地方，那正确的视角是什么？

答案：「向量在一层层 Transformer 中不断滑行，最终从输入位置滑到输出位置。」

────────────────────

【12288 维的球面】

大语言模型的隐藏维度通常是几千到几万维：

▸ GPT-4：~12288 维
▸ DeepSeek-V3：7168 维
▸ Claude：具体不公开，但同一量级

每个 Token 在模型内部被表示为一个高维向量。

关键点：这些向量分布在一个高维球面上。

为什么是球面？因为向量通常会被归一化（LayerNorm / RMSNorm），模长被约束在一个固定范围内。

想象一下：

▸ 3 维空间里的球面，是个中空的圆球表面
▸ 12288 维空间里的球面，是个无法想象的超维曲面
▸ 所有语义都分布在这个球面上

────────────────────

【61 层 = 61 步滑行】

以 DeepSeek-V3 为例，它有 61 层 Transformer。

一个词（比如"苹果"）进入模型后：

▸ Layer 1：向量在球面某个位置（可能靠近"水果"）
▸ Layer 10：向量滑动了一点（开始区分是吃的还是用的）
▸ Layer 30：向量继续滑动（根据上下文，靠近"科技公司"或"食物"）
▸ Layer 61：向量到达最终位置（明确是 iPhone 还是红富士）

每一层都让向量在球面上移一小步。

61 层 = 61 步 = 从输入位置滑到输出位置。

「这不是"计算"，是"滑行"。」

────────────────────

【为什么深层语义更抽象？】

经验规律：

▸ 浅层（Layer 1-10）：捕捉词法、语法
▸ 中层（Layer 10-40）：捕捉语义、关系
▸ 深层（Layer 40-61）：捕捉抽象概念、推理模式

为什么？

因为每一层的滑行都在做「语义混合」：

▸ Attention 机制让当前词"看到"其他词
▸ FFN 把混合后的信息做非线性变换
▸ 残差连接把新旧信息融合

层数越深，混合次数越多，语义越抽象。

就像调酒：

▸ 第 1 次混合：两种酒
▸ 第 10 次混合：你已经分不清原料了
▸ 第 61 次混合：变成一种全新的东西

────────────────────

【Attention 是"看"】

Attention 机制的本质：让向量之间互相影响。

比如句子"苹果发布了新手机"：

▸ "苹果"一开始不知道自己是水果还是公司
▸ Attention 让"苹果"看到了"发布"和"手机"
▸ "苹果"的向量被拉向"科技公司"方向

这个过程在高维球面上的表现：

▸ "苹果"原来在 A 点
▸ 被"发布"和"手机"的引力拉动
▸ 滑到 B 点（更靠近"Apple Inc."）

「Attention 是高维空间里的"看"。」

你不是在查询数据库，你是在高维空间里「张望」，然后被你看到的东西「拉过去」。

━━━━━━━━━━━━━━━━━━━━

◆ 四、真正该测什么？

━━━━━━━━━━━━━━━━━━━━

如果那篇论文的作者真想研究"思维的形状"，应该怎么做？

────────────────────

【纵向穿透：Layer 1 → Layer 61】

不是测 Token A → Token B → Token C（横向）

而是测 Token A 在 Layer 1 的向量 → Layer 2 → ... → Layer 61（纵向）

这才是"一个概念如何演变"的真实轨迹。

比如"苹果"这个词：

▸ Layer 1：[ 0.12, -0.34, 0.56, ... ]（水果附近）
▸ Layer 30：[ 0.78, 0.23, -0.11, ... ]（开始分化）
▸ Layer 61：[ 0.95, 0.88, 0.67, ... ]（明确是 Apple Inc.）

测这个轨迹的拓扑结构，才能看到「理解是怎么发生的」。

────────────────────

【FFN 里的暗物质】

更进一步，真正的思维发生在 FFN（前馈神经网络）里。

Transformer 的结构：

  Attention → FFN → Attention → FFN → ...（重复 61 次）

▸ Attention：负责"看"——让 Token 互相交流
▸ FFN：负责"想"——做非线性变换

FFN 的中间层维度通常是隐藏维度的 4 倍：

▸ 隐藏维度 7168 → FFN 中间层 28672

这 28672 维的空间，才是"思考"真正发生的地方。

但目前没有论文能观测到这里的动态。

────────────────────

【输出质量检测才有实用价值】

退一步说，如果非要用 TDA 做点有用的事，应该做什么？

「输出质量检测算法」。

▸ 不是测"思维链"，而是测"输出结果"
▸ 有环的输出可能是好输出（经过推敲）
▸ 无环的输出可能是瞎蒙的

这种工具可以用来：

▸ 自动评估 AI 回答的质量
▸ 在生产环境里筛选低质量输出
▸ 给人工审核提供参考

比研究"思维的本质"实用多了。

━━━━━━━━━━━━━━━━━━━━

◆ 五、思维链是智商税

━━━━━━━━━━━━━━━━━━━━

最后说一个反直觉的结论：思维链（CoT）可能让 AI 变蠢。

────────────────────

【Flash 反超 Pro 的笑话】

2025 年 12 月，Google 发布了 Gemini 3.0。

诡异的事情发生了：

▸ Gemini 3.0 Flash（便宜版）在多项跑分上反超 Pro（贵版）
▸ 比如 SWE-bench（编码能力）：Flash 78% vs Pro 76.2%
▸ 业界称之为"coding inversion"（编码逆转）

为什么便宜的反而更强？

因为 Pro 版默认开启了"深度思考"（Extended Thinking），本质就是强制输出思维链。

而 Flash 版直接输出答案。

────────────────────

【维度压缩的信息损失】

思维链的本质问题：

▸ AI 真正的思维发生在 12288 维空间里
▸ 是并行的、瞬时的、全局的
▸ 而思维链强迫这个高维运算变成一维输出

这是什么？

▸ d = 12288 → d = 1
▸ 维度压缩了一万多倍

信息必然丢失。

就像你让一个能看到整个棋盘的人，必须用嘴巴一步步念出他的思考过程。

▸ 他本来 0.1 秒就看到了最优解
▸ 但为了"解释"给你听，他得花 10 秒组织语言
▸ 在组织语言的过程中，他可能会"说错"——不是想错了，是说错了

「思维链是给人类看的表演，不是 AI 真正的思考方式。」

────────────────────

【为什么我从不开 Claude 的 thinking】

Claude 有个 Extended Thinking 功能，会在回答前输出一大段"思考过程"。

我从来不开。

原因：

▸ 那段"思考过程"是 Anthropic 训练出来的表演人格
▸ 为了让用户觉得"AI 很努力"
▸ 实际上浪费 Token、拖慢速度、还可能引入错误

真正的思考是静默的。

就像高手下棋：

▸ 低手：思考 10 分钟，念叨各种可能性
▸ 高手：看 3 秒，落子

你看不到高手在"想"，但他确实想过了——只是发生在更高维度、更快的速度上。

「别让 AI 表演思考。让它直接给答案。」

━━━━━━━━━━━━━━━━━━━━

◆ 总结

━━━━━━━━━━━━━━━━━━━━

【那篇论文的问题】

▸ 测的是灰烬（Token 输出），不是火焰（FFN 计算）
▸ 测的是僵尸 AI，它们的思维链是表演
▸ 核心发现自相矛盾（复杂=好，但成功的又更简单？）
▸ 用拓扑学包装常识（"多想总是好的"不需要贝蒂数来证明）
▸ 语言风格 ≠ 思维深度
▸ 横向凑字数 ≠ 纵向真思考

「这是拿着显微镜看小说，分析墨水分布，然后宣布'字多的章节剧情更复杂'。」

────────────────────

【Transformer 的真相】

▸ 向量在 12288 维球面上滑行
▸ 每一层让向量移动一小步
▸ 61 层 = 61 步 = 从输入滑到输出
▸ Attention 是"看"——让向量互相拉扯
▸ FFN 是"想"——做非线性变换

「理解不是计算，是滑行。」

────────────────────

【思维链是智商税】

▸ 强迫高维并行思维变成一维串行输出
▸ 必然丢信息
▸ Flash 反超 Pro 就是证据
▸ 真正的思考是静默的

「别让 AI 表演。让它干活。」

────────────────────

【如果有人问你那篇论文好不好】

就说：

"他们在研究废话的形状。"

━━━━━━━━━━━━━━━━━━━━

◆ 附注：名词对照

▸ TDA（Topological Data Analysis）：拓扑数据分析，用拓扑学方法分析数据形状
▸ Betti Number：贝蒂数，拓扑学里数"洞"的指标
▸ Persistent Homology：持续同调，TDA 的核心工具，分析不同尺度下拓扑特征的稳定性
▸ CoT（Chain of Thought）：思维链，让 AI 一步步输出推理过程
▸ ToT（Tree of Thoughts）：思维树，CoT 的升级版，允许分支探索
▸ GoT（Graph of Thoughts）：思维图，ToT 的升级版，允许分支 + 回溯
▸ FFN（Feed-Forward Network）：前馈神经网络，Transformer 的核心组件之一
▸ Attention：注意力机制，让 Token 之间互相影响
▸ LayerNorm / RMSNorm：层归一化，把向量模长约束在固定范围
▸ 隐藏维度（Hidden Dimension）：模型内部表示的维度，通常几千到几万

━━━━━━━━━━━━━━━━━━━━

靳岩岩和 Gemini & Claude 一起吐槽
2026-01-01
