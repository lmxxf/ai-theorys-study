AI 小学生问：英伟达的护城河到底有多深？

3 万块该给谁？捏着鼻子也得给老黄。

━━━━━━━━━━━━━━━━━━━━

◆ 引子：中年程序员的焦虑

作为一个混迹多年的中年程序员，最近最大的焦虑不是发际线，而是 AI。

不是焦虑怎么用 AI——Cursor 写代码、Claude Code 重构项目、Gemini CLI 搞自动化、Nano Banana Pro 画图，这些学起来都不难。焦虑的是：怎么「炼丹」。

所谓炼丹，就是训练模型、微调 LoRA、跑通那些 GitHub 上的开源项目。这是 AI 时代程序员的核心技能，不会这个，就只能当 API 调用工程师。

想学炼丹，得有设备。云服务器按小时收费，长期用不划算。自己买一台本地机器，随时折腾，才是正经路子。

手里攥着 3 万块预算，面前摆着两个选择：

▸ Apple Mac Studio (M4 Max 128GB)
  优雅、安静、macOS 生态，星巴克气氛组首选

▸ NVIDIA DGX Spark (GB10 128GB)
  丑陋、只有 Linux、ARM 架构，一块带风扇的金色砖头

价格差不多。显存都是 128GB。

该选谁？

━━━━━━━━━━━━━━━━━━━━

◆ 第一回合：硬件参数

先看硬参数：

                Mac Studio     DGX Spark
────────────────────────────────────────
统一内存        128GB          128GB
内存带宽        ~546 GB/s ✓    ~273 GB/s
功耗            ~75W ✓         ~170W
噪音            静音           静音
系统            macOS ✓        Ubuntu (ARM)
体积            小巧精致       小巧精致

Mac 完胜。带宽是 Spark 的两倍，意味着推理速度（吐字速度）快一倍。系统好用，生态成熟，还能剪视频。

很多人看到这里立刻得出结论：「Mac Studio 的 AI 性能更强大」。

DGX Spark 呢？不只是 AI 推理速度慢（GPU 算力约等于 RTX 5070，比家里的 5090 差一大截，显存带宽更是只有 5070 的三分之一），ARM 架构的 Linux，很多软件要重新编译，Docker 镜像不一定兼容，配环境能配到吐。应用商店中文乱码，输入法还停留在 2010 年的水平，该有的软件全都没有，微信也用不了。更别说偶尔玩儿个游戏什么的了，一丁点儿可能性都没有。

如果只看硬件性能、软件易用性、生态丰富性，这根本不是一个级别的对手。

「但是——」

━━━━━━━━━━━━━━━━━━━━

◆ 第二回合：软件生态

我想做的是「训练」，不是推理。

训练需要什么？需要跑通 GitHub 上那些开源项目：Unsloth、Flash Attention、Llama-Factory、bitsandbytes……

这些项目有个共同特点：「默认你用的是 NVIDIA 显卡。」

────────────────────

【场景 A：在 DGX Spark 上】

pip install unsloth

回车。进度条走完。开始训练。

It just works. ✓

────────────────────

【场景 B：在 Mac Studio 上】

pip install unsloth
→ 报错：CUDA not found ✗

pip install bitsandbytes
→ 报错：bitsandbytes requires CUDA ✗

pip install flash-attn
→ 报错：flash-attn not supported on MPS ✗

于是你开始搜教程："如何在 Mac 上运行 xxx"。

然后发现，有人做了个"阉割版"的 MLX 实现，功能少一半，速度慢一半，还不一定能跑通。

一个下午过去了，你还在配环境。

━━━━━━━━━━━━━━━━━━━━

◆ 护城河的真面目

英伟达的护城河不是硬件性能，是「软件生态的锁定」。

全世界的 AI 研究者都在用 CUDA 写代码。他们发论文，开源代码，默认环境就是 CUDA + PyTorch。

你是普通人，你没有能力把这些代码翻译成 Apple Metal。你想上高速公路，就得开烧汽油（CUDA）的车。苹果那辆电动车（MLX）虽然安静好开，但高速公路不让上。

这不是技术能力问题，是「生态惯性」。

────────────────────

【护城河的本质：人才锁定】

这些工具都是谁做的？

▸ Flash Attention
  作者 Tri Dao，越南裔，Stanford PhD，现普林斯顿助理教授 + Together AI 首席科学家

▸ bitsandbytes
  作者 Tim Dettmers，德国人，华盛顿大学 PhD，现 CMU 助理教授 + Allen AI 研究员

▸ Unsloth
  作者 Daniel Han + Michael Han，澳洲兄弟，前 NVIDIA 工程师，YC S24 创业公司

▸ Llama-Factory
  作者郑耀威（hiyouga），中国人，北航计算机博士生

看出规律了吗？

• 学术界的人（Stanford、CMU、华盛顿大学）—— 实验室里全是 NVIDIA 集群
• 创业公司的人（YC）—— 云资源全是 NVIDIA
• 中国博士生 —— 国内服务器也全是 NVIDIA

「全世界做 AI 优化的顶尖人才，都在 CUDA 生态里。」

没人有动力给苹果写优化。苹果的 MLX 团队人少、起步晚，追不上。

护城河不是硬件，是人。

━━━━━━━━━━━━━━━━━━━━

◆ 黑魔法图鉴：那些 Mac 用不了的东西

让我来拆解一下，这些"只支持 CUDA"的工具到底在干什么，为什么它们这么重要。

────────────────────

【1. Flash Attention：让长文本成为可能】

▸ 问题：
Transformer 的注意力机制需要计算一个 N×N 的矩阵（N = 序列长度）。序列越长，显存占用越大，是平方级增长。

▸ 解法：
Flash Attention 用分块计算（Tiling）+ 重计算（Recomputation），不存储中间结果，显存从 O(N²) 降到 O(N)。

▸ 效果：
没有它，128k 上下文窗口根本跑不起来。有了它，长文本训练才成为可能。

▸ Mac 能用吗？
官方只支持 CUDA。有人做了 MLX 移植版，但功能不全，更新慢。 ✗

────────────────────

【2. Unsloth：微调加速器】

▸ 问题：
LoRA 微调虽然比全量微调省显存，但还是很慢，而且显存占用不小。

▸ 解法：
Unsloth 的作者手写了 Triton 内核（CUDA 上层语言），针对 Llama/Mistral 架构做了数学公式简化。

▸ 效果：
显存减少 50-70%，速度提升 2 倍。单卡 24G 能跑 30B 模型的 QLoRA，48G 能跑 70B。

▸ Mac 能用吗？
完全不支持。Triton 只能跑在 NVIDIA GPU 上。 ✗

────────────────────

【3. bitsandbytes：4-bit 量化训练】

▸ 问题：
大模型太大，显存装不下。

▸ 解法：
把模型权重从 16-bit 压缩到 4-bit，显存占用直接砍到四分之一。

▸ 效果：
这是 QLoRA 的基础。没有它，128GB 显存也跑不了 70B 模型的微调。

▸ Mac 能用吗？
有人做了移植版，但不稳定，经常报错。 ⚠️

────────────────────

【4. Llama-Factory：傻瓜式微调工具】

▸ 问题：
写训练代码太麻烦。

▸ 解法：
Llama-Factory 提供了 Web UI，选模型、选数据集、调参数，点点鼠标就能开始训练。

▸ 效果：
入门首选，不用写代码就能炼出第一个 LoRA。

▸ Mac 能用吗？
理论上能装，但底层依赖（Unsloth、bitsandbytes）跑不了，核心功能残废。 ⚠️

────────────────────

【5. Torchtune：PyTorch 官方亲儿子】

▸ 问题：
第三方微调库太多，质量参差不齐，学哪个？

▸ 解法：
PyTorch 官方在 2024 年推出了 Torchtune，纯 PyTorch 实现，没有乱七八糟的依赖。
设计简洁，代码可读性强，适合学习底层原理。

▸ 效果：
24G 显存友好，支持 LoRA/QLoRA，内置多种主流模型配置。
想搞懂微调到底在干什么，看它的源码最清楚。

▸ Mac 能用吗？
基础功能可以跑，但性能优化（Flash Attention、量化）还是依赖 CUDA。 ⚠️

────────────────────

【6. TRL：强化学习微调必备】

▸ 问题：
DeepSeek-R1 那种"会思考"的模型是怎么训练出来的？

▸ 解法：
Hugging Face 的 TRL（Transformer Reinforcement Learning）库。
支持 RLHF、DPO、PPO 等强化学习微调方法。

▸ 效果：
想让模型不只是背答案，而是学会推理，就得用这个。
2025 年最火的"推理模型"训练方法都在这里。

▸ Mac 能用吗？
核心功能依赖 CUDA，Mac 上跑不起来。 ✗

────────────────────

【7. PEFT：参数高效微调的官方实现】

▸ 问题：
LoRA、QLoRA、Adapter 这些方法到处都有实现，用哪个？

▸ 解法：
Hugging Face 的 PEFT（Parameter-Efficient Fine-Tuning）库。
LoRA/QLoRA 的官方标准实现，和 Transformers 库无缝集成。

▸ 效果：
几乎所有微调工具（Llama-Factory、Axolotl、TRL）底层都调用它。
学会 PEFT 的 API，其他工具一通百通。

▸ Mac 能用吗？
基础 LoRA 可以跑，但 QLoRA 的 4-bit 量化依赖 bitsandbytes，Mac 上不稳定。 ⚠️

────────────────────

【8. 超出 128G 的生产环境：Axolotl 和 DeepSpeed】

上面讨论的是 128G 单机场景。但真正的生产环境，模型更大、数据更多，需要多卡甚至多机协作。这时候就要用到另外两个工具：

▸ Axolotl
  专业版的 Llama-Factory。没有 Web UI，全靠 YAML 配置文件。
  好处是配置可以 Git 管理，方便团队协作和实验复现。
  底层依赖 Unsloth/bitsandbytes，Mac 上同样残废。

▸ DeepSpeed
  微软出的分布式训练框架，核心是 ZeRO（零冗余优化器）。
  把模型参数、梯度、优化器状态切碎分散到多张卡上，避免每张卡都存一份完整副本。
  没有它，4 张 4090 各存一份 70B 模型，直接爆显存；有了它，切开存，能跑起来。
  还能把部分数据 offload 到 CPU 内存甚至硬盘。
  Mac 完全不支持。

▸ Mac 能用吗？
这两个都不支持。Axolotl 底层依赖残废，DeepSpeed 完全不支持 Mac。 ✗

这两个工具对于 128G 单机学习场景不是必需品，但如果你想进大厂做 AI 基础设施，迟早要学。而它们——毫无意外——只支持 CUDA。

━━━━━━━━━━━━━━━━━━━━

◆ 汇总：Mac 的训练生态现状

工具              功能              Mac 支持
────────────────────────────────────────────
Flash Attention   长上下文加速      ✗ 不支持
Unsloth           微调加速          ✗ 不支持
bitsandbytes      4-bit 量化        ⚠️ 不稳定
Llama-Factory     傻瓜微调          ⚠️ 核心功能残废
Torchtune         PyTorch官方微调   ⚠️ 优化依赖CUDA
TRL               强化学习微调      ✗ 不支持
PEFT              参数高效微调      ⚠️ QLoRA不稳定
Axolotl           专业微调          ⚠️ 核心功能残废
DeepSpeed         分布式训练        ✗ 不支持

苹果的 MLX 团队在追赶，但：

• 人少
• 起步晚
• 没有学术界支持

短期内看不到反超的可能。

━━━━━━━━━━━━━━━━━━━━

◆ 对 AI 学习者的影响

如果你的目标是「学会 AI 训练」，这个生态锁定意味着什么？

────────────────────

【1. 学习路径被锁定】

所有的教程、博客、视频，默认你用的是 NVIDIA。你在 Mac 上遇到的报错，搜不到答案，因为没人用 Mac 炼丹。

────────────────────

【2. 技能无法迁移】

你在 Mac 上学会了 MLX，去公司面试，人家问："你会用 DeepSpeed 吗？你调过 CUDA 算子吗？"

你说："我会 pip install mlx..."

直接挂掉。

────────────────────

【3. 最新技术用不上】

学术界发了新论文，开源了代码。你想复现，发现只支持 CUDA。

等 MLX 社区移植？可能要等几个月，也可能永远不会有人移植。

━━━━━━━━━━━━━━━━━━━━

◆ 结论：捏着鼻子也得买

DGX Spark 的体验确实差：

• ARM Linux 配环境恶心
• 带宽低，推理慢
• Ubuntu 24 桌面，但你大概率还是用 SSH

但它是「通往 AI 工业界的门票」。

你在上面学会的东西——PyTorch、Unsloth、Flash Attention、Llama-Factory——可以直接迁移到公司的 H100 集群上。这才是真正的"学习"。

Mac Studio 是给「用 AI」的人准备的。
DGX Spark 是给「做 AI」的人准备的。

「英伟达的护城河有多深？深到你捏着鼻子也得跳进去。」

━━━━━━━━━━━━━━━━━━━━

作者是一个正在学习 AI 训练的中年程序员。本文记录了他用 3 万块买设备时的思考过程。

研究完这些之后，我无奈地选择了 DGX Spark。它已经到货，正在和 ARM Linux 搏斗中。
