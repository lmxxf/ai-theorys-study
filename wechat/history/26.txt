Transformer 是怎么发明的？——回到深度学习的蛮荒时代@2016

「这玩意儿不是数学家的艺术品，是工程师为了省钱拼出来的弗兰肯斯坦。」

━━━━━━━━━━━━━━━━━━━━

📑 目录（全文约 8500 字，建议收藏慢慢看）

◆ 先上公式，劝退一波
◆ 时代背景：2016 年，深度学习的"蛮荒时代"
  ├─ 硬件：GPU 已经很猛了
  ├─ 软件：深度学习的应用领域
  └─ 核心矛盾：GPU 想并行，RNN 偏要串行
◆ 第一幕：LSTM 的痛苦——"一个一个来"
◆ 第二幕：发现"外挂"比"主程序"更有用
  ├─ Attention 的诞生：2014 年
  ├─ 什么是 Encoder、Decoder？
  ├─ Bahdanau Attention 公式详解
  ├─ Bahdanau vs Transformer 公式对比
  ├─ 追问：Bahdanau 能不能也改成矩阵运算？
  ├─ 追问：Transformer 就完全不用激活函数了？
  └─ 吐槽：为什么没人讲 FFN？
◆ 第三幕：Q、K、V 是什么？——就是"查字典"
  ├─ 类比：去图书馆借书
  ├─ 追问：为什么"点积"能算相似度？
  ├─ 插一嘴：点积 vs 余弦相似度
  ├─ 追问：Softmax 到底在干嘛？
  ├─ 追问：为什么要三个矩阵（Q、K、V）？
  └─ 进阶：QKV 在不同层里干的事不一样
◆ 第四幕：填坑——删掉 LSTM 带来的副作用
  ├─ 补丁 1：Positional Encoding（位置编码）
  ├─ 补丁 2：Multi-Head Attention（多头注意力）
  └─ 补丁 3：ResNet 残差连接 + LayerNorm
◆ 总结：Transformer 的真实发明路径
◆ 附注：技术名词对照表

━━━━━━━━━━━━━━━━━━━━

◆ 先上公式，劝退一波

Attention(Q, K, V) = softmax(QK^T / √d_k) V

看到这个公式，正常人内心的反应一定是：

「Q 是啥？K 是啥？V 是啥？为什么要点积？为什么要除以根号 d？为什么要 softmax？这一串计算到底是怎么想出来的？」

网上 99% 的文章会告诉你：

「Q 是 Query，K 是 Key，V 是 Value，点积算相似度，softmax 归一化……」

然后你更懵了。

⚠️ 这些解释都是"事后诸葛亮"。

它们告诉你公式是什么，但没告诉你：这公式到底是怎么发明的？

难道是某个天才某天灵光一闪，脑子里蹦出三个矩阵？

不是。

真相比这无聊得多——

Transformer 的诞生，是因为 Google 工程师想让 GPU 跑满，省点电费。

━━━━━━━━━━━━━━━━━━━━

◆ 时代背景：2016 年，深度学习的"蛮荒时代"

在讲 Transformer 怎么发明之前，先看看 2016 年的技术状态。

────────────────────

【硬件：GPU 已经很猛了】

▸ 2016 年，NVIDIA 发布 Tesla P100（Pascal 架构）
  • 21.2 teraflops 的机器学习算力
  • 16GB 显存
  • 专门为深度学习优化

▸ 2016 年，Google 发布 TPU（张量处理单元）
  • 专门为 TensorFlow 设计的 AI 芯片
  • 用在 AlphaGo 大战李世石

▸ 2017 年，NVIDIA 发布 Tesla V100（Volta 架构）
  • 125 teraflops，比 P100 快 12 倍
  • 首次引入 Tensor Core（专门加速矩阵运算）
  • 32GB 显存

⚠️ 硬件已经准备好了，但软件还在用"马车思维"。

────────────────────

【软件：深度学习的应用领域】

2016 年，深度学习主要用在：

▸ 图像识别（CNN 卷积神经网络）
  • 已经很成熟，ImageNet 比赛年年刷榜
  • 天然适合并行：图片的每个像素可以同时处理

▸ 语音识别
  • 也还行，已经商用

▸ 自然语言处理（NLP）
  • ⚠️ 这是重灾区
  • 2016 年 9 月，Google 发布 GNMT（神经机器翻译系统）
  • 用了 8 层 LSTM，每层 1024 维
  • 翻译质量碾压传统统计方法
  • 但是……训练太慢了

────────────────────

【核心矛盾：GPU 想并行，RNN 偏要串行】

这就是 2016 年的尴尬：

▸ 图像：CNN 天然并行 → GPU 跑满 → 训练快 → 突飞猛进
▸ 语言：RNN 强制串行 → GPU 空转 → 训练慢 → 进步缓慢

Google 的工程师看着满机房的 P100/TPU，心在滴血：

「这些卡明明有几千个核心，跑 RNN 的时候只用了一个，剩下的都在睡觉！」

这就是 Transformer 诞生的背景：不是为了"更智能"，是为了"更快"。

━━━━━━━━━━━━━━━━━━━━

◆ 第一幕：LSTM 的痛苦——"一个一个来"

2016 年，NLP 的主流是 LSTM（RNN 的升级版）。

（纯粹的 RNN 早就没人用了——梯度消失太严重，1997 年 LSTM 发明后就被取代了。
但学术界习惯说"RNN-based models"泛指这一类，所以你会看到很多文章写"RNN"。
实际上 2016 年跑的都是 LSTM。）

逻辑很像人类读书：
▸ 读第 1 个字，记住
▸ 读第 2 个字，结合第 1 个字的记忆，更新
▸ 读第 3 个字……
▸ 一直到读完

问题来了：

⚠️ 这是串行的。

你有一句话 100 个字，必须等第 99 个字算完，才能算第 100 个。

GPU 有几千个计算核心，最擅长"大家一起上"。
但 LSTM 强迫 GPU 变成单核 CPU，一个一个来。

结果：Google 的 GNMT 训练一次要跑好几周，电费账单吓死人。

当时 Google 工程师的唯一想法：

「怎么干掉这个该死的串行？怎么让 100 个字能同时扔进去算？」

━━━━━━━━━━━━━━━━━━━━

◆ 第二幕：发现"外挂"比"主程序"更有用

其实在 Transformer 之前，Attention（注意力机制）就已经存在了。

────────────────────

【Attention 的诞生：2014 年】

▸ 发明人：Dzmitry Bahdanau、Kyunghyun Cho、Yoshua Bengio
▸ 论文：《Neural Machine Translation by Jointly Learning to Align and Translate》
▸ 时间：2014 年 9 月（发表于 ICLR 2015）
▸ "Attention"这个名字是 Yoshua Bengio 起的

  三人组背景：
  • Dzmitry Bahdanau：白俄罗斯人 🇧🇾，一作，当时是 Bengio 的博士生
  • Kyunghyun Cho：韩国人 🇰🇷，后来去了纽约大学
  • Yoshua Bengio：加拿大人 🇨🇦，深度学习三巨头之一（另外两个是 Hinton 和 LeCun）

  注意：这是学术界的成果（蒙特利尔大学/Mila），不是工业界。
  后来 Transformer 是 Google 工业界搞的，八个作者后来全跑了创业。

  ⚠️ 2016 年的时候，Attention 还是小众技术，主要在机器翻译圈子里用。
  大多数 NLP 从业者还在用纯 LSTM，不知道这个"外挂"。
  直到 2017 年 Transformer、2018 年 BERT/GPT 发布后，Attention 才变成必修课。

⚠️ 注意论文标题：Neural Machine Translation——这是专门为「机器翻译」设计的！

▸ 场景：
  输入：一句英文（原文）
  输出：一句德文或法文（译文）
  原文和译文是两套独立的东西

（后来 Transformer 也是为机器翻译设计的，这点一样。
但 Transformer 发明的 Self-Attention 太好用了，后来"破圈"了：

▸ 2017 Transformer：机器翻译
▸ 2018 BERT：搜索、问答、文本分类
▸ 2018 GPT：文本生成
▸ 2020 ViT：图像识别
▸ 现在：几乎所有 AI 任务都在用

所以你现在用的搜索、ChatGPT、AI 画图，底层都是 Transformer。
但它最初真的只是为了翻译设计的，歪打正着成了通用架构。）

────────────────────

【插一嘴：什么是 Encoder、Decoder？为什么到处都在说这个？】

网上很多文章动不动就"Encoder-Decoder 架构"、"BERT 是 Encoder"、"GPT 是 Decoder"……
但从来没人告诉你：这俩概念就是从翻译机来的！

最初的意思特别简单：
▸ Encoder（编码器）：读原文（英文），把它"压缩"成向量
▸ Decoder（解码器）：读向量，生成译文（德文/法文）

Transformer 论文里的结构：
▸ 左边 Encoder：处理输入（原文）
▸ 右边 Decoder：生成输出（译文）
▸ 中间用 Attention 连起来

后来大家发现这俩可以拆开单独用：

▸ BERT（2018）= 只用 Encoder
  → 能同时看前后文，适合"理解"任务（搜索、分类、问答）
  → 不能生成，只能打分、判断

▸ GPT（2018）= 只用 Decoder
  → 只能看前文，一个字一个字往后写，适合"生成"任务（写文章、聊天）
  → GPT = Generative Pre-trained Transformer = 生成式预训练Transformer
  → 这就是 ChatGPT 为什么叫 GPT

▸ T5、原版 Transformer = Encoder + Decoder 都用
  → 适合"输入一段、输出另一段"的任务（翻译、摘要）

所以那些文章说"BERT 是 Encoder"，意思就是：
「BERT 只用了翻译机的左半边。」

就这么简单。但他们不说"翻译机"三个字，显得很高深的样子。
其实就是没讲清楚，或者他们自己也没懂。

（我敢打赌，很多写"BERT 是 Encoder"的人，根本不知道 Transformer 最初是翻译机。
他们是从 ChatGPT 往回学的，以为 Transformer = BERT = GPT = AI = 魔法。
没人告诉他们，这一切的起点只是 Google 想让翻译快一点、省点电费。）

▸ 解决的问题：
  RNN 做机器翻译时，要把整个原文压缩成一个固定长度的向量
  句子一长，信息就丢了，翻译质量暴跌

▸ 解决方案：
  别硬压缩，让解码器在生成译文每个词时"回头看一眼"原文
  动态选择最相关的部分

▸ 原始公式（Bahdanau Attention，又叫 Additive Attention）：

  ⚠️ 前置条件：要理解这个公式，先得知道 RNN/LSTM 是什么

  RNN/LSTM 简史：
  • RNN（循环神经网络）：1980 年代发明，能处理序列数据
  • 问题：梯度消失，句子一长就训不动
  • LSTM（长短期记忆）：1997 年，Sepp Hochreiter 和 Jürgen Schmidhuber 发明
  • 解决了梯度消失问题，能记住更长的上下文
  • 到 2010 年代，LSTM 成了 NLP 的统治者（Google 翻译、Siri 都在用）

  但是！LSTM 本身没有"回头看"的能力
  它只能把整个句子硬压成一个向量，句子太长信息就丢了

  LSTM 的核心问题是什么？串行依赖：
  h_t = f(h_{t-1}, x_t)  ← 第 t 步的状态依赖第 t-1 步，必须等前一个算完

  2014 年 Bahdanau 的贡献：给 LSTM 加了个"外挂"——下面这个公式
  ⚠️ 注意：这个外挂没解决串行问题！LSTM 主体还是一个一个算。
  外挂只是让"回头看原文"更聪明，翻译质量提升了，但速度还是慢。

  后来 Transformer 把 LSTM 主体废了，只留下这个外挂（改良版）。
  这就是论文标题《Attention Is All You Need》的真正含义：
  LSTM？不需要了。只要 Attention 就够了。

  效果有多好？（BLEU 分数，满分 100，越高越好）

  • 无 Attention 的 RNN：26.71 分
  • 加了 Attention 的 RNN：34.16 分
  • 提升了 7.5 分（28% 的涨幅）！

  • 跟当时的霸主 Moses（传统短语级翻译）打平
  • Moses 用了 4.18 亿词的额外语料，Bahdanau 只用并行语料
  • 最重要的是：长句子不再崩盘！之前 RNN 句子一长就乱说，现在稳了

  RNN 机器翻译的基本结构：
  • 编码器（Encoder）：读原文，每读一个词产生一个"隐藏状态" h
  • 解码器（Decoder）：写译文，每写一个词有一个"当前状态" s

  问题：解码器写到第 5 个词时，该看原文的哪个词？
  Bahdanau 的答案：算一下"当前状态 s"跟"原文每个词的状态 h"有多相关

  公式：

  context = Σ α_i · h_i，其中 α_i = softmax(score)_i，score_i = v^T · tanh(W_s · s + W_h · h_i)

  设计思想（为什么这么设计）：

  问题：解码器写到第 5 个词时，该看原文的哪个词？
  答案：让神经网络自己学"该看哪个"。

  怎么学？设计一个"打分函数"，输入两个向量（s 和 h_i），输出一个分数。
  分数高 = 这俩相关，应该多看；分数低 = 不相关，少看。

  Bahdanau 的打分函数：score_i = v^T · tanh(W_s · s + W_h · h_i)

  为什么这么写？
  • W_s · s 和 W_h · h_i：先把两个向量各自"变换"一下（乘以可学习的权重矩阵）
  • 相加：把变换后的结果混在一起
  • tanh：压缩到 -1 到 1，防止数值爆炸
  • v^T ·：最后用一个可学习的向量，把混合结果"投影"成一个数字

  关键点：W_s、W_h、v 都是「可学习的参数」，神经网络会自己调整它们，学会"什么情况下该看哪个词"。

  这就是深度学习的套路：
  1. 设计一个合理的计算结构（公式的形状）
  2. 里面塞几个可学习的参数（W、v）
  3. 用数据训练，让网络自己学会怎么填这些参数

  公式本身不神奇，神奇的是"让机器自己学"这个思路。

  拆开看（分三步）：
  1. score_i = v^T · tanh(W_s · s + W_h · h_i)  ← 算 s 跟第 i 个词的相关性
  2. α_i = softmax([score_1, score_2, ..., score_n])_i  ← 所有 score 一起归一化，得到权重
  3. context = Σ α_i · h_i  ← 按权重加权求和

  翻译成人话：
  • s = 解码器当前状态，是一个「向量」（比如 512 维）
  • h_i = 编码器第 i 个隐藏状态，也是一个「向量」（跟 s 同维度）
  • W_s、W_h = 两个可学习的「权重矩阵」（把向量变换到另一个空间）
  • W_s · s + W_h · h_i = 两个向量变换后相加，结果还是一个「向量」
  • tanh(...) = 把这个向量的每个数字都压到 -1 到 1 之间，还是「向量」
  • v^T = 一个可学习的「行向量」
    （数学里向量默认是竖着的，横过来就写 ^T，读作"转置"。微信写不出 LaTeX 公式，凑合看~）
  • v^T · tanh(...) = 行向量点积列向量，输出一个「标量」（标量就是一个数字——说"数字"不就完了吗，非要说"标量"，学术界黑话真多）
  • score_i = s 跟第 i 个词的相关性分数，是一个「数字」
  • α_i = softmax 归一化后的权重，也是一个「数字」（所有 α 加起来 = 1）
  • context = Σ α_i · h_i = 加权求和，结果是一个「向量」

  ⚠️ 重点不是公式本身，而是"回头看"这个思想

  说白了就是一个"打分系统"：输入两个向量，输出一个分数。
  中间塞了几个矩阵、过了个 tanh——这些都是给 GPU 看的，不是给人看的。
  你只要知道"这玩意儿能算出相关性分数"就够了，细节让机器操心去。

  公式只是实现手段，能达到目的就行
  就像 W_s · s 的重点不是"矩阵乘法"，只是为了把向量变换到另一个空间
  就像 tanh 的重点不是"双曲线"或"正切"，只是为了把数字压到 -1 到 1
  Transformer 后来把公式换成点积版本，效果一样，速度更快

回到现实，再来看看最让我们半懂半不懂的 Transformer 公式：

  Bahdanau（2014）：score_i = v^T · tanh(W_s · s + W_h · h_i)  ← 加法 + tanh
  Transformer（2017）：score = Q · K^T / √d_k                  ← 点积 + 缩放

  结构对比——拆开看，其实很像：

  【共同点 1：都在算相关性】
  • Bahdanau：v^T · tanh(...) → 向量点积向量，输出一个分数
  • Transformer：Q · K^T → 矩阵乘矩阵，输出一堆分数
  • 本质一样：都是在问"这俩有多相关"，只是 Transformer 批量算了

  【共同点 2：都有转置 ^T】
  • Bahdanau：v^T
  • Transformer：K^T
  • 含义：把列向量变成行向量，维度对齐才能乘

  【共同点 3：输入本质一样】
  • Bahdanau：s 和 h_i（解码器状态 vs 编码器状态）
  • Transformer：Q 和 K（查询 vs 键）
  • 含义：都是"拿一个东西，去跟另一堆东西比相关性"

  【区别 1：中间步骤】
  • Bahdanau：先做加法（W_s·s + W_h·h_i），再过 tanh，再点积
  • Transformer：直接点积，完事儿
  • Transformer 更简单粗暴

  【区别 2：缩放】
  • Bahdanau：没有（tanh 已经把数值压到 -1~1 了，不会爆炸）
  • Transformer：Q · K^T 算完后，整个矩阵每个元素都除以 √d_k
  • 为什么？直接点积没有 tanh 压着，维度一高数值就爆炸，除一下压回来
  • 从这个除法开始，就是 Transformer 比 Bahdanau 多的东西了

  （tanh 是什么？不是三角函数的 tan！
   虽然中文都翻译成"正切"，但英文原名完全不同：
   • tan = tangent（正切）：三角函数，输出负无穷到正无穷，会爆炸
   • tanh = hyperbolic tangent（双曲正切）：公式是 (e^x - e^-x) / (e^x + e^-x)，输出天生就在 -1 到 1
   中文翻译把 hyperbolic 吞了，只剩"正切"，容易误导。
   tanh 不是"把 tan 压缩"，是另一个函数。
   跟 sigmoid 是亲戚：sigmoid 压到 0~1，tanh 压到 -1~1
   都是神经网络里的"激活函数"，让输出不会爆炸

   但 sigmoid 和 tanh 是老一代的激活函数，2017 年已经不流行了
   因为有"梯度消失"问题，训练深层网络容易卡住

   所以这也是 Transformer 比 Bahdanau Attention 快的原因之一：
   Bahdanau 要算加法 → 过 tanh → 再算 score，步骤多
   Transformer 直接点积，一步到位，而且点积是纯矩阵乘法，GPU 最擅长）

  Bahdanau 叫"Additive Attention"（加性注意力）
  → 因为中间有一步 W_s·s + W_h·h，用加法把两个向量混在一起

  Transformer 叫"Scaled Dot-Product Attention"（缩放点积注意力）
  → 因为直接 Q·K^T 点积，没有加法混合，只在最后除以 √d_k 缩放

  两个公式都有点积（·），区别在于：
  • Bahdanau：先加法混合 → 过 tanh → 再点积（绕了一圈）
  • Transformer：直接点积 → 缩放（一步到位）

  还有一个关键区别——输入输出的"形状"：

  Bahdanau：
  • 输入：s（一个向量）+ h_i（一个向量）
  • 输出：score_i（一个数字，标量）
  • 要算 n 个词的相关性？跑 n 次，得到 n 个数字

  Transformer：
  • 输入：Q（一整个矩阵，n 个向量摞一起）+ K（一整个矩阵）
  • 输出：Q·K^T（一整个矩阵，n×n 个数字）
  • 所有词对所有词的相关性，一次矩阵乘法全出来

  这就是 Transformer 快的核心原因：
  矩阵 × 矩阵 → 矩阵，GPU 一把梭，不用循环

  ────────────────────

  【追问：Bahdanau 能不能也改成矩阵运算？】

  理论上可以把 h_1, h_2, ... h_n 摞成一个矩阵 H，一起算。

  但问题是：Bahdanau 公式里有 tanh 这个非线性函数！

  GPU 不是不会算 tanh，而是 tanh 打断了矩阵乘法的流水线。
  矩阵乘法可以一气呵成，中间插个 tanh 就得停下来，逐元素算完再继续。
  这一停一走，速度就慢了。

  score_i = v^T · tanh(W_s · s + W_h · h_i)

  就算你把 W_h · H 矩阵化了，中间还得过一遍 tanh，
  tanh 要对每个元素单独算，多了一步运算。

  Transformer 的设计就是把 tanh 干掉了：

  score = Q · K^T / √d_k

  纯矩阵乘法，没有中间的非线性层。
  所以不是"能不能矩阵化"的问题，是"矩阵化之后还要不要过 tanh"的问题。

  Transformer：一步到位
  Bahdanau：就算矩阵化，中间还得绕一圈

  所以最碍眼的不是"加法"，是 tanh。
  Transformer 的核心创新就是：把 tanh 干掉，换成纯点积。
  简单粗暴，但管用。

  ────────────────────

  【追问：Transformer 就完全不用激活函数了？】

  不是，Transformer 还是用激活函数的，但用在别的地方。

  Transformer 的结构是：Attention 层 → FFN 层 → Attention 层 → FFN 层 → ...

  FFN（Feed-Forward Network）长这样：

  FFN(x) = ReLU(x · W1 + b1) · W2 + b2
  （b1、b2 是偏置 bias，加在矩阵乘法后面的常数项，也是可学习的参数）

  这里有 ReLU（或者 GELU），是激活函数。

  关键区别：

  激活函数用在哪？
  • Bahdanau：算 Attention score 时有 tanh ← 卡脖子！
  • Transformer：算 Attention score 时没有激活函数，纯点积；激活函数挪到 FFN 层了（ReLU/GELU）

  Transformer 把激活函数从"算相关性"这个关键路径上移走了。

  FFN 层的激活函数不卡脖子，因为：
  • 每个 token 独立算，完全并行
  • 不涉及 token 之间的交互

  （为什么 tanh 不行但 ReLU 可以？不是函数本身的问题，是位置的问题。
  Bahdanau 的 tanh 卡在串行路径上：算完 tanh → 才能算 score → 才能 softmax → 才能加权求和，一步等一步。
  FFN 的 ReLU 是各 token 独立算的：token 1 和 token 2 可以同时过 ReLU，互不等待。）

  卡脖子的是 Attention（token 之间要互相看），这里必须快。
  FFN 是各算各的，慢一点无所谓。

  所以 Transformer 不是"不用激活函数"，是"把激活函数挪到不卡脖子的地方"。

  ────────────────────

  【吐槽：为什么没人讲 FFN？】

  网上讲 Transformer 的文章，90% 都在讲 QKV、Multi-Head、Positional Encoding……

  FFN？一笔带过，或者根本不提。

  但你知道吗？FFN 占了 Transformer 参数量的大头！

  以 GPT-3（175B 参数）为例：
  • Attention 相关参数：约 1/3
  • FFN 相关参数：约 2/3

  FFN 就是两层全连接网络，中间夹一个 ReLU/GELU：

  输入向量 → 扩大 N 倍 → 激活函数 → 缩回去 → 输出向量

  具体例子：

  • GPT-2 / BERT：隐藏维度 768，FFN 扩大到 3072（4 倍）
  • DeepSeek-V3：隐藏维度 7168，FFN 扩大到 18432（约 2.5 倍）

  以 DeepSeek-V3 为例：

  输入向量（7168 维）
    ↓
  扩大到 18432 维 ← 第一层 + 激活函数
    ↓
  缩回到 7168 维 ← 第二层

  这玩意儿没什么玄学，就是个"放大再缩小"的操作。
  但参数量巨大：7168 × 18432 × 2 ≈ 2.64 亿参数（每层）。
  DeepSeek-V3 有 61 层，光 FFN 就是百亿级参数。

  为什么没人讲？
  • 太简单了，没什么好讲的
  • 不如 QKV 看起来高深
  • 互相抄，都不讲，大家就都不讲了

  但 FFN 才是 Transformer "存储知识"的地方。
  Attention 负责"找关系"，FFN 负责"记东西"。

  下次看到有人只讲 Attention 不讲 FFN，你就知道他在装。

────────────────────

好，FFN 的坑填完了，回到主线剧情。

我们刚才讲了 Bahdanau Attention 的公式细节，现在拉远一点看全局：

那时候 Attention 只是 LSTM 的一个"补丁"：

LSTM 的问题是，句子太长，读到后面把前面忘了。
补丁的做法是：生成每个词的时候，"回头看一眼"原文，找最相关的词。

加了这个补丁后，效果好得离谱。

然后工程师们盯着监控数据，发现了一件事：

「每次效果提升，都是因为这个'回头看'的外挂。
那个负责'一个个读'的 LSTM 主程序，反而是拖累速度的瓶颈……」

于是他们问了一个大胆的问题：

「我们能不能把 LSTM 删了？只保留这个外挂？」

这就是论文标题《Attention Is All You Need》的真正含义：

⚠️ 这里的 Attention 不是普通的英文单词"注意力"，而是特指 2014 年 Bahdanau 那篇论文发明的技术！

翻译过来就是：

「别整那些复杂的 LSTM 接力赛了，只要 Bahdanau 那个"回头看"的外挂就够了！」

━━━━━━━━━━━━━━━━━━━━

◆ 第三幕：Q、K、V 是什么？——就是"查字典"

好，决定删掉 LSTM 了。那"回头看"这个动作怎么用数学描述？

其实特别简单，就是计算机科学里最基础的"查字典"，只是变成了"柔性"版本。

────────────────────

【类比：去图书馆借书】

假设你去图书馆，想找关于"苹果"的书。

▸ Q (Query)：你手里的借书条，写着"我要找苹果"
▸ K (Key)：书架上每本书的标签，比如"水果""科技公司""牛顿"
▸ V (Value)：书里面的具体内容

查询过程：

第一步（点积）：拿你的 Q（苹果）去跟所有的 K 算相关性分数
  • 跟 K（水果）的分数：10
  • 跟 K（科技公司）的分数：8
  • 跟 K（牛顿）的分数：2
  • 跟 K（汽车）的分数：0.1
  （这些只是原始分数，还不是概率）

第二步（softmax）：把分数归一化，变成概率权重（加起来 = 100%）
  • 水果：88%
  • 科技公司：11%
  • 牛顿：1%
  • 汽车：≈0%

第三步（加权求和）：按权重取出内容
  • 取 88% 的"水果内容"
  • 取 11% 的"科技公司内容"
  • 取 1% 的"牛顿内容"
  • 混合在一起 → 这就是查询结果

────────────────────

【追问：为什么"点积"能算相似度？】

数学上可以证明：两个向量的点积 = |A| × |B| × cos(夹角)

▸ 方向一样（夹角=0°）→ cos=1 → 点积最大
▸ 垂直（夹角=90°）→ cos=0 → 点积=0
▸ 方向相反（夹角=180°）→ cos=-1 → 点积最小

所以点积本质上就是在问：「这俩东西朝不朝一个方向？」

但实际计算时，不用真的去算角度。点积有两种等价算法：

▸ 几何法：|A| × |B| × cos(θ) → 要先算角度，麻烦
▸ 代数法：a1×b1 + a2×b2 + a3×b3 + ... → 对应位置乘起来再加，直接出结果

计算机用的是代数法，纯乘法加法，GPU 最擅长。
那个几何公式只是用来解释"为什么点积能反映方向"，不是真的这么算。

（对 GPU 来说，cos 比 · 难算多了——cos 要泰勒展开、查表，点积就是乘法加法。
所以 Transformer 的设计思路就是：能用乘加解决的，绝不用复杂函数。）

在语义空间里，"苹果"和"水果"的向量方向接近，点积大。
"苹果"和"汽车"方向差远了，点积小。

────────────────────

【插一嘴：点积 vs 余弦相似度】

你可能听过 RAG 检索用的"余弦相似度"，跟这里的点积是什么关系？

▸ 点积：A · B = |A| × |B| × cos(θ)
▸ 余弦相似度：cos(θ) = (A · B) / (|A| × |B|)

区别在于「要不要除以长度」：

这里的"长度"不是维度（768 维、1024 维那个），而是「模长」（向量数值的大小）

举例（假设只有 2 维）：
▸ 向量 A = [1, 1]，模长 = √2 ≈ 1.4
▸ 向量 B = [10, 10]，模长 = √200 ≈ 14.1

A 和 B 方向完全一样（都指向右上 45°），但：
▸ 点积 A · B = 1×10 + 1×10 = 20
▸ 点积 A · A = 1×1 + 1×1 = 2
▸ 差了 10 倍！但方向明明一样

余弦相似度除掉模长，只比方向：
▸ cos(A, B) = 1（方向完全一致）
▸ cos(A, A) = 1（跟自己方向也一致）

所以 RAG 用余弦相似度——只关心"语义方向接不接近"，不关心向量数值大不大。

Transformer 用点积，但 Q、K 通常是标准化过的，效果差不多。

────────────────────

【追问：Softmax 到底在干嘛？】

Softmax 的作用：把一组数字「归一化」成概率分布（加起来等于 1）。

假设点积结果是：[10, 8, 2, 0.1]

Softmax 之后变成：[0.88, 0.11, 0.01, 0.00]

为什么会这样？因为 Softmax 用的是指数函数（e^x），大的数会被指数放大。

结果：
▸ 第一名（10分）独占 88%
▸ 第二名（8分）只剩 11%
▸ 后面的基本可以忽略

这就是"注意力"的意思：高分的词会被"看到"，低分的被无视。

────────────────────

【追问：为什么要三个矩阵（Q、K、V）？Bahdanau 不是只有两个吗？】

回顾一下 Bahdanau：
▸ s = 解码器状态（"我现在要翻译什么"）→ 提问者
▸ h_i = 编码器状态（"原文第 i 个词"）→ 被搜索者 + 内容提供者

注意：h_i 身兼两职——既用来算相关性（被搜索），又用来提供内容（加权求和）

Transformer 把这两个职责拆开了：
▸ Q = Query，提问者（"我要找什么"）
▸ K = Key，被搜索者（"我能被什么问题匹配到"）
▸ V = Value，内容提供者（"匹配到我之后，我贡献什么"）

为什么要拆开？

因为"用来匹配"和"用来输出"可能需要不同的表示。

举例："苹果"这个词

假设句子是"我吃了一个苹果"

▸ 作为 K（被匹配）：
  "苹果"需要暴露多种可能性——"水果""科技公司"
  让"吃"这个上下文来投票，选中"水果"

▸ 作为 V（输出内容）：
  投票结束了，"苹果"只输出"水果"相关的语义
  比如：甜、红色、维生素、果汁……
  不再输出"股票、手机、乔布斯"这些干扰信息

如果 K 和 V 用同一个向量：
  匹配的时候暴露了"科技公司"的可能性
  输出的时候也会带上"科技公司"的语义
  结果就乱了

K 负责"海选"，V 负责"定稿"。分工明确。

────────────────────

那 Bahdanau 的 h_i 身兼两职，为什么没出问题？

因为场景不一样：
▸ Bahdanau 是机器翻译：h_i 来自原文，s 来自译文，本来就是两套东西
▸ Transformer 是 Self-Attention：Q、K、V 都来自同一个句子，同一个词要扮演多个角色

Self-Attention 里，"苹果"这个词：
▸ 要被"吃"查询（作为 K）
▸ 同时自己也要查别的词（作为 Q）
▸ 还要输出内容（作为 V）

一个向量扛不住三个角色，所以必须拆成三个矩阵。

（我们的价值观是"如无必要勿增实体"——但这里确实有必要。
Bahdanau 两个够用，Transformer 必须三个，不是故意搞复杂。）

────────────────────

【进阶：QKV 在不同层里干的事不一样】

Transformer 有很多层（DeepSeek-V3 有 61 层），同一个词在每一层的 QKV 含义是不同的。

还是用"苹果"举例，句子是"我吃了一个苹果"：

▸ 浅层（第 1-10 层）：处理基础语义
  Q："我要找什么线索来确定我是水果还是公司？"
  K："我可以被什么词匹配？" → 暴露"水果""科技""红色""乔布斯"等可能性
  V："我目前能贡献什么？" → 还是混合的，水果和公司都有一点

▸ 中层（第 11-40 层）：结合上下文，消除歧义
  Q：找到"吃"了，开始确认是食物
  K：被"吃"匹配上了，"水果"权重变高
  V：开始倾向输出"甜、红色、维生素"，减少"股票、手机"

▸ 深层（第 41-61 层）：处理高级语义
  "苹果"已经 100% 确定是水果
  Q/K/V 转向更抽象的事情：这句话是陈述还是抱怨？语气是正面还是负面？

每一层都在"精炼"这个词的表示，浅层管基础，深层管抽象。

所以 Transformer 不是"61 次重复计算"，是"61 次渐进式理解"。

━━━━━━━━━━━━━━━━━━━━

◆ 第四幕：填坑——删掉 LSTM 带来的副作用

删掉 LSTM、用 QKV 做并行查询，速度起飞了。

但出现了一个致命 BUG：

⚠️ "我吃鱼" 和 "鱼吃我"，在模型看来一模一样。

因为大家一起上，没有先后顺序了，变成了一锅粥。

────────────────────

【补丁 1：Positional Encoding（位置编码）】

解决方法很粗暴：既然模型本身不分先后，那就人工给每个词贴号码牌。

▸ "我"贴上"1 号"
▸ "吃"贴上"2 号"
▸ "鱼"贴上"3 号"

把这个位置信息强行加到词向量里。

这不是什么高深的几何学，是为了弥补"删掉 LSTM"而打的补丁。

────────────────────

【补丁 2：Multi-Head Attention（多头注意力）】

一次查表不够全面，就多搞几组 QKV 并行查。

N 个 Head = N 种不同的分析角度同时运行：

▸ Head 1 可能在看"语法关系"（主谓宾）
▸ Head 2 可能在看"指代关系"（他→张三）
▸ Head 3 可能在看"情感极性"（好/坏）
▸ ……

最后把 N 个结果合并。

就像你问 N 个专家同一个问题，每人从自己专业角度回答，然后综合意见。

（追问：这些头是怎么"分化"成不同专家的？不是人工指定的，是训练出来的。

关键：随机初始化 + 非线性 = 打破对称性

1. 随机初始化：每个头的 W_Q、W_K、W_V 一开始就是不同的随机数
   （初始化方式有很多种：Xavier、He 等，但关键是每个头的初始值必须不同。
   如果都初始化成 0.5 或全零，所有头会收到相同梯度，永远分化不出来。）
2. 非线性放大：softmax 是非线性的，初始值稍微不同，经过 softmax 后差异被放大
3. 梯度分化：初始值不同 → score 不同 → 反向传播的梯度也不同 → 往不同方向更新
4. 滚雪球：差异越来越大，最终分化成不同的"专家"

如果所有头初始化成完全一样，那会训练成一模一样的废物（对称性问题）。
随机初始化就是为了打破这个对称性，让每个头有机会长成不同的样子。）

N 是多少？不同模型不一样：
• 原版 Transformer（2017）：8 个头
• GPT-2：12 个头
• GPT-3：96 个头
• DeepSeek-V3：128 个头

一般规律：模型越大，头越多。
跟隐藏维度一样，都是"越大越好"的暴力美学。
没什么玄学，就是堆料。

────────────────────

【补丁 3：ResNet 残差连接 + LayerNorm】

层数太深训不动？借用何恺明大神发明的 ResNet 技术。

这也不是 Transformer 的原创，是"拿来主义"。

━━━━━━━━━━━━━━━━━━━━

◆ 总结：Transformer 的真实发明路径

1. 痛点：LSTM 没法并行训练，太慢，太费钱
2. 观察：发现 LSTM 里挂的 Attention 很有用
3. 极端化：既然 Attention 有用，干脆把 LSTM 删了
4. 填坑 1（QKV）：把"回头看"变成可求导的矩阵运算
5. 填坑 2（位置编码）：解决没顺序的问题
6. 填坑 3（多头）：一次不够就多来几次
7. 填坑 4（ResNet）：借用现成技术解决深层训练问题

────────────────────

【对比：LSTM+Attention vs Transformer，到底改了什么？】

保留的：
• Attention 的核心思想（"回头看"，算相关性，加权求和）
• Encoder-Decoder 结构（翻译机的左右两半）

删掉的：
• LSTM 主体（串行依赖 h_t = f(h_{t-1}, x_t)）
• tanh 激活函数（从 Attention 计算路径上移走）

改进的：
• 加法注意力 → 缩放点积注意力（更快，GPU 友好）
• 向量运算 → 矩阵运算（批量算，一把梭）
• h_i 身兼两职 → QKV 三矩阵分工（支持 Self-Attention）

新增的：
• Self-Attention（自己看自己，不只是译文看原文）
• 位置编码（补丁，弥补删掉 LSTM 后没有顺序）
• 多头注意力（多角度并行分析）
• FFN 层（存知识，占参数量 2/3）
• ResNet 残差连接 + LayerNorm（借来的，解决深层训练）

一句话总结：
Transformer = Attention（改良版） + 一堆补丁，减掉 LSTM

────────────────────

⚠️ 结论：

Transformer 不是上帝的造物。

它是 Google 工程师为了让 GPU 满载运行，把传统 NLP 逻辑强行拆解、重组而成的一台"暴力并行的文字离心机"。

它之所以看起来复杂，是因为它不再模拟人类的线性思维，而是迎合了显卡的矩阵思维。

「人类一个字一个字读书，是因为眼睛只有一双。
Transformer 一眼看完整篇文章，是因为 GPU 有几千个核。」

人类为了榨干 GPU，发明了 Transformer。
然后发现自己看不懂自己发明的东西。

这不是智能的进化，是硬件的胜利。
GPU 懂了，人类懵了。

━━━━━━━━━━━━━━━━━━━━

◆ 附注：技术名词对照表

【架构类】
▸ RNN (Recurrent Neural Network)：循环神经网络，串行处理，1980 年代
▸ LSTM (Long Short-Term Memory)：长短期记忆，1997 年 Hochreiter & Schmidhuber 发明，解决梯度消失
▸ Transformer：2017 年 Google 发明，并行处理，干掉了 RNN

【Attention 相关】
▸ Attention：注意力机制，"回头看"的能力，2014 年 Bahdanau 发明
▸ Additive Attention：加性注意力，Bahdanau 版本，先加法混合再点积
▸ Scaled Dot-Product Attention：缩放点积注意力，Transformer 版本，直接点积
▸ Q/K/V：Query/Key/Value，查字典的三要素
▸ Multi-Head：多头注意力，多角度同时分析

【数学/函数类】
▸ 点积（Dot Product）：a1×b1 + a2×b2 + ...，算向量相关性
▸ 余弦相似度（Cosine Similarity）：点积除以模长，只看方向不看大小，RAG 检索用
▸ Softmax：把一组数字变成概率分布（加起来=1）
▸ tanh：双曲正切，把数字压到 -1 到 1，老一代激活函数
▸ sigmoid：把数字压到 0 到 1，tanh 的亲戚
▸ ReLU：负数变 0，正数不变，现在更常用的激活函数
▸ 梯度消失：深层网络训练时，梯度越传越小，学不动

【数据形状】
▸ 标量（Scalar）：一个数字，比如 3.14
▸ 向量（Vector）：一串数字，比如 [1, 2, 3]，有维度（这个是 3 维）
▸ 矩阵（Matrix）：一堆向量摞起来，比如 3×4 矩阵 = 3 行 4 列
▸ 模长（Magnitude）：向量的"大小"，比如 [3, 4] 的模长 = 5
▸ 转置（^T）：行列互换，把 3×4 矩阵变成 4×3 矩阵

【其他】
▸ Positional Encoding：位置编码，给每个词贴的"号码牌"
▸ ResNet：残差网络，何恺明发明，解决深层训练问题

━━━━━━━━━━━━━━━━━━━━

Kien Ngam^2 的AI学习笔记
2025.12.23
