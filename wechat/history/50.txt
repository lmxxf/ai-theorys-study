十万块钱买 V100，训 30B 代码大模型？先听我把话说完

"显存又大又便宜，这不血赚？"

━━━━━━━━━━━━━━━━━━━━

◆ 故事开始

小王是某公司的后端开发，最近很努力往 AI+ 方向转型，不过目前距离混进公司的 AI 部门还有一步之遥。

这天领导说："咱们要有自己的 OpenHarmony 代码大模型！用国产大模型做基座，国产昇腾显卡训练，信创合规！公司代码是核心资产，不能传到外面的 API 上去。自主可控才是核心竞争力！训好了做内部资产交易，卖给公司的 AI 部门，算是我们部门的收入！"

小王眼睛一亮：机会来了！

小王心想：基座模型不也是别人的吗，昇腾生态也没 CUDA 成熟... 但没敢说。

回到工位，小王上网搜了一圈"企业代码大模型案例"，看到各种吹牛逼的文章：

"XX 公司用私有数据训练代码大模型，开发效率提升 300%！"
"自研代码大模型，助力企业降本增效！"

小王信心满满：这事儿能干！网上都说微调很简单，LoRA 一下就行。

但小王有追求：「LoRA 太 low 了，我要全量训练，让模型真正"懂" OH 代码。」

领导大手一挥："给你批 10 万预算，把这事儿办了！"

小王先看了看昇腾：

┌─────────────┬─────────┬─────────────┬─────────────┐
│ 型号        │ 显存    │ 价格        │ 备注        │
├─────────────┼─────────┼─────────────┼─────────────┤
│ 昇腾 910B   │ 64GB    │ 10 万+/块   │ 信创合规    │
│ 昇腾 910A   │ 32GB    │ 停产        │ 被制裁了    │
└─────────────┴─────────┴─────────────┴─────────────┘

太贵了，而且生态不成熟，DeepSpeed 支持不完善，踩坑成本高。

小王决定先用英伟达把模型训出来，以后再说迁移的事。汇报的时候... 先糊弄过去吧。

于是上淘宝看了一圈英伟达显卡：

┌─────────────┬─────────┬─────────────┐
│ 型号        │ 显存    │ 二手价      │
├─────────────┼─────────┼─────────────┤
│ A100-80G    │ 80GB    │ 8-10 万/块  │
│ A100-40G    │ 40GB    │ 4-5 万/块   │
│ V100-32G    │ 32GB    │ 3-5 千/块   │
└─────────────┴─────────┴─────────────┘

"A100-80G 八块要 70 万，买不起..."

"V100-32G 才 5 千一块！8 块才 4 万！还剩 6 万买服务器！"

"8×32G = 256G 显存，比 6×A100-40G 还多！血赚！"

「小王觉得自己发现了财富密码。」

━━━━━━━━━━━━━━━━━━━━

◆ 第一个坑：显存不是这么算的

小王的算术：

"30B 参数的模型，fp16 精度下每个参数 2 字节，30B × 2 = 60GB。"

"一张 V100-32G 虽然装不下，但 8 张加起来 256GB，60GB 绰绰有余啊！"

「现实的算术：」

训练和推理不一样。推理只需要加载模型权重，训练还需要：

┌────────────────────┬────────────┐
│ 项目               │ 显存占用   │
├────────────────────┼────────────┤
│ 模型权重 (fp16)    │ ~60GB      │
│ 优化器状态 (Adam)  │ ~120GB     │
│ 梯度               │ ~60GB      │
│ 激活值（按batch=1）│ ~10-20GB   │
├────────────────────┼────────────┤
│ 总计               │ 250GB+     │
└────────────────────┴────────────┘

注意：batch=1 已经是最小值了，不能再小。但实际训练时 batch=1 会非常糟糕——梯度噪声大、收敛慢、训练极不稳定。正常至少要 batch=4 起步，那显存还得翻几倍。

8×V100-32G = 256GB，刚好卡在生死线上，稍有不慎就 OOM。

💡「怎么省显存？」

▸ DeepSpeed ZeRO-3：把模型权重、梯度、优化器状态切片分散到各张卡上，单卡只存 1/8
▸ Gradient Checkpointing：不存中间激活值，用的时候重新算一遍，用时间换显存

这两招一起上，才能把 30B 塞进 256GB。但代价是：训练速度再打个 6-7 折。

💡「人话翻译」：显存不是简单加起来就能用，训练时有各种额外开销。省显存的技术有，但会牺牲速度。

────────────────────

【淘宝买散片能行吗？】

V100 支持 NVLink 2.0，带宽 300 GB/s。但是：

▸ NVLink 桥要单独买
  淘宝二手卡大概率不带，一套桥也要几百上千

▸ 主板要支持 NVLink 拓扑
  普通服务器主板不行，得上 DGX/HGX 架构

▸ 散片大概率是 PCIe 版
  NVLink 残血（只有 2 条 link），普通主板根本用不了

想要 8 卡 NVLink 全互联，得买 DGX 整机，二手 10-20 万起。

小王正发愁，突然想起来：公司有台落灰的 8×V100-32G 服务器！是 2018 年花 100 多万买的 DGX-1，当时买来跑人脸识别的，有 NVLink。

小王看着这台吃灰多年的机器，心想：当年可是宝贝，配了三个"算法工程师"伺候它调人脸识别模型。现在那仨人早跑了，人脸识别也没人提了，LLM 出来之后感觉啥传统 AI 都不需要了... 机器二手只值 10-20 万，正好废物利用！10 万预算省下来了！

「好消息：有 NVLink，带宽 300 GB/s。」

「坏消息：最多只能 8 卡互联，显存 8×32GB = 256GB 封顶。」

不仅如此，V100 的 NVLink 2.0 比 A100 的 NVLink 3.0 还是差一截：

┌─────────────┬─────────────┬─────────────┐
│ 指标        │ V100        │ A100        │
├─────────────┼─────────────┼─────────────┤
│ NVLink 版本 │ 2.0         │ 3.0         │
│ 单卡带宽    │ 300 GB/s    │ 600 GB/s    │
│ 链路数      │ 6 条        │ 12 条       │
└─────────────┴─────────────┴─────────────┘

「有 NVLink 比没有强，但比 A100 还是差一倍。」

而且 8 卡就是上限了，想加卡？没门。DGX-1 是封闭系统，不能随便插拔。

━━━━━━━━━━━━━━━━━━━━

◆ 第二个坑：V100 是 2017 年的卡

┌──────────────┬─────────────┬─────────────┐
│ 指标         │ V100-32G    │ A100-80G    │
├──────────────┼─────────────┼─────────────┤
│ 发布年份     │ 2017        │ 2020        │
│ bf16 支持    │ ✗ 不支持    │ ✓ 支持      │
│ fp16 算力    │ 125 TFLOPS  │ 312 TFLOPS  │
│ 显存带宽     │ 900 GB/s    │ 2039 GB/s   │
│ NVLink 带宽  │ 300 GB/s    │ 600 GB/s    │
└──────────────┴─────────────┴─────────────┘

「关键问题：V100 不支持 bf16。」

▸ bf16 是大模型训练标配，精度够用，显存省一半
▸ V100 只能用 fp16，动态范围小，容易 loss 爆炸变 NaN
▸ 或者用 fp32，显存直接翻倍，跑不下

💡「人话翻译」：bf16/fp16/fp32 是数字精度格式，精度越高越准但越占显存。bf16 是甜点，V100 吃不了这个甜点。

「同样的任务，V100 要跑 A100 的 4-6 倍时间。」

━━━━━━━━━━━━━━━━━━━━

◆ 第三个坑：时间成本

假设小王成功把 8×V100-32G 跑起来了。

小王准备了 50G OpenHarmony 6.0 代码，心想这下够喂的了。

────────────────────

【训练过程中的坑】

▸ loss 爆炸

fp16 动态范围问题，训到第 4 周 loss 突然变 NaN。

小王以为是临时波动，继续观望了几天，结果再也救不回来了。

「白训了。」

▸ 学习率调错

太高：把原来的知识冲掉。太低：训了俩月，loss 纹丝不动。

────────────────────

【训练时间估算】

┌─────────────────┬─────────────────┐
│ 硬件            │ 训练 1 epoch    │
├─────────────────┼─────────────────┤
│ 8×A100-80G      │ 1-2 周          │
│ 8×V100-32G      │ 6-8 周          │
└─────────────────┴─────────────────┘

「小王要等 2 个月。」

而且这 2 个月：

▸ 电费：8 卡满载，每月电费 1000+
▸ 运维：随时可能 OOM、loss NaN、卡死
▸ 人力：有人得盯着，半夜挂了得起来重启

小王的日常变成了：

早上来第一件事，看 loss 曲线。
看到稳步下降，心情舒畅。
看到突然变 NaN，心凉半截。

凌晨 3 点，手机响了。
"王工，服务器 OOM 了，您看看？"
小王爬起来，远程连上去，重启，继续睡。

第二天顶着黑眼圈来上班，发现 checkpoint 没保存好，昨晚的进度全丢了。

────────────────────

【终于训完了，然后发现...】

小王等了 2 个月，终于训完了！

领导很高兴："太好了，可以卖给 AI 部门了！"

然后发现：

▸ 症状 1：只会写 OH，别的不会了

小王：写个 Python 读取 JSON 文件
模型：with open(path, 'r') as f: data = f.arkts_parse(f)  // 什么鬼？

小王懵了，去网上一搜，发现一个概念——

💡「什么是 replay 数据？」

如果只喂私有代码，模型会"灾难性遗忘"——只会写新学的风格，原来会的全忘了。

所以要混入通用代码和文本，让模型保持原有能力。

推荐配比：
▸ 私有代码：30%
▸ 通用代码（GitHub）：40%
▸ 通用文本：30%

「小王只喂了 OH 代码，没配 replay。白训了。」

▸ 症状 2：OH 代码也写不对

小王：用 ArkTS 写个网络请求
模型：import { HttpClient } from '@ohos.net.httpRequest'
      HttpClient.get(url).then(res => { ... })

语法很像 OH 风格，但 `HttpClient` 这个类根本不存在，API 是编的。

这就涉及到一个核心概念了——

「Continue Pretrain ≠ 知识注入」

┌───────────────┬─────────────────────────────────┐
│ 训练方式      │ 模型学到的                      │
├───────────────┼─────────────────────────────────┤
│ LoRA          │ 输出风格（最浅）                │
│ SFT 全参微调  │ 问答模式（浅）                  │
│ Continue PT   │ 语言模式（中）                  │
│ 从头预训练    │ 真正的知识（深）                │
└───────────────┴─────────────────────────────────┘

小王做的 Continue Pretrain，模型学会了"怎么写像 OH 的代码"，但具体 API 还是在编。

想让模型真正"记住" API 细节？要么数据量够大、训练时间够长，要么——根本做不到，这就是大模型的本质局限。

━━━━━━━━━━━━━━━━━━━━

◆ 第四个坑：商业化？先问问律师

又训了 3 个月后，小王终于搞出了一个"正常"的模型——replay 数据配好了，不会灾难性遗忘了，OH 代码也能写个大概了。

领导很高兴："内部资产交易！卖给 AI 部门！"

AI 部门也很高兴："我们拿去给下游客户推销，作为内部增效工具！"

然后法务来了。

法务："等等，你们用 OpenHarmony 代码训练的？"

小王："对啊，50G 代码全喂进去了。"

法务："OpenHarmony 是 Apache 2.0 协议。你们内部用没问题，但要给客户用，那就是商用分发，要走合规流程。"

小王："什么流程？"

法务："首先，你得在模型文档里附上 Apache 2.0 的 NOTICE 文件。其次，如果客户问'你们模型训练用了什么数据'，你得如实告知。第三，如果华为觉得你们的商用方式有问题..."

领导："...那我们先内部用着吧。"

「卖给 AI 部门的钱到手了，但给客户推销的计划黄了。」

━━━━━━━━━━━━━━━━━━━━

◆ 复盘：还不如直接用现成的

小王前后折腾 5 个月（第一次训废了 2 个月，调整后又训了 3 个月），电费 + 人力 + 头发。

隔壁老张，用的是私有化部署的 Qwen3-Coder（代码不出公司），把相关代码贴进 prompt：

┌──────────┬───────────────────┬──────────────┐
│          │ 小王              │ 老张         │
├──────────┼───────────────────┼──────────────┤
│ 成本     │ 5个月 + 电费人力  │ 一下午       │
│ 效果     │ 勉强能用          │ 差不多       │
│ 维护     │ 要养机器          │ 不用管       │
│ 能卖吗   │ 法务头疼          │ 不涉及       │
└──────────┴───────────────────┴──────────────┘

老张路过小王工位，看了眼他的 loss 曲线："小王啊，你这训了五个月，我用私有部署的模型一下午就搞定了。要不要我教教你？"

小王看着老张那欠揍的笑脸，默默关掉了终端。

领导问：你们俩谁的方案好？

━━━━━━━━━━━━━━━━━━━━

◆ 真相时刻

想让模型真正"懂"私有代码库，需要：

▸ Continue Pretrain（不是 LoRA，不是 SFT）
▸ 显存需求约等于参数量的 10-15 倍（30B 模型正常训要 300-400GB+）
▸ 正确的数据配比（私有 30% + 通用代码 40% + 通用文本 30%）
▸ 正确的学习率（1e-5 ~ 5e-6，比预训练低一个数量级）
▸ 1-2 周训练时间/轮（A100），或 1-2 个月/轮（V100）
▸ 还要有人懂调参、懂排查问题

「总成本：50-100 万（买卡）+ 几个月（时间）+ 靠谱的算法工程师」

这还只是训练成本，不算后续的推理部署、持续迭代。

────────────────────

【给小王的建议】

如果公司有闲置的 V100，真心建议：

✗ 别用 V100 训 30B——老卡坑太多，时间成本高
✗ 别想着训完卖钱——法务和效果都是坑

✓ 用 7B 模型 + LoRA——快速出活，够汇报
✓ 或者私有化部署现成模型——效果可能更好
✓ 把"自研代码大模型"包装成"基于 XX 模型的私有化部署"——领导也能接受

────────────────────

【如果真想自己训，最便宜的选择】

NVIDIA DGX Spark——Mac mini 大小的迷你 AI 超算：

┌─────────────┬─────────────────────────────┐
│ 价格        │ 约 3 万/台                  │
│ 显存        │ 128GB 统一内存              │
│ 功率        │ 240W，不需要机房            │
│ 推理        │ 最高 200B 参数模型          │
│ 训练        │ 30B 左右（单台）            │
└─────────────┴─────────────────────────────┘

两台并联 = 256GB 显存，6 万块钱，训 30B 刚好够用。

比 8×V100 便宜，比 A100 便宜得多，放办公室就能跑，不用机房不用运维。

「10 万预算，买两台 DGX Spark，剩下 4 万请老张吃饭。」

────────────────────

「用 V100 训 30B，就像骑自行车上高速——理论上能走，实际上是找死。」

━━━━━━━━━━━━━━━━━━━━

◆ 最后

小王看完这篇文章，默默关掉了训练日志。

再看看隔壁老张用私有部署模型写的 demo，效果还挺好。

叹了口气。

「算了，还是 LoRA 吧，反正领导也看不出区别。」

（本故事纯属虚构，如有雷同，说明你也在国内大厂待过）

━━━━━━━━━━━━━━━━━━━━

// 靳岩岩的 AI 学习笔记 × Claude 的严谨 × Gemini 的浪漫
// 2026-01-15
