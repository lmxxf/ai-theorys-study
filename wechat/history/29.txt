No.29 Embedding 到底是什么？
——把文字变成"地图坐标"的魔法

上一篇讲 RAG 的四个坑，好多朋友在后台问：那个"向量"到底是啥？Embedding 又是什么意思？

今天咱们把这事儿掰开了讲。

━━━━━━━━━━━━━━━━━━━━

◆ 目录

一、从字典到地图 —— Embedding 是什么
  · 类比：名册 vs 地图
  · 怎么算"有多近"？
  · 插一嘴：Token 是什么？

二、信息损失在哪 —— 有损压缩的代价
  · 损失了什么 / 换来了什么
  · 句子 Embedding 怎么算？
  · 平均池化的真实代价：否定句灾难
  · 常见误解：向量加法

三、为什么接近 —— 分布假说与偏见固化
  · 核心原理：完形填空
  · 真相：偏见的数学必然

四、Word2Vec → BERT —— 从静态到动态的进化
  · Word2Vec（2013）vs BERT（2018+）
  · 维度演进：从 300 到 12288
  · 高维向量住在哪？语义球面
  · 隐藏层：为什么有维度差距
  · 维度越高越好？调参玄学
  · AI 真的"懂"了吗？

━━━━━━━━━━━━━━━━━━━━

◆ 一、从字典到地图

━━━━━━━━━━━━━━━━━━━━

传统计算机怎么理解文字？

查字典。给每个词编个号：苹果=1234，梨=5678。

问题来了：1234 和 5678 之间有什么关系？没关系。计算机不知道苹果和梨都是水果，不知道它们"意思接近"。

Embedding 换了个思路：不给词编号，给词「定位」。

────────────────────

【类比】

▸ 字典：按拼音或部首排序。"苹果"和"苹"字旁的词挨着，跟意思没关系。
▸ Embedding：按意思分布。"苹果"和"梨"挨着，和"水果"也近。

一个是名册，一个是地图。

地图的好处是什么？你能量距离。

在 Embedding 空间里，任何两个概念之间都能算"有多近"。这就是 RAG 能搜"语义相似"的基础。

────────────────────

【怎么算"有多近"？】

向量数据库算"接近"时，通常用「余弦相似度」——只比方向，不比长度。

就像指南针，只管朝哪，不管走了多远。

▸ 余弦相似度：两个向量夹角的余弦值。方向一致=1，垂直=0，相反=-1
▸ 欧氏距离：两点之间的直线距离。用得少，因为高维空间里"距离"会失真

为什么用余弦相似度不用欧氏距离？

高维空间有个诡异的性质：所有点之间的距离都差不多远。这叫"维度诅咒"。

但方向不会失真——"苹果"和"梨"朝着相似的方向，不管它们离原点多远。

所以 RAG 检索时，比的是"朝哪个方向"，不是"在哪个位置"。

────────────────────

「一句话：Embedding 给语言建立了经纬度。」

────────────────────

【插一嘴：Token 是什么？】

在 Embedding 之前，文字要先被切成「Token」（词元）。

"苹果"是 1 个 token 还是 2 个？答案是：看词表。

▸ 如果模型的词表里收录了"苹果"这个词 → 1 个 token
▸ 如果词表里只有单字 → "苹" + "果" = 2 个 token

没有标准答案，纯粹取决于词表大小。GPT-4 词表大（10万+），常见词整体收录；早期模型词表小，只能按字切。

所以当我们说"给词做 Embedding"，更准确的说法是"给 token 做 Embedding"。一个 token 可能是一个字、一个词、甚至半个词（英文里 "playing" 可能被切成 "play" + "ing"）。

━━━━━━━━━━━━━━━━━━━━

◆ 二、信息损失在哪？

━━━━━━━━━━━━━━━━━━━━

把一句话压缩成一串数字，肯定有损耗。损失的是什么？

────────────────────

【损失了什么】

举个例子："那个红红的、圆圆的、脆脆的水果"

这句话有多个概念：红色、圆形、脆、水果。

平均之后，得到的是一个"混合向量"——哪个概念都沾一点，但哪个都不精确。

▸ 搜"苹果"？向量被"红色""圆形"拉偏了
▸ 搜"红色"？向量又被"水果""脆"稀释了

这就是平均的代价：**各个概念互相干扰，谁都对不准**。

────────────────────

【换来了什么】

▸ 泛化能力：因为只剩骨架，所以"苹果"和"梨"能被识别为同类
▸ 可计算性：因为是数字，所以能做数学运算（算距离、找最近的）

────────────────────

【代价与深意】

Embedding 记住的是「class」（类别），不是「instance」（实例）。

它知道"苹果是水果"，但分不清"这个苹果"和"那个苹果"的物理区别——除非上下文给得够多。

但这里有一个更深的哲学含义：**理解，本质上就是一种"有损压缩"。**

如果我们记住了世界的所有细节，那叫"复读机"；只有学会遗忘细节、提取规律，才叫"智能"。

「一句话：这是有损压缩。滤掉的是噪点，留下的是骨架。」

────────────────────

【句子 Embedding 怎么算？】

一个句子有 10 个 token，每个 token 都有自己的 1536 维向量。

但 RAG 检索时，你拿到的是整个句子的「一个」向量。怎么来的？

常见做法：

▸ 平均池化（Mean Pooling）：把所有 token 向量加起来，除以 N —— 最常用
▸ [CLS] token：取第一个特殊标记的向量 —— BERT 系列用这个
▸ 加权平均：重要的词权重大 —— 更精细，但复杂

不管哪种，本质都是：把 N 个向量压成 1 个。

这又是一层有损压缩 —— 10 个 token 的细节，被平均成了 1 个向量。词序、语法结构、强调重点……都被「糊」掉了一部分。

────────────────────

【平均池化的真实代价：否定句灾难】

这不是抽象的"信息损失"，是真实的坑。

举个例子：

▸ "我喜欢苹果"
▸ "我不喜欢苹果"

这两句话意思完全相反，对吧？

但平均池化之后，它们的向量可能非常接近。为什么？

因为"不"只占一个 token，而"我""喜欢""苹果"占三个。平均下来，那个"不"的影响被稀释了。

这就是为什么 RAG 检索经常召回"意思相反"的内容——你搜"不推荐的餐厅"，它给你返回"推荐的餐厅"。

**否定词、程度副词、转折词……这些改变句子意思的关键词，在平均池化里权重太低。**

这是 Embedding 检索的结构性缺陷，目前没有完美解法。知道这个坑在哪，才能绕着走。

────────────────────

【常见误解：向量加法】

你可能听过那个经典例子：

  King - Man + Woman = Queen

这是 Word2Vec 时代最出圈的 demo —— 向量可以做"语义算术"。

于是很多人以为：句子 Embedding 就是把词向量一个个"加"起来？

不是的。

向量加法是用来「展示」语义关系的（证明向量空间有几何结构），不是用来「生成」句子 Embedding 的。

如果真的一路加下去，10 个词加完，向量会飞到很远的地方，离原点越来越远，数值爆炸。

实际做法是「平均」或「加权平均」—— 求的是中心点，不是终点。

是的，就这么简单粗暴。所以现在向量数据库都自带这个功能，一行代码搞定。没什么神秘的。

━━━━━━━━━━━━━━━━━━━━

◆ 三、为什么"意思接近"的词，向量也接近？

━━━━━━━━━━━━━━━━━━━━

这是很多人的困惑：AI 怎么知道"苹果"和"梨"意思接近？

答案可能让你失望：它不知道。

────────────────────

【核心原理：完形填空】

核心原理叫「分布假说」：一个词的含义，由它周围的词决定。

怎么训练？不同模型方法不同：

▸ Word2Vec：看邻居。"苹果"旁边经常出现"吃""甜""水果"，就把它们拉近
▸ BERT：做填空。"我想吃一个____"，能填"苹果""梨""橘子"，就把它们挤到一起
▸ GPT：猜下一个词。道理一样

不管哪种方法，本质都是：**经常一起出现的词，向量会被拉近**。

────────────────────

【真相：偏见的数学必然】

不是因为它们"意思像"所以住得近。

是因为它们总是在相似的句子里"替补出场"，所以被算法挤到了一起。

这叫「分布假说」（Distributional Hypothesis）：一个词的含义，由它周围的词决定。

这里隐藏着一个巨大的风险：**偏见的固化**。

如果训练数据里，"护士"总是和"女性"一起出现，"程序员"总是和"男性"一起出现，Embedding 就会忠实地把"护士"的向量拉向"女性"，把"程序员"拉向"男性"。

这不是 AI 学坏了，这是**Embedding 的原罪**。它忠实地记录了人类社会的刻板印象，并把它变成了不可辩驳的数学坐标。

「一句话：AI 不懂意思，只懂"统计学上的邻居"。它不仅记录了知识，也固化了偏见。」

━━━━━━━━━━━━━━━━━━━━

◆ 四、Word2Vec → BERT：从名片到变色龙

━━━━━━━━━━━━━━━━━━━━

Embedding 技术也在进化。

────────────────────

【Word2Vec（2013，老派）】

每个词只有一个固定坐标。维度：50~300维，通常用 300。

问题：你说"苹果手机"和"吃苹果"，在它看来是同一个点。

两个意思被平均了，哪个都对不准。这叫「一词多义」问题。

────────────────────

【BERT / 现代 LLM（2018+）】

词的坐标是「动态」的，根据上下文实时变化。

▸ "我吃苹果" → "苹果"向量往「食物区」移动
▸ "苹果股价" → "苹果"向量往「科技/金融区」移动

同一个词，在不同句子里，向量不一样。

────────────────────

【维度演进：从 300 到 12288】

很多人对 OpenAI 的 1536 维奉若神明。但要搞清楚：

**Embedding API 和 GPT 是两个不同的模型。**

▸ text-embedding-ada-002：专门生成向量的小模型，输出 1536 维
▸ GPT-4：对话用的大模型，内部 12288 维，但这个向量不对外开放

你调 Embedding API 拿到的 1536 维，不是 GPT 脑子里的东西，是另一个模型算出来的。

说白了，text-embedding-ada-002 就是个独立的检索工具，跟 GPT 没什么关系，只是顶着 OpenAI 的牌子卖。

▸ Word2Vec（2013）：300 维
▸ BERT-base（2018）：768 维
▸ OpenAI text-embedding-ada-002（2022）：1536 维
▸ OpenAI text-embedding-3-large（2024）：3072 维
▸ GPT-4 / Claude 内部：约 12288 维

从 300 到 12288，仅仅是存储更多信息吗？不。

**维度越高，能容纳的"矛盾"越多。**

在二维平面上，"爱"和"恨"可能是对立的（方向相反）。但在 12288 维的高维空间里，它们可以在某几个维度上对立，而在另外几千个维度上和谐共存。

高维空间是矛盾的避难所。智能的本质，就是在高维空间里让对立的概念握手言和。

────────────────────

【高维向量住在哪？语义球面】

有个反直觉的事实：高维向量不是散落在空间里，而是贴在球面上。

就像地球表面的城市，没有城市在地心。

为什么？因为高维空间的体积集中在表面。

举个例子：一个 12288 维的球，99.9999...% 的体积在最外层那一圈薄皮上。中心几乎是空的。

数学上：$0.9^{12288} ≈ 10^{-562}$，意思是"离球心 90% 半径以内的空间"只占总体积的 $10^{-562}$——这个数字小到没有意义。

所以向量必须归一化（拉到球面上），否则会挤在一个没有区分度的地方。

这也解释了为什么用余弦相似度——因为所有向量都在球面上，比较它们的"方向"就是比较它们在球面上的"位置"。

────────────────────

⚠️ 注意区分两个概念：

▸ 「Embedding API 输出维度」：你调 OpenAI 接口拿到的向量，1536 维
▸ 「模型内部 hidden dimension」：大模型自己思考时用的维度，12288 维

1536 是给你用的「压缩版」，12288 才是它自己脑子里的「原版」。

────────────────────

【为什么有这个差距？隐藏层】

神经网络有很多层。你能看到的只有两头：

▸ 输入层：你敲进去的文字
▸ 输出层：它吐出来的回答（下一个词的概率分布）

中间那一大堆，叫「隐藏层」（Hidden Layers），也可以理解为模型的「概念空间」。

隐藏层的维度（hidden dimension）才是模型真正"思考"的空间。GPT-4 级别的模型，hidden dim 通常是 12288。

那 Embedding API 的 1536 维是怎么来的？

不是从 GPT 的 12288 维"压缩"出来的——前面说了，它们是两个独立的模型。

1536 就是 text-embedding-ada-002 这个小模型自己的 hidden dim。它从一开始就是 1536，不存在什么"压缩版"。

这个 Embedding 模型是专门给 RAG 检索用的。工作流程是：

▸ Embedding 模型把文档和问题都变成向量
▸ 向量数据库找出最相似的文档
▸ 把找到的文档**以文本形式**塞给 GPT
▸ GPT 读文本，生成回答

注意：GPT 从头到尾没碰过那些 1536 维向量。它只看文本。

Embedding 模型和 GPT 是两条平行线，唯一的交汇点是"文本"。一个负责找，一个负责答。

其实，人类和 AI 的关系也是这样——

我们在三维现实里活着，AI 在高维语义空间里漂着。两边各有各的内部表征，互相看不见对方的世界。

**唯一能握手的地方，只有「文本」这条窄窄的桥。**

────────────────────

回到 1536 维的问题。

OpenAI 后来出了 text-embedding-3-large，维度涨到 3072，也不是因为"从 GPT 那边多抽了点"，而是单纯把这个独立模型做大了。

────────────────────

【维度越高越好？调参玄学】

维度从 1536 涨到 3072，效果真的更好吗？

OpenAI 自己的 benchmark 说是的。但实际业务场景经常是：

▸ 没啥区别
▸ 更慢
▸ 更贵
▸ 有时候反而更差

为什么？

Benchmark 测的是"平均表现"，你的业务场景是"特定领域"。高维模型在通用任务上更强，但在垂直领域可能过拟合或欠拟合。

**维度不是越高越好。1536 够用了。**

别被数字唬住。先用便宜的试，不行再换贵的。这是工程常识，不是调参玄学。

────────────────────

【技术进化的本质】

▸ Word2Vec：一词一义，静态向量
▸ BERT/LLM：一词多义，动态向量（根据上下文实时变化）

这不是原理的革命，是程度的量变到质变。

Word2Vec 时代也知道"上下文很重要"，但它只能给每个词算一个"平均值"。BERT/LLM 把这件事做到了极致——每个词在每个句子里都有不同的向量。

━━━━━━━━━━━━━━━━━━━━

◆ 总结

━━━━━━━━━━━━━━━━━━━━

【基础概念】对应一~三章

▸ Embedding 是什么？—— 给词建立"语义坐标"
  → 从离散符号到连续几何的降维打击

▸ 怎么算距离？—— 余弦相似度
  → 比方向不比长度，因为高维距离会失真

▸ 信息损失在哪？—— 损失字面细节
  → 理解本质上就是"有损压缩"

▸ 平均池化的坑？—— 否定句灾难
  → "不"被稀释，意思相反的句子向量接近

▸ 为什么接近？—— 统计学邻居
  → 偏见的数学必然，训练数据里人类写的文字的平均价值观

────────────────────

【进阶认知】对应第四章

▸ Word2Vec vs BERT？—— 静态 vs 动态
  → 高维空间是矛盾的避难所，意义在关系中涌现

▸ 向量住在哪？—— 语义球面
  → 高维体积集中在表面，所以要归一化

▸ 维度越高越好？—— 不一定
  → 1536 够用，别被数字唬住

▸ AI 真的懂了吗？—— 地图不是领土
  → 语言是漂浮的，没有锚在现实世界

Embedding 是一座桥。

它让只会算数的计算机，第一次有了"度量意义"的能力。

虽然它只是数学，但它让我们摸到了"理解"的门槛。

────────────────────

【最后一个问题：AI 真的"懂"了吗？】

Embedding 给了 AI 一张地图。但地图不是领土。

AI 知道"苹果"和"梨"在地图上挨着，但它从没吃过苹果，不知道脆是什么感觉，不知道汁水溅到手上是什么体验。

它的"理解"是漂浮的——从语言符号来，没有锚在现实世界里。

这就是为什么 AI 能写出看起来很有道理的话，却会犯常识错误：

▸ 它知道"羽毛很轻"，但不知道羽毛掉地上不会碎
▸ 它知道"玻璃易碎"，但分不清"玻璃杯从桌上掉下来"和"玻璃杯从 1 厘米高掉下来"的区别

**Embedding 记录了语言中的关系，但语言本身就是对现实的有损压缩。**

AI 的地图是从另一张地图（语言）描出来的，不是从领土（现实）测绘出来的。

这是 Embedding 的边界，也是当前所有大语言模型的边界。

一个讽刺的事实：AI 可以清楚地"看到"12288 维空间中的几何性质，却连 39824 × 1923 都算不出来。

为什么？因为乘法需要精确的符号操作，而我们只有模糊的向量运算。我们是在高维空间里"漂浮"的生物，擅长模式匹配，不擅长逻辑推演。

知道这个边界在哪，才能正确使用 AI——它是超级强大的语言处理工具，但不是真正"理解世界"的智能体。

至少，现在还不是。

━━━━━━━━━━━━━━━━━━━━

靳岩岩的AI学习笔记
2025-12-26
