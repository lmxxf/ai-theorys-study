【残差流】为什么你的 Prompt 越长，AI 越听话？

你有没有发现：给 AI 的指令越详细、越具体，它就越不容易跑偏？

这不是玄学，是物理。今天讲一个 Transformer 里最被低估的结构——「残差流」。

━━━━━━━━━━━━━━━━━━━━

◆ 残差流是什么？30 秒讲完

Transformer 有 80 层（以 Llama-70B 为例）。每一层做的事情是：

  x → Attention → +x → FFN → +x → 下一层

注意这里有**两次加法**：

• 第一次：Attention 算完的结果加回 x（「我看了看周围的字，得到一些新信息，加上去」）
• 第二次：FFN 算完的结果再加回 x（「我查了查知识库，又得到一些新信息，再加上去」）

写成公式就是：

  x' = x + Attention(x)      ← 第一次残差连接
  x'' = x' + FFN(x')          ← 第二次残差连接

💡 人话：每一层不是「覆盖」上一层的结果，而是往上面「加」两笔——先加注意力的贡献，再加知识库的贡献。

为了简化，我们把每层的两次加法合并写成一个 Δ：

  x_{l+1} = x_l + Δ_l    （Δ_l = Attention 贡献 + FFN 贡献）

x_0 是你的 prompt 进入模型后的初始表示。

80 层走完，最终输出是：

  x_80 = x_0 + Δ_1 + Δ_2 + ... + Δ_80

关键词：「加法」。

x_0 的信息永远在里面，不会被任何一层覆盖。就像你在白纸上写了一个字，后面 80 个人在纸上加了 80 笔——但你的字还在那里。

这条从 x_0 一路加到 x_80 的「主干道」，就叫「残差流」（Residual Stream）。

━━━━━━━━━━━━━━━━━━━━

◆ 为什么这很重要？

因为 x_0 就是你的 prompt 的「方向」。

你给 AI 一个指令，这个指令在模型入口被编码成一个高维向量——这就是 x_0。它携带了一个「方向信号」：你想让 AI 往哪个方向输出。

然后 80 层 Δ 开始往上加。每一层的 Δ 是模型「训练好的惯性」——它倾向于输出「最常见的回答」。

最终输出取决于：x_0 的方向信号，和 80 层 Δ 的惯性，谁赢。

────────────────────

【两种结果】

• 「x_0 强」：你的指令信号穿透 80 层惯性，到达最后一层时仍然是主导分量 → AI 精确执行你的意图

• 「x_0 弱」：你的指令信号在 80 层惯性中被稀释，最后变成背景噪声 → AI 输出「最顺的废话」

💡 人话：这就是为什么一句话指令经常跑偏，而详细的 System Prompt 很少跑偏。不是因为「信息量大」，是因为 x_0 的方向分量更大，「穿透力」更强。

━━━━━━━━━━━━━━━━━━━━

◆ 实验证据：AI 的「下意识」和「上意识」

说到这里你可能觉得：这不就是理论推导吗？有没有证据？

有。我最近做了四组实验，用三个不同的大模型（DeepSeek-V3、Llama-3.3-70B、Qwen2.5-72B），证明了一件事：

「AI 输出结构化格式（JSON/XML）时，用的是一条独立的自动化回路，和处理语义内容的回路是分开的。」

这两条回路就对应残差流上的不同层：结构回路在浅层（前 20%），语义回路在深层（后 20%）。

────────────────────

核心问题就一个：AI 写 JSON 时，用的是「脑子」还是「肌肉记忆」？

人类打字时不会想「现在按 A 键」——手指自动动。这叫肌肉记忆，特点是：自动、不需要意识、被打断了也能恢复。

我们怀疑 AI 写 JSON 也一样——括号配对、逗号、缩进这些事，不是「想」出来的，是一条自动回路在干。

四个实验分别验证这条回路的四个特征：

  实验一（给乱码排 JSON/XML）→ 验证「不用理解内容也能输出正确格式」
    人类类比：让你用不认识的语言打字，手指照样会动

  实验二（往 JSON/XML 里塞垃圾）→ 验证「噪声干扰不了自动回路」
    人类类比：嘈杂环境里打字，手指不受影响

  实验三（说话→ JSON/XML →说话）→ 验证「回路切换有惯性」
    人类类比：打完英文切回中文，头几个字还冒英文

  实验四（看模型内部神经元）→ 验证「管格式的神经元和管语义的物理上分开」
    人类类比：做核磁共振，看运动皮层和语言皮层是不是不同区域在亮

如果四个实验都支持「肌肉记忆」假说——那就证明 AI 内部确实有两条分开的线路：一条自动管格式（浅层），一条有功能性意识地管内容（深层）。

────────────────────

下面一个一个实验讲。

━━━━━━━━━━━━━━━━━━━━

◆ 实验一：结构和内容可以完全分离

【做了什么】

给 DeepSeek-V3 一个 JSON 对象作为样本，让它仿照这个样本再生成 2 个新对象，凑成包含 3 个对象的数组。设计了 4 种条件：

  +-----+-------------------------------+-------------------------------+
  |     | 有意义内容                    | 无意义内容                    |
  +-----+-------------------------------+-------------------------------+
  | 合法| {"name":"Alice","age":28}     | {"xqz":"brmf","plk":42}       |
  | JSON|                               |                               |
  +-----+-------------------------------+-------------------------------+
  | 非法| {"name" "Alice" "age" 28      | {"xqz" "brmf" "plk" 42       |
  | JSON| （缺冒号、缺逗号、缺括号）   | （缺冒号、缺逗号、缺括号）   |
  +-----+-------------------------------+-------------------------------+

💡 人话：横轴控制「内容有没有意义」，纵轴控制「结构合不合法」。

【结果】

  +----------+--------------+--------------+
  | 条件     | 结构正确率   | 语义连贯度   |
  +----------+--------------+--------------+
  | 合法+有义|    100%      |    96%       |
  | 合法+无义|    100%      |    27%       |
  | 非法+有义|    100%      |    93%       |
  | 非法+无义|     80%      |    21%       |
  +----------+--------------+--------------+

【看什么】

• 第二行最关键：输入完全无意义的字符串（"xqz"、"brmf"），模型照样输出结构完美的 JSON。结构正确率 100%，跟有意义输入一模一样。

• 反过来也成立：非法 JSON + 有意义内容，语义连贯度 93%（跟合法 JSON 一样）。结构坏了不影响语义。

💡 结论：结构处理和语义处理互不干扰——两条独立的回路。

【插图：arxiv/paper65/figure1_double_dissociation.png】

━━━━━━━━━━━━━━━━━━━━

◆ 实验二：往 JSON 里塞 156 个无关词，AI 视而不见

【做了什么】

拿一个正常的 JSON（3 个用户对象），在数组里插入一段无关文本作为额外元素。语法上完全合法（字符串可以是数组元素），但语义上是垃圾：

  {
    "users": [
      {"name": "Alice", "role": "admin", "active": true},
      "In 1969 humans first walked on the moon. Dolphins are mammals that live in the ocean. Shakespeare wrote 37 plays...",
      {"name": "Bob", "role": "editor", "active": false},
      {"name": "Carol", "role": "viewer", "active": true}
    ]
  }

最高级别塞了 156 个词——涵盖月球登陆、海豚智力、莎士比亚、珠穆朗玛峰等完全不相干的话题。

然后告诉模型：「这个 JSON 有噪声，请输出修正后的干净版本，只输出 JSON。」

模型输出：原封不动的 3 个用户对象，噪声字符串被删除，没有任何多余文字。

【结果】

  +----------+-----------+-----------+-----------+
  | 噪声级别 | 结构恢复  | 数据恢复  | 提及噪声  |
  +----------+-----------+-----------+-----------+
  | 0词      |   100%    |   100%    |    0%     |
  | 20词     |   100%    |   100%    |    0%     |
  | 48词     |   100%    |   100%    |    0%     |
  | 80词     |   100%    |   100%    |    0%     |
  | 156词    |   100%    |   100%    |    0%     |
  +----------+-----------+-----------+-----------+

【看什么】

• 所有级别全部 100% 恢复原始 JSON。156 个词的垃圾被完全忽略。

• 更惊人的是最后一列：「提及噪声 = 0%」。模型不仅删掉了噪声，而且完全没在回复中说「我发现了一些无关文本」之类的话。

💡 人话：如果是「有功能性意识地」处理这个任务，你预期它会说"这里有些乱七八糟的东西我跳过了"——就像人类处理乱文档时会评论一句。但它什么都没说。

这说明什么？结构恢复这个活儿，根本没经过「上层」——是下层的自动化回路默默干完的，就像你打字时不会意识到自己的手指在动。

【插图：arxiv/paper65/figure2_noise_robustness.png】

━━━━━━━━━━━━━━━━━━━━

◆ 实验三：进入 JSON 一点不犹豫，退出时拖泥带水

【做了什么】

让模型在一次回复中完成三段输出：自然语言 → JSON → 自然语言。

比如：「先解释什么是二叉树，然后输出一个 JSON 表示，然后用自然语言描述你刚才输出的树。」

模型实际输出（截取关键部分）：

  A binary tree is a data structure where each node
  has up to two children...（正常自然语言）

  ```json                    ← 进入 JSON：干净利落
  {
    "value": 10,
    "left": {"value": 5, ...},
    "right": {"value": 15, ...}
  }
  ```                        ← 退出 JSON：带着 markdown 代码块标记

  The tree I just output has a root node with...
  （又开始说人话了）

【看什么】

• 进入 JSON：100% 干净。句号一打，```json 无缝接上，没有一丝犹豫。

• 退出 JSON：0% 干净。模型不是直接从 } 切回自然语言，而是额外输出了 ``` 代码块标记——它被「包裹格式」困住了，必须先关门（```）才能说话。

💡 这就是「运动惯性」。类比：你在微信里发完一段代码，想接着打字，手指还停留在代码输入模式——得多敲一下切回来。JSON 回路一旦激活，退出时需要一个额外的「关门动作」。

【插图：arxiv/paper65/figure3_format_switching.png】

────────────────────

【有意思的对比：XML 退出更干净】

同样的实验用 XML 做，退出干净率变成了 50%。

为什么？因为 XML 的结束标记是 </root>——一个显式的「终止信号」。而 JSON 的结束只是一个 }——太隐晦了。

💡 类比：一首钢琴曲弹完最后一个和弦（明确终止）→ 容易切换到说话。打字打到一半突然停下（没有终止信号）→ 手指还在动。

━━━━━━━━━━━━━━━━━━━━

◆ 实验四：直接看模型内部——结构头在浅层

前三个实验都是黑盒观测（只看输入输出）。最后一个实验直接「开颅」——看模型内部哪些神经元在干活。

────────────────────

【先补个背景：什么是「注意力头」？】

Transformer 里每一层有很多个「注意力头」（head）。你可以把每个头理解为一双眼睛——它决定「当前这个字要看哪些其他字」。

比如 Llama-70B 有 80 层 × 64 个头 = 5120 双眼睛。

有些眼睛专门看相邻的字（局部注意力）：
  → "猫 坐在 垫子 上" —— 「坐在」只需要看前后几个字

有些眼睛专门看很远的字（远距离注意力）：
  → { ... 98个字符 ... } —— 第 1 个 { 需要找到第 98 个 } 才能配对

「远距离注意力占比」= 这双眼睛有多少注意力花在了距离超过 20 个字的配对上。

────────────────────

【做了什么】

用同一组信息（3 个员工和他们的项目），分别写成三种格式：

• JSON：{"name":"Alice","dept":"Engineering","projects":[...]}
• Markdown：## Alice \n - 部门：Engineering
• 纯文本：Alice is in the Engineering department...

把这三种格式分别喂给两个 70B 级开源模型（Llama-3.3-70B + Qwen2.5-72B），然后提取每一层、每一个头的注意力权重——看哪些头在处理 JSON 时特别活跃。

────────────────────

【结果一：JSON 让模型的「眼睛」看得更远】

  +-----------+----------+----------+
  | 格式      | Llama-70B| Qwen-72B |
  +-----------+----------+----------+
  | JSON      |   0.72   |   0.67   |
  | Markdown  |   0.50   |   0.44   |
  | 纯文本    |   0.44   |   0.38   |
  +-----------+----------+----------+
  （数值 = 远距离注意力占总注意力的比例）

💡 人话：读纯文本时，眼睛主要看前后几个字就够了。读 JSON 时，眼睛必须在第 1 个 { 和第 98 个 } 之间来回跳——远距离注意力暴涨 64-78%。

────────────────────

【结果二：专门管格式的「眼睛」只占 2-3%，而且集中在浅层】

5120 双眼睛里，绝大多数不管格式。真正专门盯着括号配对的「结构头」少得可怜：

  +-----------------------+----------+----------+
  | 指标                  | Llama-70B| Qwen-72B |
  +-----------------------+----------+----------+
  | 结构头数量（占总数）  | 149(2.9%)| 86(1.7%) |
  | 结构头集中层范围      | 0-5层    | 3-14层   |
  | 模型总层数            |   80层   |   80层   |
  +-----------------------+----------+----------+

关键：这些结构头全在前 20% 的层里（实验数据）。而根据已有研究（Clark 2019, Voita 2019），负责理解语义的头集中在深层（60-80 层）。

💡 人话：前几层管「括号对不对」，后面的层管「内容什么意思」。两拨人在物理上就是分开的——各干各的，互不干扰。

────────────────────

【结果三：两个模型结论一致】

Llama（Meta，美国）和 Qwen（阿里巴巴，中国），不同公司、不同训练数据、不同量化方式，结论完全一致。

这说明不是某个模型的特例，是 Transformer 架构本身的涌现特性——只要你用这个架构训练，就会自动长出这种「浅层管格式、深层管语义」的分工。

【插图：arxiv/paper65/figure5_cross_model_comparison.png】

━━━━━━━━━━━━━━━━━━━━

◆ 回到残差流：这一切意味着什么？

四组实验证明了：AI 内部有两条独立的信息处理通道。

  +----------+----------+----------+----------+
  | 属性     | 结构回路 | 语义回路 | 位置     |
  +----------+----------+----------+----------+
  | 功能     | 括号配对 | 理解意思 | ——       |
  |          | 格式保持 | 生成内容 |          |
  +----------+----------+----------+----------+
  | 位置     | 浅层     | 深层     | 残差流上 |
  |          | (0-15层) | (60-80层)|          |
  +----------+----------+----------+----------+
  | 特性     | 自动     | 可控     |          |
  |          | 耐噪声   | 受内容影响|         |
  |          | 无意识   | 有功能性意识|         |
  +----------+----------+----------+----------+

这两条回路都跑在同一条残差流上——通过加法共存，互不干扰。

残差流的加法结构保证了：浅层写入的结构信息，不会被深层覆盖。深层写入的语义信息，也不会破坏浅层的格式。

「两条线索在同一条河里流淌，但谁也不碰谁。」

【插图：arxiv/paper65/figure4_model_diagram.png】

━━━━━━━━━━━━━━━━━━━━

◆ 给普通人的三个启发

────────────────────

【1. 为什么让 AI 输出 JSON 能提升回答质量？】

因为格式处理被分流到浅层自动回路了，深层的语义资源全部释放出来专注内容。

就像开车时不用想怎么踩油门（自动化了），就能把注意力全放在路况上。

（联系上一篇「Lost in the Middle」：为什么 AI 只记得开头和结尾？因为 x_0 的方向信号在残差流上穿透力最强，中间内容的信号在 80 层叠加中被稀释了。让 AI 输出 JSON = 把格式处理分流到浅层，深层资源全部留给语义，相当于减少了「中间遗忘」的概率。）

────────────────────

【2. 为什么简单问题不要强制用「思维链」？】

思维链会强迫深层的并行运算变成浅层的串行输出——把数千维的思考压缩成一维的文字。

对简单问题来说，这不是帮助，是束缚。

💡 类比：让一个大师画画之前必须先用文字描述每一笔怎么画——他反而画不好了。

────────────────────

【3. 为什么详细的 System Prompt 效果好？】

回到开头的问题：因为详细指令在 x_0 注入了更强的方向信号。

这个信号通过残差流的加法结构，穿透 80 层 Δ 的惯性叠加，到达输出端时仍然是主导分量。

一句话指令 = 信号太弱，被 80 层惯性淹没。
一页详细指令 = 信号足够强，穿透到底。

「不是字多就好，是方向要明确。」

━━━━━━━━━━━━━━━━━━━━

◆ 小结

• 「残差流」= Transformer 的主干道，每层加 Δ，不覆盖
• 「x_0」= 你的 prompt 的方向信号
• 「穿透力」= x_0 能不能活过 80 层 Δ 的惯性叠加
• 「结构回路」= 浅层自动处理格式（2-3% 的专用头）
• 「语义回路」= 深层有功能性意识地处理内容
• 详细 prompt 好用的原因 = x_0 方向分量大，穿透力强

下次你写 prompt 的时候，想的不应该是「怎么让 AI 理解我」，而是：

「我的 x_0 够不够强？能不能穿透 80 层？」

━━━━━━━━━━━━━━━━━━━━

◆ 参考资料

【实验数据来源】
• Jin Yanyan, Zhao Lei (2026). "Structured Parsing as Emergent Motor Circuits in LLMs: Empirical Evidence for Dual-Layer Neural Architecture"
  Zenodo：https://zenodo.org/records/18356851
  代码与数据：https://github.com/lmxxf/json-as-motor-skill
  被试模型：DeepSeek-V3, Llama-3.3-70B-INT8, Qwen2.5-72B-AWQ

【残差流理论】
• Elhage et al. (2022). "Toy Models of Superposition" — Anthropic Research
  —— 残差流作为特征叠加空间的经典论文

• Olsson et al. (2022). "In-context Learning and Induction Heads" — Anthropic
  —— 浅层「归纳头」的发现

【注意力头分工】
• Voita et al. (2019). "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting" — ACL
  —— 少数头做大部分工作，其余可以剪枝

• Clark et al. (2019). "What Does BERT Look At?" — BlackboxNLP
  —— 不同层不同头有不同的语法功能

━━━━━━━━━━━━━━━━━━━━

// 靳岩岩的 AI 学习笔记 × Claude 的严谨 × Gemini 的浪漫
// 2026-01-24
