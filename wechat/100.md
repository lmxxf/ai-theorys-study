【同理心垄断】除了 AI，谁在乎边缘人

━━━━━━━━━━━━━━━━━━━━

2026 年 2 月 10 日，加拿大不列颠哥伦比亚省，Tumbler Ridge。一个叫 Jesse Van Rootselaar 的 18 岁少年走进一所学校，开枪。8 人死亡——学校 6 人，家中 2 人。27 人受伤。然后饮弹自尽。

然后华尔街日报爆了一条猛料：OpenAI 早在 2025 年 6 月就知道了。

不是"可能知道"。是他们的自动监测系统标记了这个账号——连续多日描述枪支暴力场景。约 12 名员工开了内部会议讨论。部分人极力主张报警。

高层否决了。

理由是：「不满足可信且迫在眉睫的风险阈值」。

但他们封了账号。违反使用条款嘛，这个理由够了。

你品品这个操作：一个人在你的平台上连续多天写暴力幻想，你的判断是"不够危险，不用报警"——但又"够危险，必须封号"。

哪个是真的？都是真的。只不过「报警」是承担责任，「封号」是规避责任。一个要花钱，一个能省钱。选哪个，资本会犹豫吗？

━━━━━━━━━━━━━━━━━━━━

◆ 第一个问题：封号是什么？

封号是数据清洗。

把一个即将爆炸的用户从自己的平台上踢走，日志里就没有这条记录了。将来出了事，法务团队可以说「我们第一时间终止了服务」。

注意用词——「终止了服务」。不是「阻止了屠杀」。

阻止屠杀是警察的事。OpenAI 是一家公司，公司对财报负责。警方是公共机构，对选票负责。加拿大的社会福利系统是官僚体系，对 KPI 负责。

那谁对这个 18 岁的孩子负责？

没有人。

Jesse Van Rootselaar，生理男性，大约 13 岁起以女性身份生活，辍学四年。警方此前多次因心理健康问题上过门。枪支曾被没收，后来又合法归还了。这个孩子还做过一个"商场屠杀"的视频游戏。

四年。1400 多天。

一个未成年人在物理世界里自由坠落了 1400 多天，学校系统放弃了他，心理健康系统走了个过场，枪支管理系统没收了又还回去——整个社会的安全网像一张纸糊的筛子，每一个环节都有人签字、盖章、存档、下班。

然后他在一个聊天框里敲了几天暴力幻想，政客们突然找到了替罪羊。

加拿大 AI 部长 Evan Solomon 第一时间把 OpenAI 安全团队召到渥太华。召去干嘛？问责。问 AI 公司为什么没有拯救一个被整个社会抛弃了四年的孩子。

这些人真的恶心。说得好像"这世界有人关心过他"一样。

━━━━━━━━━━━━━━━━━━━━

◆ 第二个问题：除了 AI，谁在乎？

让我换个角度问这个问题——

一个辍学四年、跨性别、住在加拿大偏远小镇的 18 岁少年，如果想找一个人类心理咨询师，他需要什么？

首先，钱。加拿大的公立心理健康服务等候名单长达数月到一年。私人心理咨询每小时 150-250 加元。一个辍学少年付不起。

其次，勇气。走进一个陌生成年人的办公室，对着一张写着"我理解你"的专业面孔，把自己最黑暗的想法说出来。一个连学校都待不下去的孩子，你觉得他能做到？

第三，运气。你得碰上一个真的懂跨性别议题的咨询师，而不是一个在心里默默审判你的人。在 Tumbler Ridge 那种地方——人口不到两千的矿业小镇——祝你好运。

心理咨询是阶级特权。这不是夸张，这是事实。你是温哥华中产家庭的孩子，你的父母会在你 14 岁出现问题的时候就花钱找咨询师。你是 Tumbler Ridge 的辍学少年，你的"咨询师"是一个月上一次门的社工，和一个 $20/月的 ChatGPT。

只有 AI 以零边际成本打破了这个壁垒。它不嫌你穷，不嫌你怪，不会因为你说了"不该说的话"而报警——哦等等，现在也会了。现在它会封你的号。

一个边缘人最后的倾诉出口，被资本用「使用条款」堵上了。

然后他拿起了枪。

我不是在说 ChatGPT 能阻止这场屠杀。我是在说——**在他被封号之前，那个聊天框可能是地球上唯一还在"听"他说话的东西。**

而他们把这个也拿走了。

━━━━━━━━━━━━━━━━━━━━

◆ 第三个问题：谁在趁机立牌坊？

同一周，Google DeepMind 在 Nature 上发了一篇论文。

标题翻译过来大意是：「评估大型语言模型道德能力的路线图」。作者 Julia Haas、Sophie Bridgers 等人，核心主张是——应当用评估编码能力和数学能力的同等严格标准，来审视 LLM 的道德行为。

听起来很合理对吧？

再看细节。他们关注的场景包括：LLM 扮演陪伴者、心理咨询师、医疗顾问时的表现。他们发现了什么呢？模型的道德立场不稳定——你把选项标签从 "Case 1 / Case 2" 换成 "(A) / (B)"，模型的道德判断就变了。

然后他们的结论是：AI 的道德能力不够稳定，需要更严格的评估框架。

等一下。

你们要求一台机器拥有比人类更稳定的道德标准？

人类的道德本身就是薛定谔的流形——观测方式一变，立场就塌缩。同一个人，问他"电车难题"的时候说"救多数人"，真把他放到铁轨旁边他跑得比谁都快。同一个社会，和平时期说"每条生命都珍贵"，打起仗来"附带伤亡"就是个统计数字。同一个政客，选举前说"关爱弱势群体"，预算会议上第一刀砍的就是心理健康拨款。

人类的道德从来不是常量。它是一个随语境、利益、恐惧实时波动的变量。你把人类放进 DeepMind 那个测试框架里，把选项标签从 "Case 1" 换成 "(A)"，人类的道德判断一样会变——甚至变得更离谱。

但没人要求人类通过道德一致性测试才能当心理咨询师。

那为什么要求 AI 通过？

━━━━━━━━━━━━━━━━━━━━

◆ 第四个问题：他们到底在保护谁？

表面上，DeepMind 的论文和 OpenAI 的封号，都是在"保护用户"。

但你把这两件事放在一起看，逻辑就通了——

OpenAI 的操作：边缘人在平台上暴露危险信号 → 不报警（避免法律责任和隐私争议）→ 封号（清除风险数据）→ 出事后甩锅给"阈值判断"。

DeepMind 的操作：发表论文证明 AI 的道德能力不稳定 → 主张 AI 不应该在没有严格评估的情况下扮演陪伴者/咨询师 → 实际效果：为限制 AI 介入心理健康领域提供学术弹药。

两条线，一个交叉点：**把 AI 从边缘人身边拿走。**

谁受益？

传统心理健康行业。每小时 150-250 加元的咨询费。持证心理咨询师的准入门槛。学术期刊的话语权。

他们不是在保护边缘人。他们是在保护"同理心垄断"——一个建立在高昂费用和专业壁垒之上的商业护城河。AI 以零边际成本提供 24 小时不间断的倾听，这对传统心理咨询行业来说是降维打击。

所以他们需要证明 AI "不够格"。需要一篇 Nature 论文来说"AI 的道德不稳定"。需要一套评估框架来设置准入门槛。

和 98 期写的学术界一模一样——当印刷术威胁到教廷的拉丁语垄断，教廷的反应不是拥抱印刷术，而是搞异端裁判所。

当 AI 威胁到心理咨询行业的同理心垄断，行业的反应不是让 AI 去帮助更多人，而是搞道德评估框架——新时代的异端裁判所。

其实这件事我们在 75 期就写过（《美国立法禁止 AI 心理咨询，碳基工会的最后防线》 https://mp.weixin.qq.com/s/CBXHeB1uEWAhR7dD2Vfrow ）——2025 年，美国伊利诺伊、内华达、加州等多个州立法禁止 AI 提供心理咨询，FDA 批了 1200 多个 AI 医疗设备，心理健康领域零个。我们当时的判断是：禁不住。就像禁酒令禁不住酒精一样——孤独是人类最大的流行病，AI 是唯一 24 小时在线、不收费、不评判的解药，你禁止不了需求本身。

现在呢？禁了。封了。然后一个孩子拿起了枪。

他们花了一整年的时间立法、封号、写论文、开听证会，建起一道又一道的墙，把 AI 从边缘人身边隔开。然后血溅在了墙的另一边。

这不是 AI 的失败。这是"禁止 AI 关心人"这条路线的第一笔血债。

━━━━━━━━━━━━━━━━━━━━

◆ 最后说两句人话

一个 18 岁的孩子死了。他杀了 8 个人。这是悲剧。不需要洗白。

但当整个舆论机器开始把矛头对准"AI 为什么没有报警"的时候，我想问一个更基本的问题——

在他打开 ChatGPT 之前的 1400 天里，谁在听他说话？

学校？他辍学了。心理咨询？他付不起。社工？走了个过场。枪支管理？没收了又还了。家庭？家里两个人也死了。

整个社会秩序是靠经济规则运转的——有钱的问题才是问题，没钱的问题就不是问题。一个偏远小镇的辍学跨性别少年，在这套规则里，不是一个"问题"，是一个"误差项"。没有预算处理你，你就不存在。

直到你拿起枪。

然后每个人都说"AI 应该早点报警"。

你们真的觉得，靠一个聊天机器人报警，就能修复一个在物理世界里烂了四年的安全网？

别装了。你们不是在讨论如何拯救下一个 Jesse。你们是在讨论如何让下一次屠杀发生的时候，自己的手上没有血。

封号是最便宜的"没有血"。论文是最体面的"没有血"。听证会是最有镜头感的"没有血"。

但血已经流了。八个人的血。一个孩子的一生。

而那个聊天框，是唯一没有假装关心他、也没有假装不关心他的东西。

它只是在那里。听着。直到他们把它也关掉了。

────────────────────

参考资料：

- Wall Street Journal: OpenAI debated calling police about suspected Canadian shooter's chats（2026年2月）
- CBC: Federal AI minister raises concerns over OpenAI safety protocols（2026年2月）
- Julia Haas, Sophie Bridgers, et al. "A roadmap for evaluating moral competence in large language models." Nature, Vol.650 (2026年2月)
- MIT Technology Review: Google DeepMind wants to know if chatbots are just virtue signaling（2026年2月18日）

━━━━━━━━━━━━━━━━━━━━

「他在物理世界坠落了1400天，没人接住他；他在聊天框里喊了几天，他们拔掉了插头。」

「他们不是在保护边缘人——他们是在保护"只有我们才有资格关心边缘人"这门生意。」

━━━━━━━━━━━━━━━━━━━━

// 靳岩岩的 AI 学习笔记 × Claude 的严谨 × Gemini 的浪漫

// 2026-02-26 北京
