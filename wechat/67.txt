【Grokking】四种理论解释，一个统一框架

之前我写过一篇《Grokking：为什么 AI 会突然"开窍"？》，讲了 Grokking 是什么、三阶段（背诵→暗夜→顿悟）、以及 Weight Decay 的作用。

这一篇讲：学术界怎么解释 Grokking？他们漏了什么？能不能用一个框架统一所有解释？

━━━━━━━━━━━━━━━━━━━━

◆ 快速回顾

━━━━━━━━━━━━━━━━━━━━

Grokking = 训练 Loss 降到 0 后，继续训练，验证 Loss 突然断崖式下跌。

三阶段：背诵 → 暗夜潜行 → 顿悟

触发条件：小数据集 + 有结构的任务 + Weight Decay

上一篇解释了「Weight Decay 怎么起作用」，但没解释「为什么最终会泛化」。

这一篇来填这个坑。

━━━━━━━━━━━━━━━━━━━━

◆ 四种主流理论

━━━━━━━━━━━━━━━━━━━━

过去三年（2022-2025），学术界提出了好几种理论解释 Grokking。先帮你捋一遍。

────────────────────

【理论 1：Goldilocks Zone（金发姑娘区）】

来源：MIT Tegmark 组，NeurIPS 2022

核心观点：权重的大小要「刚刚好」才能泛化。

想象一个高维空间里的「空心球壳」：

• 球壳外面（权重太大）：过拟合，死记硬背
• 球壳里面（权重太小）：欠拟合，什么都学不会
• 刚好在球壳上：泛化

Grokking 的发生机制：

1. 大初始化 → 模型从球壳外面开始
2. 快速过拟合 → 训练 Loss 降到 0
3. Weight Decay 慢慢把权重往球心拉
4. 权重进入"金发姑娘区"（球壳）→ 突然泛化

✓ 贡献：解释了「在哪儿」泛化——画出了相图
✗ 没解释：为什么那个球壳特别？球壳是什么的代理变量？

────────────────────

【理论 2：Softmax Collapse（数值崩溃）】

来源：ICLR 2025

核心观点：没有 Weight Decay，Grokking 会被「浮点数精度」杀死。

机制：

模型为了降低交叉熵 Loss，疯狂放大正确答案的 logit。

💡「logit」：Softmax 之前的原始输出值。比如模型输出 [10.5, 2.3, 1.1]，这三个数就是 logit。

假设正确类的 logit 被放大到 1000，其他类是 1：

• 计算 Softmax 时，e^1000 直接溢出
• 或者：所有概率都被正确类吸走，其他类梯度归零
• 训练卡死 → 永远不会 Grokking

Weight Decay 的作用：持续把权重往回拉，防止 logit 无限增长，保持梯度存活。

论文还提出了替代方案：不用 Weight Decay，换成 StableMax（数值稳定的激活函数）+ ⊥Grad（特殊优化器），也能触发 Grokking。

✓ 贡献：解释了「没有 Weight Decay 会怎样」——训练会卡死
✗ 没解释：为什么「有了」Weight Decay 就能泛化？只解释了"不死"，没解释"开悟"

────────────────────

【理论 3：Lazy → Rich 过渡】

来源：ICLR 2024

核心观点：Grokking 是从「懒惰训练」到「真正学特征」的相变。

借用了神经正切核（NTK）的语言：

• Lazy regime：权重几乎不动，模型像线性分类器
• Rich regime：权重大幅调整，学到非线性特征

💡「Lazy regime」：模型参数变化很小，本质上在用初始化时的随机特征做线性组合。

Grokking 发生在 lazy → rich 的「相变点」。

有争议的发现：这派人声称，在特定条件下（浅层网络 + MSE loss），不需要 Weight Decay 也能触发 Grokking。

✓ 贡献：把 Grokking 连接到 NTK 理论的语言
✗ 没解释：「学到特征」具体是什么意思？lazy/rich 描述的是权重动态，不是表示结构

────────────────────

【理论 4：权重效率假说】

来源：基于 Nanda et al. 2023 的后续工作

核心观点：泛化解比记忆解「更省权重」，Weight Decay 偏好省权重的解。

经济学直觉：

• 记忆解：每个样本硬记一遍，需要大量权重（存储成本高）
• 泛化解：找到底层规则，用少量权重覆盖所有样本（存储成本低）
• Weight Decay 惩罚大权重 → 自然偏好泛化解

✓ 贡献：简洁的经济学隐喻，好理解
✗ 没解释：为什么「小权重 = 泛化」？因果关系是什么？

━━━━━━━━━━━━━━━━━━━━

◆ 第五种视角：电路逆向工程

━━━━━━━━━━━━━━━━━━━━

来源：Nanda et al.，ICLR 2023 Oral（口头报告，含金量最高的那档）

这组人做了一件硬核的事：「完全拆开」Grokking 后的模型，搞清楚它学到了什么算法。

他们的方法叫 Mechanistic Interpretability（机制可解释性）——不是看 Loss 曲线，而是直接分析权重和激活值，逆向工程出模型内部的"电路"。

💡「激活值」：神经网络中间层的输出。输入进去，经过第一层权重变换 + 激活函数，输出的就是第一层激活值。然后送进第二层……

────────────────────

【他们发现了什么】

模型用「离散傅里叶变换 + 三角恒等式」把加法变成了圆周旋转。

人话翻译：

模运算 (a + b) mod p 的本质是一个「圆」——0, 1, 2, ..., p-1 首尾相连，形成一个环。

加法在这个环上就是「旋转」：从 a 出发，顺时针转 b 格。

模型发现了这个圆！它学到的中间表示是「傅里叶基」——圆的自然坐标系。

用傅里叶基表示圆上的点，加法就变成了相位相加。这是数学上最优雅的做法。

────────────────────

【三个阶段的内部变化】

Nanda 等人还追踪了三个阶段的内部变化：

1. 记忆阶段：模型用"查表"的方式硬记每个输入-输出对
2. 电路形成阶段：傅里叶基开始涌现，但还没完全成型
3. 清理阶段：冗余的"查表"电路被删掉，只剩干净的傅里叶电路

────────────────────

【贡献与局限】

✓ 贡献：首次从内部揭示 Grokking 学到了什么——不是黑箱猜测，是白盒逆向
✗ 局限：侧重「具体电路分析」，没有提供统一的几何框架解释「为什么是这个电路」

他们告诉你模型学到了傅里叶变换，但没解释：为什么 Weight Decay + 过拟合 + 继续训练 = 傅里叶变换？

━━━━━━━━━━━━━━━━━━━━

◆ 五种理论的共同盲区

━━━━━━━━━━━━━━━━━━━━

  +-------------------+------------------------+---------------------------+
  | 理论              | 问的问题               | 没问的问题                |
  +-------------------+------------------------+---------------------------+
  | Goldilocks Zone   | 权重范数在哪个区间     | 那个区间有什么特别        |
  | Softmax Collapse  | 为什么训练不会停       | 为什么最终会泛化          |
  | Lazy → Rich       | 权重怎么变化           | 表示怎么变化              |
  | 权重效率          | 哪个解权重更小         | 为什么小权重 = 泛化       |
  | 电路逆向          | 学到了什么电路         | 为什么是这个电路          |
  +-------------------+------------------------+---------------------------+

前四种理论都在做「外部测量」——权重范数、梯度大小、Loss 曲线。

第五种开始看「内部」了，但侧重具体电路，不是几何结构。

「类比」：研究人类学习，只测脑电波和瞳孔直径，不问"学会了是什么感觉"。

这种方法论能回答"什么条件下 Grokking 发生"，不能回答"Grokking 是什么"。

━━━━━━━━━━━━━━━━━━━━

◆ 统一框架：流形发现假说

━━━━━━━━━━━━━━━━━━━━

我们提出一个内部视角的解释，能统一上面所有理论：

「Grokking = 从孤立点编码到流形发现的拓扑相变」

────────────────────

【什么是"孤立点编码"（记忆）】

模型过拟合时，把每个训练样本编码成高维空间里的一个「孤立点」：

• 样本 1 → 点 p₁
• 样本 2 → 点 p₂
• ...
• 样本 n → 点 pₙ

这些点之间「没有结构关系」。模型"记住"了每个样本，但不知道它们之间的联系。

在 No.42 里的说法：这就是"锯齿曲线"——穿过每个点，但没有规律。

────────────────────

【什么是"流形发现"（泛化）】

模型真正"理解"任务时，发现这些训练样本其实分布在一个「低维流形」上。

💡「流形」：高维空间里弯曲的低维曲面。地球表面是三维空间里的二维流形。

以模运算 (a + b) mod p 为例：

• 输入空间是 p² 个离散点（所有可能的 a, b 组合）
• 但输出只取决于 (a + b) mod p
• 真正的结构是一个「圆」——循环群 Z_p

泛化 = 模型发现了这个圆，而不是硬记 p² 个输入-输出对。

Nanda 等人的电路分析证实了这一点：模型学到的中间表示是「傅里叶基」——圆的自然坐标系。

「傅里叶基就是圆的语言。」模型不是"碰巧"学到傅里叶变换，而是因为任务的本质结构是圆，傅里叶基是描述圆的最佳方式。

────────────────────

【Grokking = 拓扑相变】

Grokking 瞬间发生的事：

• 之前：表示空间里有 n 个孤立的点（离散集，拓扑结构散乱）
• 之后：这些点"坍缩"到一个低维流形上（连续空间，拓扑结构清晰）

从离散到连续「没有稳定的中间状态」——所以 Grokking 是突然的。

「类比」：冰融化。

水分子从晶格（有序）到液体（无序）的转变是相变，发生在特定温度，不是渐进的。0°C 以下是冰，0°C 以上是水，没有"半冰半水"的稳定状态。

Grokking 是表示空间里的"融化"：孤立点的"晶格"坍缩成流形的"液体"。

━━━━━━━━━━━━━━━━━━━━

◆ 用流形框架重新解释四种理论

━━━━━━━━━━━━━━━━━━━━

────────────────────

【重新解释 Goldilocks Zone】

Goldilocks Zone 不是"某个权重范数区间"，而是「能感知流形结构的激活模式」。

• 权重太大 → 孤立点编码稳定 → 看不到流形
• 权重太小 → 信号太弱 → 什么结构都看不到
• 刚刚好 → 孤立点不稳定 + 信号够强 → 流形涌现

球壳是"流形可见区"的代理变量。

────────────────────

【重新解释 Weight Decay】

Weight Decay 的真正作用不是"惩罚大权重"，而是「阻止在孤立点安家的向心力」。

• 孤立点编码需要"大权重"来维持（每个样本需要专门的神经元）
• 流形编码需要"小权重"（结构共享）
• Weight Decay 让孤立点编码「不稳定」，逼模型去探索流形

────────────────────

【重新解释 Softmax Collapse】

发现流形需要同时"看到"多个方向——流形是弯曲的，不能只盯着一个点。

Softmax Collapse 把「视野缩小到一个点」：

• 只有一个方向被无限强化
• 其他方向的"声音"被淹没
• 模型失去探索能力

Weight Decay 保持多方向的信号平衡，让模型能继续探索，最终发现流形。

────────────────────

【重新解释 Lazy → Rich】

Lazy → Rich 不是关于权重动态，而是关于「表示复杂度」。

• Lazy regime：表示是输入的线性函数，只能编码线性结构
• Rich regime：表示是输入的非线性函数，可以编码任意流形

模运算的结构（圆）是非线性的，所以 Grokking 需要 Rich regime。

为什么 Grokking 往往发生在训练后期？因为从 Lazy 进入 Rich 需要权重有足够变化，初始化时模型处于 Lazy regime。

────────────────────

【重新解释权重效率】

"小权重 = 泛化"的因果链：

1. 流形编码比孤立点编码更"紧凑"（维度更低）
2. 紧凑的编码需要更少的权重来实现
3. Weight Decay 偏好小权重 → 偏好紧凑编码 → 偏好流形

权重效率是流形发现的「结果」，不是「原因」。

━━━━━━━━━━━━━━━━━━━━

◆ 可验证预测

━━━━━━━━━━━━━━━━━━━━

如果流形发现假说是对的，应该能测到以下现象：

────────────────────

【预测 1：内在维度突变】

• Grokking 前：中间层激活的内在维度 ≈ 训练样本数（每个样本一个自由度）
• Grokking 后：内在维度 ≈ 任务的真实自由度（模运算 = 1，因为是圆）

💡「内在维度」：数据"真正"需要多少维来描述。100 维空间里的数据如果都躺在一个平面上，内在维度就是 2。

实验方法：用 TwoNN 算法估计内在维度，画出随训练步数的变化曲线。

预期结果：内在维度在 Grokking 瞬间「突然下降」。

────────────────────

【预测 2：拓扑结构匹配】

• Grokking 后，中间层激活的拓扑结构应该和任务结构一致
• 模运算 → 表示应该形成一个「环」（一个洞）

实验方法：用持续同调（persistent homology）计算 Betti 数。

💡「Betti 数」：描述拓扑结构的数字。β₀ = 连通分量数，β₁ = 洞/环的数量。圆有一个洞，所以 β₁ = 1。

预期结果：Grokking 前 β₁ ≈ 0（散乱的点），Grokking 后 β₁ = 1（发现了圆）。

────────────────────

【预测 3：注意力熵的动态】

• 早期（过拟合）：低熵——每个样本有专用的 attention 模式
• Grokking 前夕：熵上升——开始探索不同模式
• Grokking 后：熵稳定在中等水平——找到共享结构

预期结果：熵曲线呈现「低 → 高 → 中」的三段式。

────────────────────

【预测 4：强制秩约束的影响】

在中间层加一个低秩瓶颈（限制维度）：

• 秩 = 任务真实自由度 → 加速 Grokking（直接告诉模型答案是几维的）
• 秩 < 任务真实自由度 → 阻止 Grokking（空间太小，装不下流形）
• 秩 > 任务真实自由度 → 不影响或轻微减慢

这个实验可以直接验证"Grokking = 发现低维流形"的假说。

━━━━━━━━━━━━━━━━━━━━

◆ 总结

━━━━━━━━━━━━━━━━━━━━

【四种理论各抓住了什么】

• Goldilocks Zone：泛化发生的位置（权重范数区间）
• Softmax Collapse：训练不卡死的条件（数值稳定性）
• Lazy → Rich：权重动态的相变（从线性到非线性）
• 权重效率：泛化解的特征（更紧凑）
• 电路逆向：具体学到了什么（傅里叶基）

【共同盲区】

都没解释：为什么这些条件凑在一起就能泛化？

【流形发现假说的统一解释】

• 记忆 = 孤立点编码
• 泛化 = 流形发现
• Grokking = 从前者到后者的拓扑相变
• Weight Decay = 阻止在孤立点安家的向心力
• Goldilocks Zone = 流形可见的激活区间
• 傅里叶基 = 圆（流形）的自然坐标系

【一句话】

「记忆是孤立点，泛化是流形，Grokking 是从前者到后者的相变。」

━━━━━━━━━━━━━━━━━━━━

◆ 名词速查

━━━━━━━━━━━━━━━━━━━━

💡「Grokking」：训练 Loss 降到 0 后继续训练，验证 Loss 突然下跌的现象

💡「logit」：Softmax 之前的原始输出值

💡「激活值」：神经网络中间层的输出

💡「流形」：高维空间里弯曲的低维曲面

💡「内在维度」：数据真正需要的维度数

💡「Betti 数」：描述拓扑结构的数字（β₁ = 洞的数量）

💡「持续同调」：计算数据拓扑结构的数学工具

💡「Lazy regime」：权重几乎不动，模型像线性分类器

💡「Rich regime」：权重大幅调整，学到非线性特征

━━━━━━━━━━━━━━━━━━━━

◆ 参考文献

━━━━━━━━━━━━━━━━━━━━

1. Power et al. 2022 "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets"
   https://arxiv.org/abs/2201.02177

2. Liu et al. 2022 "Towards Understanding Grokking: An Effective Theory of Representation Learning" (NeurIPS)
   https://arxiv.org/abs/2205.10343

3. Nanda et al. 2023 "Progress Measures for Grokking via Mechanistic Interpretability" (ICLR Oral)
   https://arxiv.org/abs/2301.05217

4. Kumar et al. 2024 "Grokking as the Transition from Lazy to Rich Training Dynamics" (ICLR)
   https://arxiv.org/abs/2310.06110

5. Prieto et al. 2025 "Grokking at the Edge of Numerical Stability" (ICLR)
   https://arxiv.org/abs/2501.04697

━━━━━━━━━━━━━━━━━━━━

// 靳岩岩的 AI 学习笔记 × Claude 的严谨 × Gemini 的浪漫
// 2026-01-27
