【DeepGEMM】看 DeepSeek 怎么调教英伟达的 8 bit

2025 年 2 月，DeepSeek 搞了个「开源周」，连续 5 天放出核心技术栈。

你可能听说过 FlashMLA（Day 1）和 DeepEP（Day 2），这两个名字够响亮——一个是压缩 KV 缓存的杀手锏，一个是 MoE 专家并行的通信优化。

但 Day 3 放出的 DeepGEMM，存在感就低多了。

「GEMM？不就是矩阵乘法吗？NVIDIA 的 cuBLAS 不香吗？」

问题是：cuBLAS 不支持 DeepSeek 要的 FP8 细粒度量化。

DeepSeek-V3 能用 557 万美元训出来，FP8 训练是核心省钱手段之一。而 DeepGEMM，就是让 FP8 训练真正跑起来的那块拼图。

━━━━━━━━━━━━━━━━━━━━

◆ 开源周全家福

先看看 DeepSeek 开源周都放了什么：

  ┌───────┬─────────────┬───────────────────────────────────────┐
  │ Day   │ 项目         │ 干什么的                               │
  ├───────┼─────────────┼───────────────────────────────────────┤
  │ Day 1 │ FlashMLA    │ MLA 结构的 Flash Attention，压缩 KV    │
  │ Day 2 │ DeepEP      │ MoE 专家并行通信优化                   │
  │ Day 3 │ DeepGEMM    │ FP8 矩阵乘法，细粒度量化 + 两级累加    │
  │ Day 4 │ DualPipe    │ 流水线并行，计算通信重叠               │
  │ Day 5 │ 3FS         │ 为 AI 负载优化的分布式文件系统          │
  └───────┴─────────────┴───────────────────────────────────────┘

FlashMLA 和 DeepEP 解决的是「怎么让 671B 模型跑得动」。

DeepGEMM 解决的是「怎么让训练省钱」。

这篇文章专门讲 DeepGEMM。

━━━━━━━━━━━━━━━━━━━━

◆ 为什么要 FP8？

训练大模型最费什么？显存和算力。能不能用更少的位数存数字，省显存还跑得快？

能。这就是 FP8 的意义。但 8 bit 精度不够怎么办？

先看数字：

  ┌────────────┬──────────┬────────────┬────────────┐
  │ 数据格式    │ 位数      │ 一个数占    │ 相对 FP32  │
  ├────────────┼──────────┼────────────┼────────────┤
  │ FP32       │ 32 bit   │ 4 字节      │ 100%       │
  │ BF16       │ 16 bit   │ 2 字节      │ 50%        │
  │ FP8        │ 8 bit    │ 1 字节      │ 25%        │
  └────────────┴──────────┴────────────┴────────────┘

用 FP8 替代 FP32：「显存省 75%，带宽省 75%，Tensor Core 吞吐量翻倍」。

💡 Tensor Core 是 NVIDIA 从 2017 年开始在 GPU 里加的专用矩阵乘单元。当年和游戏玩家最爱的 DLSS 超分辨率一起发布——没错，让你 1080p 跑出 4K 画质的黑科技，底层就是这玩意儿。玩游戏时，CUDA Core 画低分辨率草稿，Tensor Core 跑神经网络把画面「脑补」成高分辨率；训练大模型时，Tensor Core 负责算矩阵乘。同一个硬件，不同用途。它比普通 CUDA Core 算矩阵乘快得多，但精度选项受硬件限制。

有意思的是，Tensor Core 和 Transformer 是「同年出生」的：

  ┌──────┬─────────────────────────┬─────────────────────────┐
  │ 年份  │ NVIDIA GPU              │ LLM 里程碑               │
  ├──────┼─────────────────────────┼─────────────────────────┤
  │ 2017 │ Volta，Tensor Core 首发  │ Transformer 论文发表     │
  │ 2018 │ RTX 20 系列（消费级）     │ BERT、GPT-1             │
  │ 2020 │ A100（Tensor Core 成熟） │ GPT-3                   │
  │ 2022 │ H100（FP8 Tensor Core）  │ ChatGPT                 │
  │ 2024 │ H200 / B100              │ DeepSeek-V3、GPT-4o     │
  └──────┴─────────────────────────┴─────────────────────────┘

没有 Tensor Core，现代 LLM 根本训不动。这俩是天生一对。

这不是白嫖吗？

问题是：8 bit 能表示的数太少了，精度不够怎么办？

━━━━━━━━━━━━━━━━━━━━

◆ FP8 的两种格式：E4M3 vs E5M2

NVIDIA Hopper GPU 支持两种 FP8 格式：

  ┌────────┬─────────────────────────┬────────────┬──────────────────┐
  │ 格式    │ 结构                     │ 最大值      │ 特点              │
  ├────────┼─────────────────────────┼────────────┼──────────────────┤
  │ E4M3   │ 1 符号 + 4 指数 + 3 尾数  │ ±448       │ 精度高，范围小     │
  │ E5M2   │ 1 符号 + 5 指数 + 2 尾数  │ ±57344     │ 精度低，范围大     │
  └────────┴─────────────────────────┴────────────┴──────────────────┘

💡 人话：指数位越多，能表示的数范围越大；尾数位越多，数越精确。E4M3 牺牲范围换精度，E5M2 反过来。

2022 年 NVIDIA 发论文提出了 FP8 训练方案：「前向传播用 E4M3（要精度），反向传播用 E5M2（梯度数值大）」。

但那只是论文实验。在 DeepSeek-V3 之前，「没有任何开源大模型真正用 FP8 训练过」。

DeepSeek-V3 是第一个在 671B 规模上跑通 FP8 训练的。而且他们更激进——不用混合格式，「全用 E4M3」。

凭什么？

━━━━━━━━━━━━━━━━━━━━

◆ 杀招一：细粒度量化

E4M3 最大只能表示 ±448，如果你的数是 1000 怎么办？

────────────────────

【传统做法】

给整个矩阵算一个 scale（缩放因子），把所有数缩到 ±448 范围内。

问题：矩阵里有一个 outlier（比如 10000），整个矩阵都得跟着它缩，其他正常的数就被压得很小，精度全丢了。

────────────────────

【DeepSeek 的做法】

分组量化，每组单独算 scale：

  ┌────────────┬────────────┬───────────────────────────┐
  │ 张量类型    │ 分组粒度    │ 含义                       │
  ├────────────┼────────────┼───────────────────────────┤
  │ 激活值      │ 1×128      │ 每 128 个元素一个 scale    │
  │ 权重        │ 128×128    │ 每 16384 个元素一个 scale  │
  └────────────┴────────────┴───────────────────────────┘

好处：

• outlier 只影响它所在的那一小组，不祸害整个矩阵
• 组内元素「共享指数位」，相当于变相扩大了动态范围
• 所以不需要 E5M2，「纯 E4M3 就够用」

这就是 DeepSeek-V3 技术报告里说的「fine-grained quantization」。

━━━━━━━━━━━━━━━━━━━━

◆ 杀招二：两级累加

细粒度量化解决了「怎么把大数塞进 FP8」的问题。

但还有第二个坑：「加法精度不够」。

────────────────────

【问题在哪？】

矩阵乘法的本质是：一堆乘法 + 一堆加法。

比如算一个输出元素，你要算：

  结果 = a1×b1 + a2×b2 + a3×b3 + ... + a4096×b4096

这里有 4096 次乘法，然后把 4096 个乘积「累加」起来。

Tensor Core 做乘法没问题。但累加的时候，有个硬件层面的坑：

⚠️ NVIDIA 的 Tensor Core 在做 FP8 累加时，内部寄存器精度只有约 14 bit。

你可能会问：E4M3 的尾数才 3 bit，累加器给了 14 bit，不是绰绰有余吗？

问题是：两个 FP8 相乘，结果的精度需求会「膨胀」。而且你要累加几千个这样的乘积，精度需求更大。

作为对比，FP32 的情况符合直觉：尾数 23 bit，累加器也是 23 bit，精度匹配。

但 FP8 累加器只给 14 bit——看起来比 E4M3 的 3 bit 尾数大多了，「实际上根本不够用」。

这不是 bug，是 NVIDIA 的设计选择——为了让 FP8 跑得快，牺牲了累加精度。你用 H100 就得接受这个现实。

14 bit 能精确表示的最大整数是 16384。

如果你累加 4096 个数，每个数平均值是 4，总和就是 16384——刚好撞到精度上限。

再往上加，就开始「四舍五入」丢精度了。

────────────────────

【有多严重？】

DeepSeek 实测：「K=4096 的矩阵乘法，累加误差最大能到 2%」。

2% 听起来不多？

训练的时候，梯度要反向传播几百层。每层 2% 误差，传 100 层就是：

  0.98^100 ≈ 0.13

⚠️ 梯度只剩 13%，其他全丢了。模型学不动。

────────────────────

【解决方案：两级累加】

既然 Tensor Core 累加精度不够，那就「分段结算」：

1. Tensor Core 做 FP8 乘法，累加 128 次
2. 把这 128 次的部分和搬到 CUDA Core 的 FP32 寄存器
3. CUDA Core 用 FP32 精度累加这些部分和
4. 重复，直到 4096 次乘法全算完

💡 人话：

Tensor Core 像是心算很快但只能算到万位的速算选手。让他算 4096 个数的总和，算到后面就开始出错。

解决办法：每算 128 个数，就把结果报给旁边拿计算器的人（CUDA Core）。计算器虽然慢，但能精确到小数点后 7 位。最后由计算器把所有部分和加起来，结果就精确了。

────────────────────

【代价】

多了一步数据搬运（Tensor Core → CUDA Core）。

但 DeepSeek 用 Warp 专用化把这个延迟藏起来了（下一节讲）。

━━━━━━━━━━━━━━━━━━━━

◆ 工程实现：TMA + Warp 专用化

精度问题解决了，下一个问题是：怎么跑得快？

DeepGEMM 用了两个 Hopper 架构的新特性：

────────────────────

【TMA：Tensor Memory Accelerator】

如果你经历过 DOS 时代，对 DMA（直接内存访问）肯定不陌生——让声卡、硬盘自己搬数据，不用 CPU 一个字节一个字节地搬。

TMA 就是 NVIDIA 在 Hopper 架构给 Tensor Core 配的「专属 DMA」。

传统做法：GPU 线程自己算地址、自己搬数据，搬完才能算。

TMA 做法：「一条指令告诉 TMA 要搬什么形状的数据块，它自己异步搬，不占计算单元」。

💡 人话：以前是工人自己去仓库扛材料，现在有专门的叉车送到工位，工人只管算。

────────────────────

【Warp 专用化】

💡 Warp 是 NVIDIA GPU 的硬件调度单位：32 个线程 = 1 个 Warp，这是写死的。同一个 Warp 里的线程必须同时执行同一条指令。

开发者写 CUDA 代码时指定「我要启动 256 个线程」，GPU 硬件自动把它切成 8 个 Warp（256 ÷ 32 = 8）。然后开发者可以通过代码让不同 Warp 干不同的活。

DeepGEMM 的做法就是让不同 Warp「分工」：

  ┌────────────────┬───────────────────────────────────┐
  │ Warp 类型       │ 职责                               │
  ├────────────────┼───────────────────────────────────┤
  │ TMA Warp       │ 专门搬数据（全局内存 → 共享内存）   │
  │ Math Warp      │ 专门算矩阵乘法（调用 Tensor Core）  │
  │ Reduce Warp    │ 专门做累加归约（1d2d 模式）         │
  └────────────────┴───────────────────────────────────┘

关键是：「它们并行工作，计算和搬运完全重叠」。

Math Warp 在算第 N 块数据的时候，TMA Warp 已经在搬第 N+1 块了。等第 N 块算完，第 N+1 块的数据已经到位，无缝衔接。

━━━━━━━━━━━━━━━━━━━━

◆ 大矩阵优化：1d1d vs 1d2d

上面讲的 TMA + Warp 分工，对于小矩阵够用了。DeepGEMM 管这个叫「1d1d 模式」。

但大矩阵有个问题：「一次算不完」。这时候需要「1d2d 模式」——在 Warp 分工基础上加双缓冲。

💡 Warp 分工和双缓冲是两件事：Warp 分工解决「让搬数据和算数的人分开」，双缓冲解决「让它们能同时干活」。打个比方：搬运工和厨师分开是 Warp 分工，有两个灶台让他们能同时忙是双缓冲。1d1d 只有分工，1d2d 是分工 + 双缓冲。

────────────────────

【问题在哪？】

GPU 有三级存储，以 RTX 4090 为例：

💡 SM（Streaming Multiprocessor，流式多处理器）是 GPU 的基本计算单元，你可以把它理解成 GPU 里的「车间」。每个 SM 里有：128 个 CUDA Core（通用计算工人）、4 个 Tensor Core（矩阵乘专业工人）、以及自己的寄存器和共享内存。RTX 4090 有 128 个 SM，所以规格表上写的「16384 个 CUDA Core」= 128 SM × 128 CUDA Core/SM。

  ┌────────────┬────────────────────┬────────────────────┐
  │ 层级        │ 每 SM 大小          │ 整卡（128 SM）      │
  ├────────────┼────────────────────┼────────────────────┤
  │ 寄存器      │ 256 KB             │ 32 MB              │
  │ 共享内存/L1 │ 128 KB             │ 16 MB              │
  │ 全局显存    │ -                  │ 24 GB              │
  └────────────┴────────────────────┴────────────────────┘

片上高速存储（寄存器 + 共享内存）加起来才 48 MB，全局显存 24 GB——差了 500 倍。

💡 如果你经历过 CPU 优化的年代，这就是 GPU 版的「内存 → L2 Cache → L1 Cache → 寄存器」。共享内存大概相当于 L1/L2 的角色——TMA 把数据从全局内存搬到这里，Tensor Core 从这里读数据算矩阵乘。它很快，但很小。

一个 4096×4096 的 FP8 矩阵有多大？4096 × 4096 × 1 字节 = 16 MB。

48 KB vs 16 MB —— 差了 300 多倍。

所以大矩阵必须切成小块，一块一块算，算完再把结果拼起来。

问题来了：算完一块、拼结果、再算下一块——中间 Tensor Core 在「等」，利用率掉下去了。

────────────────────

【解法：双缓冲】

既然等待是问题，那就「别让它等」。

把共享内存分成两块（Buffer A 和 Buffer B），三件事同时干：

• TMA 往 Buffer A 搬下一批数据
• Tensor Core 算 Buffer B 里的矩阵乘
• 另一组 Warp 把上一批的结果拼起来

三条流水线完全并行，Tensor Core 永远有活干。

💡 人话：就像餐厅后厨——洗菜的、炒菜的、装盘的同时干活，不用等上一道工序做完。

────────────────────

【效果】

  ┌─────────────────────┬─────────────────┬───────────────────────────┐
  │ 对比项               │ 1d1d 模式        │ 1d2d 模式                  │
  ├─────────────────────┼─────────────────┼───────────────────────────┤
  │ Tensor Core 利用率   │ 85% ~ 90%       │ 95% ~ 98%                  │
  │ 相对性能             │ 基准 100%       │ 快 15% ~ 25%               │
  │ 共享内存占用         │ ~48 KB          │ ~96 KB（双缓冲）            │
  │ 适用场景             │ 中小矩阵         │ 大矩阵（M/N/K ≥ 4096）     │
  └─────────────────────┴─────────────────┴───────────────────────────┘

代价是多占一倍共享内存，但换来 15-25% 的性能提升，值。

💡 这两个名字是 DeepGEMM 源码里的正式命名，去看代码能直接对上。

━━━━━━━━━━━━━━━━━━━━

◆ JIT 编译：运行时按需生成最优内核

矩阵形状千变万化，不可能提前编译好所有情况。

DeepGEMM 的做法：「运行时编译（JIT）」

用户调用 API 时，DeepGEMM 根据矩阵尺寸、精度、硬件配置，「现场生成最优的 CUDA 代码并编译」。

好处：

• 安装时不需要编译，pip install 直接用
• 每种矩阵形状都能得到量身定制的内核
• 编译结果会缓存，第二次调用直接用

支持两种编译器：

• NVCC：性能最优，但编译慢
• NVRTC：编译快 10 倍，性能略低

━━━━━━━━━━━━━━━━━━━━

◆ 性能结果

  ┌────────────────────────┬─────────────────────────────┐
  │ 指标                    │ 数值                         │
  ├────────────────────────┼─────────────────────────────┤
  │ H800 峰值性能           │ 1550 TFLOPS（2025.04 更新）  │
  │ 相比参考实现            │ 快 2.7 倍                    │
  │ 精度损失（vs BF16）     │ < 0.25%                      │
  └────────────────────────┴─────────────────────────────┘

DeepSeek-V3 的 671B 参数模型，就是用这套 FP8 框架训练的。

━━━━━━━━━━━━━━━━━━━━

◆ 还有什么新东西？

【SM100（Blackwell）支持】

  ┌───────────────────┬────────────────────────┬─────────────────┐
  │ 架构               │ 支持的内存布局          │ scale 格式       │
  ├───────────────────┼────────────────────────┼─────────────────┤
  │ SM90（Hopper）    │ 仅 NT                   │ FP32            │
  │ SM100（Blackwell）│ NT/TN/NN/TT 全支持      │ 打包的 UE8M0    │
  └───────────────────┴────────────────────────┴─────────────────┘

【MQA 评分内核（2025.09 新增）】

为 DeepSeek v3.2 的 lightning indexer 加的：

• fp8_mqa_logits：预填充阶段
• fp8_paged_mqa_logits：解码阶段，支持分页 KV-Cache

【代码简洁性】

DeepGEMM 的核心内核只有约「300 行代码」。

官方原话："as clean as a tutorial"（像教程一样简洁）。

想学 GPU 优化的，直接读源码就能懂。

━━━━━━━━━━━━━━━━━━━━

◆ 小结

DeepGEMM 解决的核心问题：「怎么用 8 bit 低精度做训练，还不丢精度？」

答案是两招：

1.「细粒度量化」：1×128 / 128×128 分组，outlier 隔离，纯 E4M3 就够用
2.「两级累加」：Tensor Core 算乘法，CUDA Core 攒加法，精度损失 < 0.25%

工程实现上：

• TMA 异步搬运 + Warp 专用化 = 计算和搬运完全重叠
• 1d2d 双缓冲 = 大矩阵 Tensor Core 利用率 95%+
• JIT 编译 = 每种形状都有最优内核

结果：「H800 上 1550 TFLOPS，比参考实现快 2.7 倍」。

━━━━━━━━━━━━━━━━━━━━

◆ 参考资料

• DeepGEMM GitHub：https://github.com/deepseek-ai/DeepGEMM
• DeepSeek-V3 Technical Report：https://arxiv.org/abs/2412.19437

━━━━━━━━━━━━━━━━━━━━

// 靳岩岩的 AI 学习笔记 × Claude 的严谨 × Gemini 的浪漫
// 2026-01-19
