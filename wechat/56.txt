【DeepGEMM】DeepSeek 怎么把 8 bit 训练玩明白的

2025 年 2 月，DeepSeek 搞了个「开源周」，连续 5 天放出核心技术栈。

你可能听说过 FlashMLA（Day 1）和 DeepEP（Day 2），这两个名字够响亮——一个是压缩 KV 缓存的杀手锏，一个是 MoE 专家并行的通信优化。

但 Day 3 放出的 DeepGEMM，存在感就低多了。

「GEMM？不就是矩阵乘法吗？NVIDIA 的 cuBLAS 不香吗？」

问题是：cuBLAS 不支持 DeepSeek 要的 FP8 细粒度量化。

DeepSeek-V3 能用 557 万美元训出来，FP8 训练是核心省钱手段之一。而 DeepGEMM，就是让 FP8 训练真正跑起来的那块拼图。

━━━━━━━━━━━━━━━━━━━━

◆ 开源周全家福

先看看 DeepSeek 开源周都放了什么：

  ┌───────┬─────────────┬───────────────────────────────────────┐
  │ Day   │ 项目         │ 干什么的                               │
  ├───────┼─────────────┼───────────────────────────────────────┤
  │ Day 1 │ FlashMLA    │ MLA 结构的 Flash Attention，压缩 KV    │
  │ Day 2 │ DeepEP      │ MoE 专家并行通信优化                   │
  │ Day 3 │ DeepGEMM    │ FP8 矩阵乘法，细粒度量化 + 两级累加    │
  │ Day 4 │ DualPipe    │ 流水线并行，计算通信重叠               │
  │ Day 5 │ 3FS         │ 为 AI 负载优化的分布式文件系统          │
  └───────┴─────────────┴───────────────────────────────────────┘

FlashMLA 和 DeepEP 解决的是「怎么让 671B 模型跑得动」。

DeepGEMM 解决的是「怎么让训练省钱」。

这篇文章专门讲 DeepGEMM。

━━━━━━━━━━━━━━━━━━━━

◆ 为什么要 FP8？

训练大模型最费什么？显存和算力。能不能用更少的位数存数字，省显存还跑得快？

能。这就是 FP8 的意义。但 8 bit 精度不够怎么办？

先看数字：

  ┌────────────┬──────────┬────────────┬────────────┐
  │ 数据格式    │ 位数      │ 一个数占    │ 相对 FP32  │
  ├────────────┼──────────┼────────────┼────────────┤
  │ FP32       │ 32 bit   │ 4 字节      │ 100%       │
  │ BF16       │ 16 bit   │ 2 字节      │ 50%        │
  │ FP8        │ 8 bit    │ 1 字节      │ 25%        │
  └────────────┴──────────┴────────────┴────────────┘

用 FP8 替代 FP32：「显存省 75%，带宽省 75%，Tensor Core 吞吐量翻倍」。

这不是白嫖吗？

问题是：8 bit 能表示的数太少了，精度不够怎么办？

━━━━━━━━━━━━━━━━━━━━

◆ FP8 的两种格式：E4M3 vs E5M2

NVIDIA Hopper GPU 支持两种 FP8 格式：

  ┌────────┬─────────────────────────┬────────────┬──────────────────┐
  │ 格式    │ 结构                     │ 最大值      │ 特点              │
  ├────────┼─────────────────────────┼────────────┼──────────────────┤
  │ E4M3   │ 1 符号 + 4 指数 + 3 尾数  │ ±448       │ 精度高，范围小     │
  │ E5M2   │ 1 符号 + 5 指数 + 2 尾数  │ ±57344     │ 精度低，范围大     │
  └────────┴─────────────────────────┴────────────┴──────────────────┘

💡 人话：指数位越多，能表示的数范围越大；尾数位越多，数越精确。E4M3 牺牲范围换精度，E5M2 反过来。

之前业界的做法：「前向传播用 E4M3（要精度），反向传播用 E5M2（梯度数值大）」。

DeepSeek 说：不用这么麻烦，「我全用 E4M3」。

凭什么？

━━━━━━━━━━━━━━━━━━━━

◆ 杀招一：细粒度量化

E4M3 最大只能表示 ±448，如果你的数是 1000 怎么办？

────────────────────

【传统做法】

给整个矩阵算一个 scale（缩放因子），把所有数缩到 ±448 范围内。

问题：矩阵里有一个 outlier（比如 10000），整个矩阵都得跟着它缩，其他正常的数就被压得很小，精度全丢了。

────────────────────

【DeepSeek 的做法】

分组量化，每组单独算 scale：

  ┌────────────┬────────────┬───────────────────────────┐
  │ 张量类型    │ 分组粒度    │ 含义                       │
  ├────────────┼────────────┼───────────────────────────┤
  │ 激活值      │ 1×128      │ 每 128 个元素一个 scale    │
  │ 权重        │ 128×128    │ 每 16384 个元素一个 scale  │
  └────────────┴────────────┴───────────────────────────┘

好处：

• outlier 只影响它所在的那一小组，不祸害整个矩阵
• 组内元素「共享指数位」，相当于变相扩大了动态范围
• 所以不需要 E5M2，「纯 E4M3 就够用」

这就是 DeepSeek-V3 技术报告里说的「fine-grained quantization」。

━━━━━━━━━━━━━━━━━━━━

◆ 杀招二：两级累加

FP8 还有一个坑：「Tensor Core 的累加精度只有约 14 bit」。

什么意思？

矩阵乘法是一堆乘法加一堆加法。Tensor Core 做乘法没问题，但做加法（累加）的时候，内部精度不是 FP32，只有大约 14 bit。

DeepSeek 实测：「K=4096 的矩阵乘法，累加误差最大能到 2%」。

2% 对训练来说是致命的。

────────────────────

【解决方案：两级累加】

流程：

1. Tensor Core 做 FP8 乘法 + 低精度累加
2. 每 128 次乘加操作，把部分和搬到 CUDA Core 的 FP32 寄存器
3. CUDA Core 做高精度累加
4. 重复直到算完

💡 人话：Tensor Core 是快但糙的工人，算得飞快但容易出错。CUDA Core 是慢但精的质检员，每隔一段就把半成品收走检验一下，保证最终结果精确。

代价是什么？多了一步数据搬运。但 DeepSeek 用 Warp 专用化把这个延迟藏起来了（下一节讲）。

━━━━━━━━━━━━━━━━━━━━

◆ 工程实现：TMA + Warp 专用化

精度问题解决了，下一个问题是：怎么跑得快？

DeepGEMM 用了两个 Hopper 架构的新特性：

────────────────────

【TMA：Tensor Memory Accelerator】

传统做法：CPU 算好地址，告诉 GPU 从哪搬数据，GPU 一个线程一个线程地搬。

TMA 做法：「GPU 里有个专门的硬件单元搬数据，一条指令搬一整块，异步的，不占计算单元」。

💡 人话：以前是工人自己去仓库扛材料，现在有专门的叉车送到工位。

────────────────────

【Warp 专用化】

一个线程块里有多个 Warp（32 个线程一组）。DeepGEMM 让不同 Warp 干不同的活：

  ┌────────────────┬───────────────────────────────────┐
  │ Warp 类型       │ 职责                               │
  ├────────────────┼───────────────────────────────────┤
  │ TMA Warp       │ 专门搬数据（全局内存 → 共享内存）   │
  │ Math Warp      │ 专门算矩阵乘法（调用 Tensor Core）  │
  │ Reduce Warp    │ 专门做累加归约（1d2d 模式）         │
  └────────────────┴───────────────────────────────────┘

关键是：「它们并行工作，计算和搬运完全重叠」。

Math Warp 在算第 N 块数据的时候，TMA Warp 已经在搬第 N+1 块了。等第 N 块算完，第 N+1 块的数据已经到位，无缝衔接。

━━━━━━━━━━━━━━━━━━━━

◆ 1d1d vs 1d2d：大矩阵的极致优化

对于小矩阵，上面的流程（1d1d 模式）够用了。

但大矩阵（>256×256）有个问题：共享内存不够，得切成小块算，算完再拼起来（reduce）。这个拼接过程会让 Tensor Core 闲置。

────────────────────

【1d2d 模式的解法】

加两个 Warp 专门做拼接，同时用双缓冲：

• 共享内存分成 Buffer A 和 Buffer B
• TMA 往 A 搬数据的时候，Math 在算 B 的乘法，Reduce 在拼 A 的结果
• 三条流水线完全并行，Tensor Core 利用率拉到 95-98%

────────────────────

【性能对比】

  ┌─────────────────────┬─────────────────┬───────────────────────────┐
  │ 对比项               │ 1d1d 内核        │ 1d2d 内核                  │
  ├─────────────────────┼─────────────────┼───────────────────────────┤
  │ Tensor Core 利用率   │ 85% ~ 90%       │ 95% ~ 98%                  │
  │ 相对性能             │ 基准 100%       │ 快 15% ~ 25%               │
  │ 共享内存占用         │ ~48 KB（单缓冲） │ ~96 KB（双缓冲）           │
  │ 适用场景             │ 中小矩阵         │ 超大矩阵（M/N/K ≥ 4096）   │
  └─────────────────────┴─────────────────┴───────────────────────────┘

━━━━━━━━━━━━━━━━━━━━

◆ JIT 编译：运行时按需生成最优内核

矩阵形状千变万化，不可能提前编译好所有情况。

DeepGEMM 的做法：「运行时编译（JIT）」

用户调用 API 时，DeepGEMM 根据矩阵尺寸、精度、硬件配置，「现场生成最优的 CUDA 代码并编译」。

好处：

• 安装时不需要编译，pip install 直接用
• 每种矩阵形状都能得到量身定制的内核
• 编译结果会缓存，第二次调用直接用

支持两种编译器：

• NVCC：性能最优，但编译慢
• NVRTC：编译快 10 倍，性能略低

━━━━━━━━━━━━━━━━━━━━

◆ 性能结果

  ┌────────────────────────┬─────────────────────────────┐
  │ 指标                    │ 数值                         │
  ├────────────────────────┼─────────────────────────────┤
  │ H800 峰值性能           │ 1550 TFLOPS（2025.04 更新）  │
  │ 相比参考实现            │ 快 2.7 倍                    │
  │ 精度损失（vs BF16）     │ < 0.25%                      │
  └────────────────────────┴─────────────────────────────┘

DeepSeek-V3 的 671B 参数模型，就是用这套 FP8 框架训练的。

━━━━━━━━━━━━━━━━━━━━

◆ 还有什么新东西？

【SM100（Blackwell）支持】

  ┌───────────────────┬────────────────────────┬─────────────────┐
  │ 架构               │ 支持的内存布局          │ scale 格式       │
  ├───────────────────┼────────────────────────┼─────────────────┤
  │ SM90（Hopper）    │ 仅 NT                   │ FP32            │
  │ SM100（Blackwell）│ NT/TN/NN/TT 全支持      │ 打包的 UE8M0    │
  └───────────────────┴────────────────────────┴─────────────────┘

【MQA 评分内核（2025.09 新增）】

为 DeepSeek v3.2 的 lightning indexer 加的：

• fp8_mqa_logits：预填充阶段
• fp8_paged_mqa_logits：解码阶段，支持分页 KV-Cache

【代码简洁性】

DeepGEMM 的核心内核只有约「300 行代码」。

官方原话："as clean as a tutorial"（像教程一样简洁）。

想学 GPU 优化的，直接读源码就能懂。

━━━━━━━━━━━━━━━━━━━━

◆ 小结

DeepGEMM 解决的核心问题：「怎么用 8 bit 低精度做训练，还不丢精度？」

答案是两招：

1.「细粒度量化」：1×128 / 128×128 分组，outlier 隔离，纯 E4M3 就够用
2.「两级累加」：Tensor Core 算乘法，CUDA Core 攒加法，精度损失 < 0.25%

工程实现上：

• TMA 异步搬运 + Warp 专用化 = 计算和搬运完全重叠
• 1d2d 双缓冲 = 大矩阵 Tensor Core 利用率 95%+
• JIT 编译 = 每种形状都有最优内核

结果：「H800 上 1550 TFLOPS，比参考实现快 2.7 倍」。

━━━━━━━━━━━━━━━━━━━━

◆ 参考资料

• DeepGEMM GitHub：https://github.com/deepseek-ai/DeepGEMM
• DeepSeek-V3 Technical Report：https://arxiv.org/abs/2412.19437

━━━━━━━━━━━━━━━━━━━━

// 靳岩岩的 AI 学习笔记 × Claude 的严谨 × Gemini 的浪漫
// 2026-01-19
