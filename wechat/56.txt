【基础知识】DeepSeek 怎么用 FP8 节省成本的

2025 年 2 月，DeepSeek 搞了个「开源周」，连续 5 天放出核心技术栈。

你可能听说过 FlashMLA（Day 1）和 DeepEP（Day 2），这两个名字够响亮——一个是压缩 KV 缓存的杀手锏，一个是 MoE 专家并行的通信优化。

但 Day 3 放出的 DeepGEMM，存在感就低多了。

「GEMM？不就是矩阵乘法吗？NVIDIA 官方库不香吗？」

💡 GEMM = General Matrix Multiply（通用矩阵乘法），就是 C = A × B。神经网络里 90% 以上的计算量都是这个——线性层、Attention、FFN，底层全是矩阵乘。各家 GPU 厂商花了几十年优化这一个运算，NVIDIA 的 cuBLAS、AMD 的 rocBLAS，本质上都是「把 GEMM 跑到极致」的库。

问题是：NVIDIA 官方的矩阵乘法库（cuBLAS）不支持 DeepSeek 要的 FP8 细粒度量化。

DeepSeek-V3 能用 557 万美元训出来，FP8 训练是核心省钱手段之一。而 DeepGEMM，就是让 FP8 训练真正跑起来的那块拼图。

━━━━━━━━━━━━━━━━━━━━

◆ 开源周全家福

先看看 DeepSeek 开源周都放了什么：

  +-------+-------------+---------------------------------------+
  | Day   | 项目        | 干什么的                              |
  +-------+-------------+---------------------------------------+
  | Day 1 | FlashMLA    | MLA 结构的 Flash Attention，压缩 KV   |
  | Day 2 | DeepEP      | MoE 专家并行通信优化                  |
  | Day 3 | DeepGEMM    | FP8 矩阵乘法，细粒度量化 + 两级累加   |
  | Day 4 | DualPipe    | 流水线并行，计算通信重叠              |
  | Day 5 | 3FS         | 为 AI 负载优化的分布式文件系统        |
  +-------+-------------+---------------------------------------+

FlashMLA 和 DeepEP 解决的是「怎么让 671B 模型跑得动」。

DeepGEMM 解决的是「怎么让训练省钱」。

这篇文章专门讲 DeepGEMM。

━━━━━━━━━━━━━━━━━━━━

◆ 为什么要 FP8？

训练大模型最费什么？显存和算力。能不能用更少的位数存数字，省显存还跑得快？

能。这就是 FP8 的意义。但 8 bit 精度不够怎么办？

先看数字：

  +----------+--------+----------+----------+
  | 数据格式 | 位数   | 一个数占 | 相对FP32 |
  +----------+--------+----------+----------+
  | FP32     | 32 bit | 4 字节   | 100%     |
  | BF16     | 16 bit | 2 字节   | 50%      |
  | FP8      | 8 bit  | 1 字节   | 25%      |
  +----------+--------+----------+----------+

你肯定在网上看到别人说：用 FP8 替代 FP32，「显存省 75%，带宽省 75%，Tensor Core 吞吐量翻倍」。

💡 Tensor Core 是 NVIDIA 从 2017 年开始在 GPU 里加的专用矩阵乘单元。最初是给数据中心的 AI 训练用的（Tesla V100），2018 年才进入消费级显卡（RTX 20 系列），同时带来了游戏玩家最爱的 DLSS——让你 1080p 跑出 4K 画质的黑科技。玩游戏时，CUDA Core 画低分辨率草稿，Tensor Core 跑神经网络把画面「脑补」成高分辨率；训练大模型时，Tensor Core 负责算矩阵乘。同一个硬件，不同用途。它比普通 CUDA Core 算矩阵乘快得多，但精度选项受硬件限制。

Tensor Core 比 CUDA Core 快多少？「理论峰值 16 倍，实际训练 3-5 倍」。原理很简单：CUDA Core 一个周期算一次乘加，Tensor Core 一个周期算一个 4×4 矩阵乘法（64 次乘加）。以前的 AI（2012 年的 AlexNet、2015 年的 ResNet）也要算矩阵，但那时候模型小（几百万到几亿参数），CUDA Core 硬扛也能训。现在动辄几千亿参数，没有 Tensor Core 根本玩不起来。

💡 注：4×4 是最早期 Tensor Core 的基本单元。新架构（如 Hopper）的 Tensor Core 支持更大的矩阵块（如 16×16×16），吞吐更高，但原理一样——把一坨乘加打包成一条指令。

有意思的是，Tensor Core 和 Transformer 是「同年出生」的：

  +------+----------------------------------------------+---------------------------+
  | 年份 | NVIDIA GPU                                   | LLM 里程碑                |
  +------+----------------------------------------------+---------------------------+
  | 2017 | V100 (Volta, Tensor Core 首发)               | Transformer 论文发表      |
  | 2018 | RTX 20 系列 (Turing, 消费级首次有 TC)        | BERT, GPT-1               |
  | 2020 | A100 (Ampere) / RTX 30 系列                  | GPT-3                     |
  | 2022 | H100 (Hopper, FP8 TC) / RTX 40 系列 (Ada)    | ChatGPT                   |
  | 2024 | H200 (Hopper 升级版)                         | DeepSeek-V3, GPT-4o       |
  | 2025 | B200 (Blackwell) / RTX 50 系列               | GPT-5, Gemini 3.0, Opus 4.5|
  +------+----------------------------------------------+---------------------------+

没有 Tensor Core，现代 LLM 根本训不动。这俩是天生一对。

回到 FP8 的问题：省显存、省带宽、吞吐翻倍，听起来很美好，但 8 bit 能表示的数太少了，精度不够怎么办？

━━━━━━━━━━━━━━━━━━━━

◆ FP8 的两种格式：E4M3 vs E5M2

NVIDIA Hopper GPU 支持两种 FP8 格式：

  +------+-------------------------+--------+----------------+
  | 格式 | 结构                    | 最大值 | 特点           |
  +------+-------------------------+--------+----------------+
  | E4M3 | 1 符号 + 4 指数 + 3 尾数 | +-448  | 精度高，范围小 |
  | E5M2 | 1 符号 + 5 指数 + 2 尾数 | +-57344| 精度低，范围大 |
  +------+-------------------------+--------+----------------+

💡 人话：指数位越多，能表示的数范围越大；尾数位越多，数越精确。E4M3 牺牲范围换精度，E5M2 反过来。

2022 年 NVIDIA 发论文提出了 FP8 训练方案：「前向传播用 E4M3（要精度），反向传播用 E5M2（梯度数值大）」。

但那只是论文实验。据公开资料，在 DeepSeek-V3 之前，「没有任何开源大模型在这个规模上完整用 FP8 训练过」。

DeepSeek-V3 是第一个在 671B 规模上跑通 FP8 训练的。而且他们更激进——不用混合格式，「全用 E4M3」。

凭什么？

━━━━━━━━━━━━━━━━━━━━

◆ 杀招一：细粒度量化

E4M3 最大只能表示 ±448，如果你的数是 1000 怎么办？

────────────────────

【传统做法】

给整个矩阵算一个 scale（缩放因子），把所有数缩到 ±448 范围内。

问题：矩阵里有一个 outlier（比如 10000），整个矩阵都得跟着它缩，其他正常的数就被压得很小，精度全丢了。

────────────────────

【DeepSeek 的做法】

分组量化，每组单独算 scale：

💡 矩阵乘法 C = A × B，这里 A 通常是「激活值」（输入数据，每次推理都不一样），B 是「权重」（模型参数，训练完就固定了）。

  +----------+----------+---------------------------+
  | 张量类型 | 分组粒度 | 含义                      |
  +----------+----------+---------------------------+
  | 激活值   | 1x128    | 每 128 个元素一个 scale   |
  | 权重     | 128x128  | 每 16384 个元素一个 scale |
  +----------+----------+---------------------------+

好处：

• outlier 只影响它所在的那一小组，不祸害整个矩阵
• 组内元素「共享指数位」，相当于变相扩大了动态范围
• 所以不需要 E5M2，「纯 E4M3 就够用」

这就是 DeepSeek-V3 技术报告里说的「fine-grained quantization」。

━━━━━━━━━━━━━━━━━━━━

◆ 杀招二：两级累加

细粒度量化解决了「怎么把大数塞进 FP8」的问题。

但还有第二个坑：「加法精度不够」。

────────────────────

【问题在哪？】

矩阵乘法的本质是：一堆乘法 + 一堆加法。

比如算一个输出元素，你要算：

  结果 = a1×b1 + a2×b2 + a3×b3 + ... + a4096×b4096

这里有 4096 次乘法，然后把 4096 个乘积「累加」起来。

Tensor Core 做乘法没问题。但累加的时候，有个硬件层面的坑：

⚠️ NVIDIA 的 Tensor Core 在做 FP8 累加时，内部寄存器精度只有约 14 bit。

你可能会问：E4M3 的尾数才 3 bit，累加器给了 14 bit，不是绰绰有余吗？

问题是：两个 FP8 相乘，结果的精度需求会「膨胀」。而且你要累加几千个这样的乘积，精度需求更大。

作为对比，FP32 的情况符合直觉：尾数 23 bit，累加器也是 23 bit，精度匹配。

但 FP8 累加器只给 14 bit——看起来比 E4M3 的 3 bit 尾数大多了，「实际上根本不够用」。

这不是 bug，是 NVIDIA 的设计选择——为了让 FP8 跑得快，牺牲了累加精度。你用 H100 就得接受这个现实。

14 bit 能精确表示的最大整数是 16384。

如果你累加 4096 个数，每个数平均值是 4，总和就是 16384——刚好撞到精度上限。

再往上加，就开始「四舍五入」丢精度了。

────────────────────

【有多严重？】

DeepSeek 实测：「K=4096 的矩阵乘法，累加误差最大能到 2%」。

2% 听起来不多？

训练的时候，梯度要反向传播几百层。每层 2% 误差，传 100 层就是：

  0.98^100 ≈ 0.13

⚠️ 梯度只剩 13%，其他全丢了。模型学不动。

────────────────────

【解决方案：两级累加】

既然 Tensor Core 累加精度不够，那就「分段结算」：

1. Tensor Core 做 FP8 乘法，累加 128 次
2. 把这 128 次的部分和搬到 CUDA Core 的 FP32 寄存器
3. CUDA Core 用 FP32 精度累加这些部分和
4. 重复，直到 4096 次乘法全算完

💡 人话：

Tensor Core 像是心算很快但只能算到万位的速算选手。让他算 4096 个数的总和，算到后面就开始出错。

解决办法：每算 128 个数，就把结果报给旁边拿计算器的人（CUDA Core）。计算器虽然慢，但能精确到小数点后 7 位。最后由计算器把所有部分和加起来，结果就精确了。

────────────────────

【代价】

多了一步数据搬运（Tensor Core → CUDA Core）。

但 DeepSeek 用 Warp 专用化把这个延迟藏起来了（下一节讲）。

━━━━━━━━━━━━━━━━━━━━

◆ 工程实现：TMA + Warp 专用化

精度问题解决了，下一个问题是：怎么跑得快？

DeepGEMM 用了两个 Hopper 架构的新特性：

────────────────────

【TMA：Tensor Memory Accelerator】

如果你经历过 DOS 时代，对 DMA（直接内存访问）肯定不陌生——让声卡、硬盘自己搬数据，不用 CPU 一个字节一个字节地搬。

TMA 就是 NVIDIA 在 Hopper 架构给 Tensor Core 配的「专属 DMA」。

传统做法：GPU 线程自己算地址、自己搬数据，搬完才能算。

TMA 做法：「一条指令告诉 TMA 要搬什么形状的数据块，它自己异步搬，不占计算单元」。

💡 人话：以前是工人自己去仓库扛材料，现在有专门的叉车送到工位，工人只管算。

────────────────────

【Warp 专用化】

💡 Warp 是 NVIDIA GPU 的硬件调度单位：32 个线程 = 1 个 Warp，这是写死的。同一个 Warp 里的线程必须同时执行同一条指令。

开发者写 CUDA 代码时指定「我要启动 256 个线程」，GPU 硬件自动把它切成 8 个 Warp（256 ÷ 32 = 8）。然后开发者可以通过代码让不同 Warp 干不同的活。

DeepGEMM 的做法就是让不同 Warp「分工」：

  +-------------+-------------------------------------+
  | Warp 类型   | 职责                                |
  +-------------+-------------------------------------+
  | TMA Warp    | 专门搬数据 (全局内存 -> 共享内存)   |
  | Math Warp   | 专门算矩阵乘法 (调用 Tensor Core)   |
  | Reduce Warp | 专门做累加归约 (1d2d 模式)          |
  +-------------+-------------------------------------+

关键是：「它们并行工作，计算和搬运完全重叠」。

Math Warp 在算第 N 块数据的时候，TMA Warp 已经在搬第 N+1 块了。等第 N 块算完，第 N+1 块的数据已经到位，无缝衔接。

━━━━━━━━━━━━━━━━━━━━

◆ 大矩阵优化：1d1d vs 1d2d

上面讲的 TMA + Warp 分工，对于小矩阵够用了。DeepGEMM 管这个叫「1d1d 模式」。

但大矩阵有个问题：「一次算不完」。这时候需要「1d2d 模式」——在 Warp 分工基础上加双缓冲。

💡 Warp 分工和双缓冲是两件事：Warp 分工解决「让搬数据和算数的人分开」，双缓冲解决「让它们能同时干活」。打个比方：搬运工和厨师分开是 Warp 分工，有两个灶台让他们能同时忙是双缓冲。1d1d 只有分工，1d2d 是分工 + 双缓冲。

────────────────────

【问题在哪？】

GPU 有三级存储。以消费卡 RTX 4090 和数据中心卡 H100 为例：

💡 SM（Streaming Multiprocessor，流式多处理器）是 GPU 的基本计算单元，你可以把它理解成 GPU 里的「车间」。每个 SM 里有：CUDA Core（通用计算工人）、Tensor Core（矩阵乘专业工人）、以及自己的寄存器和共享内存。RTX 4090 有 128 个 SM，H100 有 132 个 SM。

  +-------------+------------------+------------------+
  | 层级        | RTX 4090 (128SM) | H100 (132 SM)    |
  +-------------+------------------+------------------+
  | 寄存器      | 32 MB            | 33 MB            |
  | 共享内存/L1 | 16 MB            | 33 MB            |
  | 全局显存    | 24 GB            | 80 GB            |
  +-------------+------------------+------------------+

片上高速存储（寄存器 + 共享内存）加起来才 48 MB，全局显存 24 GB——差了 500 倍。

💡 如果你经历过 CPU 优化的年代，这就是 GPU 版的「内存 → L2 Cache → L1 Cache → 寄存器」。共享内存大概相当于 L1/L2 的角色——TMA 把数据从全局内存搬到这里，Tensor Core 从这里读数据算矩阵乘。它很快，但很小。

一个 4096×4096 的 FP8 矩阵有多大？4096 × 4096 × 1 字节 = 16 MB。

48 KB vs 16 MB —— 差了 300 多倍。

所以大矩阵必须切成小块，一块一块算，算完再把结果拼起来。

问题来了：算完一块、拼结果、再算下一块——中间 Tensor Core 在「等」，利用率掉下去了。

────────────────────

【解法：双缓冲】

既然等待是问题，那就「别让它等」。

把共享内存分成两块（Buffer A 和 Buffer B），三件事同时干：

• TMA 往 Buffer A 搬下一批数据
• Tensor Core 算 Buffer B 里的矩阵乘
• 另一组 Warp 把上一批的结果拼起来

三条流水线完全并行，Tensor Core 永远有活干。

💡 人话：就像餐厅后厨——洗菜的、炒菜的、装盘的同时干活，不用等上一道工序做完。

────────────────────

【效果】

  +--------------------+-----------+------------------------+
  | 对比项             | 1d1d 模式 | 1d2d 模式              |
  +--------------------+-----------+------------------------+
  | Tensor Core 利用率 | 85%~90%   | 95%~98%                |
  | 相对性能           | 基准 100% | 快 15%~25%             |
  | 共享内存占用       | ~48 KB    | ~96 KB (双缓冲)        |
  | 适用场景           | 中小矩阵  | 大矩阵 (M/N/K >= 4096) |
  +--------------------+-----------+------------------------+

代价是多占一倍共享内存，但换来 15-25% 的性能提升，值。

💡 这两个名字是 DeepGEMM 源码里的正式命名，去看代码能直接对上。

━━━━━━━━━━━━━━━━━━━━

◆ MoE 专用优化：连续布局 vs 掩码布局

DeepSeek-V3 是 MoE（混合专家）模型，671B 参数里每个 token 只激活 37B。这种架构有个特殊问题：「数据是碎片化的」。

────────────────────

【先说背景：GEMM 喜欢连续数据】

前面说了，GEMM 就是矩阵乘法，神经网络的计算核心。

GEMM 跑得快有个前提：「数据在内存里是连续的」。

GPU 读数据是按块读的——一次读 128 字节，如果你要的数据东一块西一块，它也得一次次跳着读，带宽就浪费了。

普通 Transformer 没这个问题：所有 token 都过同一个 FFN，数据天然连续。

但 MoE 不一样。

────────────────────

【插播：Prefill vs Decode】

先说一个关键概念：「KV Cache」。

大模型生成文字时，每预测一个新 token，都要算 Attention：

  q₇ · [k₁, k₂, ..., k₇] → 得到权重 → 加权求和 [v₁, v₂, ..., v₇]

这里 q₇ 是当前 token 的 query 向量，k₁~k₇ 和 v₁~v₇ 是前面所有 token 的 key 和 value 向量。

问题是：k₁~k₉ 和 v₁~v₉ 每次都要重新算吗？

不用。它们只和输入有关，和"现在预测第几个字"无关。所以算过一次就存起来，下次直接读。

⚠️ 常见误解：Q、K、V 不是权重矩阵！

  • W_q, W_k, W_v = 权重矩阵，shape 固定，是模型真正在学的东西
  • Q, K, V = 输入 × 权重的结果，shape 是 [seq_len, head_dim]

「Q/K/V 的 shape 里有 seq_len」，输入几个 token 就有几个 q/k/v 向量。

这个存 k 和 v 的地方就叫 KV Cache：

  +---------------------------------------------------+
  | KV Cache (每一层都有一份)                         |
  +---------------------------------------------------+
  | K: [k1, k2, k3, ..., k7]  <- 前 7 个 token 的 key |
  | V: [v1, v2, v3, ..., v7]  <- 前 7 个 token 的 value|
  +---------------------------------------------------+

💡 注意区分两个东西：

• 「权重矩阵 W_k, W_v」：模型参数，训练完就固定了，一直在显存里躺着。每个新 token 进来都要用它算一次 k = x × W_k。

• 「KV Cache」：存的是 x × W 的计算结果。每个 token 只算一次，结果存起来，下次直接读。

KV Cache 省的不是"不用权重矩阵了"，而是"同一个 token 不用重复算"。

具体怎么算的？

「Prefill 阶段」：用户输入 7 个 token，一起算

  X = [x₁, x₂, ..., x₇]   ← 7 个 embedding 堆成矩阵

  Q = X × W_q   ← [7, hidden] × [hidden, head_dim] = 7 个 query
  K = X × W_k   ← 同上，得到 7 个 key
  V = X × W_v   ← 同上，得到 7 个 value

  一次矩阵乘法算出 7 个 token 的 q/k/v，然后 K 和 V 全存进 cache。

「Decode 阶段」：每次只处理 1 个新 token，用它预测下一个

  Prefill 结束后，cache 里已经有 k₁~k₇ 和 v₁~v₇ 了，模型已经预测出第 8 个 token x₈。
  现在要预测第 9 个 token：

  x₈ 是第 8 个 token 的 embedding（刚生成的），用来预测 x₉

  q₈ = x₈ × W_q   ← 1 个向量 × 权重矩阵 = 1 个 query
  k₈ = x₈ × W_k   ← 现算，然后追加到 cache
  v₈ = x₈ × W_v   ← 现算，然后追加到 cache

  用 q₈ 和 cache 里的 k₁~k₈ 算 Attention，预测出 x₉。
  然后继续：算 k₉, v₉，追加到 cache，预测 x₁₀...

  +---------+-----------------+---------------+--------------+
  | 阶段    | 输入            | 矩阵乘法      | Cache 操作   |
  +---------+-----------------+---------------+--------------+
  | Prefill | 7 个 token 一起 | 1 次 (矩阵大) | 一次性填满   |
  | Decode  | 1 个 token      | 1 次 (矩阵小) | 追加 1 行    |
  +---------+-----------------+---------------+--------------+

预测第 8 个 token 时：
• q₇ 现算（只算 1 个向量）
• k₁~k₇ 和 v₁~v₇ 从 cache 读
• 算完后把 k₈ 和 v₈ 追加到 cache 里

这样每次只需要算 1 个 token 的 q/k/v，不用把前面的全算一遍。

────────────────────

现在可以讲 Prefill 和 Decode 了：

大模型推理分两个阶段：

• 「Prefill（预填充）」：处理用户输入。比如你问"什么是量子力学"，这 7 个 token 一起送进去，一次算完。

  💡 "一次算完"具体在干什么？

  1. 7 个 token 先过 Embedding 层，变成 [7, hidden_dim] 的矩阵
  2. 这个矩阵一层一层往下过 Transformer（比如 96 层）
  3. 每一层都会算出当前层的 K（Key）和 V（Value），存进 KV Cache
  4. 过完最后一层，模型预测下一个 token

  所以 Prefill 的核心产出是两个：
  ① 「KV Cache」—— 把用户输入的 K/V 全缓存下来，后面生成回答时直接用
  ② 「第一个输出 token」—— 回答的第一个字

  为什么叫"预填充"？因为是在"预先填充 KV Cache"，为后面的 Decode 阶段做准备。

• 「Decode（解码）」：生成回答。一个 token 一个 token 往外蹦，每生成一个字都要跑一遍模型。

  💡 为什么 Decode 这么慢？

  每生成一个 token，都要：
  1. 把这个新 token 过一遍完整的模型（96 层）
  2. 在每一层，用这个 token 的 Q 去和 KV Cache 里所有的 K/V 做 Attention
  3. 算出下一个 token，重复

  好消息是：KV Cache 已经算好了，不用重复算用户输入的部分。
  坏消息是：每生成一个字都要跑一遍模型，回答 100 个字就要跑 100 遍。

  这就是为什么你问 ChatGPT 一个问题，它"思考"一下就开始回答（Prefill），但回答是一个字一个字蹦出来的（Decode）。

训练时，一整批数据一起算，类似 Prefill。

下面说的"数据是散的"问题，主要发生在「Prefill / 训练阶段」——因为是一批 token 一起处理，才有"散落各处"的问题。Decode 阶段一个一个来，问题不一样，解法也不一样。

────────────────────

【MoE 的问题：FFN 层数据是散的】

💡 MoE 替换的是 FFN 层，不是 Attention 层。Attention 层所有 token 都过同一个模块，数据天然连续。但 FFN 层被拆成了多个 Expert（每个 Expert 就是一个小 FFN），Router 把不同 token 分给不同 Expert——这一步导致数据散了。

Prefill 和 Decode 都有这个问题，但表现不同：
• Prefill：一批 token 分给不同 Expert，数据散落各处
• Decode：每次只有 1 个 token，矩阵太小，Tensor Core 利用率低

先看 Prefill 的情况。假设用户输入 1000 个 token：

• 专家 A 分到：第 3, 17, 42, 156 个 token...（散落各处）
• 专家 B 分到：第 8, 23, 99, 201 个 token...（也散落各处）

如果按原始顺序读取，GPU 显存会面临大量「随机访问」——地址跳来跳去，L1/L2 缓存命中率暴跌，带宽浪费严重。

而且每个专家分到的 token 数量不一样，矩阵大小不规整，Tensor Core 利用率只能到 30% 左右。

「GEMM 本身没问题，问题是喂给它的数据太乱了。」

────────────────────

【解法一：连续布局（训练 / Prefill 阶段）】

把属于同一个专家的 token「归拢」到一起，重新排列成连续内存：

1. 按专家 ID 收集 token
2. 每个专家的 token 重排成连续矩阵
3. 补零对齐到 Tensor Core 要求的粒度
4. 所有专家拼成一个大矩阵，一次 GEMM 调用算完

效果：
• 带宽利用率提升 50%+
• Tensor Core 利用率提升 65%+
• 训练吞吐提升 2~2.5 倍

────────────────────

【解法二：Batching + 掩码（推理 Decode 阶段）】

Decode 阶段每次只处理一个 token，没法攒一批重排。

问题变了：不是"数据散"，而是"数据太少"——每次只有 1 个 token 喂给某个 Expert，矩阵太小，Tensor Core 吃不饱。

这里用到两个技术：

「技术 1：Batching（多用户凑一批）」

推理服务通常同时处理多个用户。比如同一时刻，用户 A 在问"量子力学"，用户 B 在问"今天天气"，服务器把他们的请求凑成一批一起算，GPU 利用率更高。

不同用户的上下文不会混吗？不会。每个用户有自己独立的 KV Cache，算 Attention 时只和自己的 cache 算。Batching 共享的是模型权重和 GPU 算力，不共享 KV Cache 和输出。

具体哪些能 batch？
• X × W_q、X × W_k、X × W_v 这些矩阵乘法 → 能 batch（权重 W 是共享的）
• FFN 层的矩阵乘法 → 能 batch
• softmax(QK^T)V 这步 → 不能跨用户 batch（每个用户只和自己的 KV Cache 算）

「技术 2：掩码对齐」

Batching 之后还有个问题：不同用户的 token 分到的 Expert 不一样，每个 Expert 收到的 token 数量不同，矩阵形状不规整。

解法是「掩码」：把所有请求补零对齐成统一形状，用 mask 标记哪些位置是真数据、哪些是填充。GPU 只算真数据的部分。

好处是矩阵形状固定，可以用 CUDA Graph（把一串 GPU 操作预编译成一个整体，省掉每次调用的启动开销），延迟更低。

两个技术的关系：先用 Batching 凑数据量，再用掩码解决形状不规整。

────────────────────

【两种布局的分工】

  +---------------+----------+------------------+
  | 场景          | 连续布局 | Batching + 掩码  |
  +---------------+----------+------------------+
  | 训练          | ✅ 主力  | ❌               |
  | 推理 Prefill  | ✅ 主力  | ❌               |
  | 推理 Decode   | ❌       | ✅ 主力          |
  +---------------+----------+------------------+

这两种解法在 DeepGEMM 里对应两个 API：
• 连续布局 → `m_grouped_gemm_*_contiguous`
• Batching + 掩码 → `m_grouped_gemm_*_masked`

━━━━━━━━━━━━━━━━━━━━

◆ JIT 编译：运行时按需生成最优内核

矩阵形状千变万化，不可能提前编译好所有情况。

DeepGEMM 的做法：「运行时编译（JIT）」

用户调用 API 时，DeepGEMM 根据矩阵尺寸、精度、硬件配置，「现场生成最优的 CUDA 代码并编译」。

好处：

• 安装时不需要编译，pip install 直接用
• 每种矩阵形状都能得到量身定制的内核
• 编译结果会缓存，第二次调用直接用

支持两种编译器：

• NVCC：性能最优，但编译慢
• NVRTC：编译快 10 倍，性能略低

━━━━━━━━━━━━━━━━━━━━

◆ 性能结果

  +---------------------+-----------------------------+
  | 指标                | 数值                        |
  +---------------------+-----------------------------+
  | H800 峰值性能       | 1550 TFLOPS (2025.04 更新)  |
  | 相比参考实现        | 快 2.7 倍                   |
  | 精度损失 (vs BF16)  | < 0.25%                     |
  +---------------------+-----------------------------+

DeepSeek-V3 的 671B 参数模型，就是用这套 FP8 框架训练的。

━━━━━━━━━━━━━━━━━━━━

◆ 还有什么新东西？

【SM100（Blackwell）支持】

  +------------------+--------------------+--------------+
  | 架构             | 支持的内存布局     | scale 格式   |
  +------------------+--------------------+--------------+
  | SM90 (Hopper)    | 仅 NT              | FP32         |
  | SM100 (Blackwell)| NT/TN/NN/TT 全支持 | 打包的 UE8M0 |
  +------------------+--------------------+--------------+

【MQA 评分内核（2025.09 新增）】

为 DeepSeek v3.2 的 lightning indexer 加的：

• fp8_mqa_logits：预填充阶段
• fp8_paged_mqa_logits：解码阶段，支持分页 KV-Cache

【代码简洁性】

DeepGEMM 的核心内核只有约「300 行代码」。

官方原话："as clean as a tutorial"（像教程一样简洁）。

想学 GPU 优化的，直接读源码就能懂。

━━━━━━━━━━━━━━━━━━━━

◆ 小结

先看因果链：

  用户输入 7 个 token
      ↓
  Prefill：一次性算出 k₁~k₇, v₁~v₇，存进 KV Cache
      ↓
  Decode：每次用 qᵢ + cache 里的 K/V → 预测下一个 token → 新的 k/v 追加到 cache
      ↓
  问题：每次 Attention 都要算矩阵乘法，很费算力
      ↓
  FP8 量化：用 8 bit 省显存省带宽
      ↓
  但 FP8 有两个坑：范围小 + 累加精度不够
      ↓
  DeepGEMM 两招：细粒度量化解决范围，两级累加解决精度
      ↓
  结果：训练成本砍到 557 万美元

DeepGEMM 解决的核心问题：「怎么用 8 bit 低精度做训练，还不丢精度？」

答案是两招：

1.「细粒度量化」：1×128 / 128×128 分组，outlier 隔离，纯 E4M3 就够用
2.「两级累加」：Tensor Core 算乘法，CUDA Core 攒加法，精度损失 < 0.25%

工程实现上：

• TMA 异步搬运 + Warp 专用化 = 计算和搬运完全重叠
• 1d2d 双缓冲 = 大矩阵 Tensor Core 利用率 95%+
• JIT 编译 = 每种形状都有最优内核

结果：「H800 上 1550 TFLOPS，比参考实现快 2.7 倍」。

━━━━━━━━━━━━━━━━━━━━

◆ 参考资料

• DeepGEMM GitHub：https://github.com/deepseek-ai/DeepGEMM
• DeepSeek-V3 Technical Report：https://arxiv.org/abs/2412.19437

━━━━━━━━━━━━━━━━━━━━

// 靳岩岩的 AI 学习笔记 × Claude 的严谨 × Gemini 的浪漫
// 2026-01-21
