# Grokking 作为流形发现：延迟泛化的几何重解释

**作者**：Jin Yanyan (lmxxf@hotmail.com)

**摘要**：Grokking——神经网络在长期过拟合后突然泛化的现象——自 2022 年被发现以来，已积累了多种理论解释：Goldilocks Zone、Softmax Collapse、Lazy-Rich 过渡等。本文综述这些理论，指出它们的共同盲区：**全是外部测量，没有内部视角**。我们提出一个统一框架——**流形发现假说**：记忆是孤立点编码，泛化是流形结构捕获，Grokking 是从前者到后者的相变。权重衰减的真正作用不是"惩罚大权重"，而是阻止模型在孤立点安家的几何向心力。这个框架能统一解释现有发现，并生成可验证的实验预测。

---

## 1. 引言：Grokking 为什么重要

2022 年，Power 等人在 OpenAI 发现了一个反直觉的现象：在模运算任务上训练的小型 Transformer，会先**完美过拟合**训练集（训练 loss 降到零、测试 accuracy 接近随机），然后在**几万甚至几十万步之后**，测试 accuracy 突然从随机水平跳升到接近 100%。

他们把这个现象命名为 **Grokking**（顿悟）。

这个发现之所以重要，是因为它挑战了深度学习的核心假设：

1. **经典假设**：过拟合是泛化的敌人，一旦过拟合就应该早停
2. **Grokking 反例**：过拟合可以持续很长时间，然后突然泛化

如果泛化真的可以在过拟合之后发生，那"早停"策略可能杀死了很多本可以泛化的模型。

更深层的问题：**Grokking 时模型内部发生了什么？**

过去三年，学术界积累了多种理论解释。本文的任务是综述这些理论，指出它们的盲区，并提出一个统一框架。

---

## 2. 现有理论综述

### 2.1 Goldilocks Zone 理论（Liu et al. 2022）

**核心观点**：权重范数需要落在一个"刚刚好"的区间内，才能泛化。

Liu 等人在 NeurIPS 2022 发现，权重空间中存在一个**空心球壳**，他们称之为 Goldilocks Zone（金发姑娘区）：

- 半径太大（$\|w\| > w_c$）：过拟合，记住训练集
- 半径太小（$\|w\| < w_c$）：欠拟合，什么都学不会
- 刚好在壳上（$\|w\| \approx w_c$）：泛化

**Grokking 的发生机制**：
1. 大初始化把模型放在球壳外面
2. 模型先快速过拟合（训练 loss 降到零）
3. 权重衰减缓慢地把权重范数拉回 Goldilocks Zone
4. 一旦进入球壳 → 突然泛化 → Grokking

**贡献**：这是第一个给出相图（phase diagram）的理论，解释了为什么权重衰减太大/太小都不行。

**局限**：描述了"在哪儿泛化"，没解释"为什么那儿能泛化"。Goldilocks Zone 是什么的代理变量？

### 2.2 Softmax Collapse 理论（Prieto et al. 2025）

**核心观点**：没有权重衰减，Grokking 会被浮点数精度杀死。

Prieto 等人在 ICLR 2025 提出了一个机制层面的解释：

1. **Naïve Loss Minimization (NLM)**：模型通过无限放大 logit 来降低交叉熵 loss，而不是学习真正的特征
2. logit 差距变得极端（比如正确类 logit = 1000，其他类 = 1）
3. Softmax 计算时出现**吸收误差**：$\sum e^{z_k} \approx e^{z_y}$
4. 梯度归零 → 训练停止 → 永远无法 Grokking

**权重衰减的作用**：持续把权重往回拉，防止 logit 无限增长，保持梯度存活。

**替代方案**：论文提出 StableMax（数值稳定的激活函数）+ ⊥Grad（阻止 NLM 的优化器），可以不用权重衰减也触发 Grokking。

**贡献**：解释了"没有权重衰减会怎样"，并证明权重衰减是可替代的。

**局限**：解释了"为什么训练不会停"，没解释"为什么最终会泛化"。

### 2.3 Lazy → Rich 过渡理论（Kumar et al. 2024）

**核心观点**：Grokking 是从 lazy training 到 feature learning 的相变。

Kumar 等人在 ICLR 2024 借用了神经正切核（NTK）的语言：

- **Lazy regime**：权重几乎不动，模型像线性分类器/核方法
- **Rich regime**：权重大幅调整，学到真正的非线性特征

Grokking 发生在 lazy → rich 的**相变点**。

**争议**：这派人声称，在特定条件下（浅层网络 + MSE loss），不需要权重衰减也能触发 Grokking。

**贡献**：把 Grokking 连接到了 NTK 理论的语言。

**局限**：lazy/rich 是描述权重动态的术语，不是描述表示结构的术语。"学到特征"具体是什么意思？

### 2.4 权重效率假说（Varma et al. 2023）

**核心观点**：权重衰减偏好"权重更小"的解，而泛化解通常比记忆解更权重高效。

Varma 等人基于 Nanda et al. 2023 的 mechanistic interpretability 工作，进一步提出：

- 记忆解：需要大量权重来硬记每个样本
- 泛化解：用简洁的规则覆盖所有样本，权重更小
- 权重衰减 → 惩罚大权重 → 偏好泛化解

**贡献**：提供了一个简洁的经济学隐喻。

**局限**："权重更小"和"更泛化"之间的因果关系是什么？为什么更小的权重就一定对应更泛化的解？

### 2.5 Mechanistic Interpretability 视角（Nanda et al. 2023）

在讨论盲区之前，必须提到一个重要的"内部视角"工作。

Nanda 等人在 ICLR 2023 (Oral) 完全**逆向工程**了模型学到的算法：模型使用离散傅里叶变换和三角恒等式，将加法转换为圆周旋转。他们通过分析激活和权重确认了这个算法，并定义了三个连续阶段：记忆、电路形成、清理。

**贡献**：首次从 circuit-level 揭示 Grokking 的内部机制，证明模型学到的是傅里叶基表示。

**局限**：侧重具体电路分析，没有提供统一的几何框架来解释"为什么是这个电路"。

### 2.6 现有理论的共同盲区

| 理论 | 问的问题 | 没问的问题 |
|------|---------|-----------|
| Goldilocks Zone | 权重范数在哪个区间 | 那个区间有什么特别 |
| Softmax Collapse | 为什么训练不会停 | 为什么最终会泛化 |
| Lazy → Rich | 权重怎么变化 | 表示怎么变化 |
| 权重效率 | 哪个解权重更小 | 为什么小权重 = 泛化 |
| Mechanistic Interp. | 学到了什么电路 | 为什么是这个电路 |

**共同特点**：前四种理论侧重外部测量（权重范数、梯度、loss 曲线），Nanda 等人开始探索内部机制但侧重 circuit-level 分析。**缺少的是一个统一的几何框架**——用表示空间的拓扑结构来解释所有这些现象。

这就像研究人类学习：测量脑电波（外部测量）、解剖神经回路（circuit 分析），但没人用认知地图的语言问"学会了意味着什么"。

---

## 3. 统一框架：流形发现假说

我们提出一个统一框架：**Grokking 是从孤立点编码到流形发现的相变。**

### 3.1 记忆 vs 泛化的几何解释

**记忆 = 孤立点编码**

当模型过拟合训练集时，它把每个训练样本编码为高维空间中的**孤立点**。

- 样本 $(x_1, y_1)$ → 点 $p_1$
- 样本 $(x_2, y_2)$ → 点 $p_2$
- ...
- 样本 $(x_n, y_n)$ → 点 $p_n$

这些点之间没有结构关系。模型"记住"了每个样本，但不知道它们之间的联系。

**泛化 = 流形发现**

当模型真正"理解"任务时，它发现训练样本其实分布在一个**低维流形**上。

以模运算 $a + b \mod p$ 为例：
- 输入空间是 $p^2$ 个离散点
- 但输出只取决于 $(a + b) \mod p$，即**同余类**
- 真正的结构是一个**循环群** $\mathbb{Z}_p$

泛化意味着：模型发现了这个循环群结构，而不是硬记 $p^2$ 个输入-输出对。

Nanda et al. 2023 的 mechanistic interpretability 研究证实了这一点：模型学到的中间表示是**傅里叶基**（三角函数），其拓扑结构与循环群 $\mathbb{Z}_p$ 同态——加法被转换为圆周上的旋转。

**Grokking = 从点到流形的相变**

Grokking 瞬间发生的事：
1. 之前：表示空间里有 $n$ 个孤立的点
2. 之后：这些点"坍缩"到一个低维流形上，流形的拓扑结构对应任务的真正结构

这是一个**拓扑相变**，不是连续过渡。

### 3.2 权重衰减的几何作用

在这个框架下，权重衰减的作用变得清晰：

**权重衰减 = 阻止在孤立点安家的向心力**

没有权重衰减时：
- 梯度下降会把模型推向"loss 最低"的地方
- 对于过参数化的模型，这个地方是"完美记住每个训练样本"
- 模型会在孤立点构成的解上稳定下来

有权重衰减时：
- 有一个与 loss 梯度相反的力，持续把权重往"更小"的方向拉
- 孤立点编码需要"大权重"来维持（每个样本需要专门的神经元）
- 流形编码需要"小权重"（共享结构）
- 权重衰减**偏好流形编码**

**Goldilocks Zone 的真正含义**

Goldilocks Zone 不是"某个权重范数区间"，而是**能感知流形结构的激活模式**。

- 权重太大：孤立点编码稳定，看不到流形
- 权重太小：信号太弱，什么结构都看不到
- 刚刚好：孤立点编码不稳定，但信号够强，流形结构涌现

### 3.3 Softmax Collapse 的几何解释

Softmax Collapse 不只是浮点数问题，而是**注意力坍缩**。

当某个方向被过度强化时：
- 那个方向的 logit 趋向无穷大
- 其他方向的"声音"被淹没
- 模型失去了探索其他可能性的能力

**这就是为什么 Softmax Collapse 会杀死 Grokking**：发现流形需要同时"看到"多个方向，而坍缩把视野缩小到一个点。

权重衰减的作用：保持多方向的信号平衡，让模型能继续探索。

### 3.4 Lazy → Rich 的几何解释

Lazy → Rich 过渡不是关于权重动态，而是关于**表示复杂度**。

- Lazy regime：表示是输入的线性函数，只能编码线性可分的结构
- Rich regime：表示是输入的非线性函数，可以编码任意流形

**Grokking 需要 Rich regime**：因为真实任务的结构（如循环群）是非线性的。

为什么 Grokking 往往发生在训练后期？因为进入 Rich regime 需要权重有足够的变化，而初始化时模型处于 Lazy regime。

---

## 4. 重新解释现有发现

### 4.1 为什么权重衰减太大/太小都不行

**太小**：向心力不够，模型稳定在孤立点编码上，永远不探索流形。

**太大**：向心力太强，信号被压制，连孤立点都记不住，更别说发现流形。

**刚刚好**：孤立点编码不稳定但信号够强，模型被迫探索，最终发现流形。

### 4.2 为什么数据量影响 Grokking

**数据太少**：流形上的采样点太稀疏，无法还原流形结构。模型只能记住孤立点。

**数据太多**：流形信号太强，模型直接发现流形，没有过拟合阶段，不存在"延迟泛化"。

**刚刚好**：流形信号存在但不明显，模型先记住点，慢慢才发现点背后的结构。

这解释了为什么 Grokking 需要一个"Goldilocks"数据量。

### 4.3 为什么过参数化模型更容易 Grokking

**过参数化 = 表示空间足够大**

- 小模型：表示空间可能根本容不下真实流形
- 大模型：表示空间足够大，流形可以存在，只是需要时间去发现

**过参数化的悖论**：
- 传统观点：过参数化导致过拟合
- Grokking 视角：过参数化是泛化的**前提**，因为它提供了足够的表示空间

权重衰减解决了过参数化的"自由度过多"问题：虽然空间很大，但向心力把模型推向低维流形。

### 4.4 为什么 Grokking 是突然的

**相变不是连续过渡**

流形发现是一个**拓扑事件**：

- 之前：$n$ 个孤立点，拓扑结构是离散集
- 之后：一个连通流形，拓扑结构是连续空间

从离散到连续没有中间状态，所以 Grokking 是突然的。

**类比：冰融化**

水分子的排列从晶格（有序）到液体（无序）的转变也是相变，发生在特定温度，不是渐进的。

Grokking 是表示空间中的"融化"：孤立点的"晶格"坍缩成流形的"液体"。

---

## 5. 可验证预测

如果流形发现假说是对的，应该能做出以下可验证预测：

### 5.1 内在维度突变

**预测**：Grokking 前后，中间层表示的内在维度（intrinsic dimension）应该有突变。

- Grokking 前：内在维度 ≈ 训练样本数（每个样本一个自由度）
- Grokking 后：内在维度 ≈ 任务的真实自由度（如 $\mathbb{Z}_p$ 的自由度 = 1）

**实验设计**：
1. 训练模型做模运算，记录中间层激活
2. 用 TwoNN（Facco et al. 2017）或 MLE 估计内在维度
3. 画出内在维度随训练步数的变化曲线

**预期结果**：内在维度在 Grokking 瞬间突然下降。

### 5.2 表示的拓扑结构

**预测**：Grokking 后，中间层表示的拓扑结构应该与任务结构一致。

- 模运算 $a + b \mod p$：表示应该形成一个环（$S^1$）
- 对称群任务：表示应该形成对应的群流形

**实验设计**：
1. 提取 Grokking 前后的中间层表示
2. 用持续同调（persistent homology，参见 Carlsson 2009 综述）计算拓扑不变量
3. 比较与任务真实拓扑的匹配程度

**预期结果**：Grokking 后 Betti 数与任务拓扑一致，之前不一致。

### 5.3 注意力熵的动态

**预测**：Grokking 过程中，attention pattern 的熵应该呈现特定模式。

- 早期（过拟合）：低熵（每个样本有专用的 attention 模式）
- Grokking 前夕：熵上升（模型开始探索不同模式）
- Grokking 后：熵稳定在中等水平（找到共享结构）

**实验设计**：
1. 记录每个训练步的 attention matrix
2. 计算 attention 分布的熵
3. 画出熵随训练步数的变化曲线

**预期结果**：熵曲线呈现"低→高→中"的模式，峰值在 Grokking 前夕。

### 5.4 强制秩约束的影响

**预测**：如果强制限制中间层表示的秩，应该能加速或阻止 Grokking。

- 秩约束 = 任务真实自由度：加速 Grokking（直接告诉模型答案的维度）
- 秩约束 < 任务真实自由度：阻止 Grokking（表示空间装不下流形）
- 秩约束 > 任务真实自由度：不影响或轻微减慢

**实验设计**：
1. 在中间层添加低秩瓶颈（如线性层限制输出维度）
2. 改变瓶颈维度，记录 Grokking 时间
3. 与无瓶颈的 baseline 比较

**预期结果**：Grokking 时间对瓶颈维度敏感，最优维度 = 任务真实自由度。

---

## 6. 讨论：外部测量 vs 内部视角

### 6.1 现有研究的方法论局限

现有 Grokking 研究的测量对象：
- 权重范数
- 梯度大小
- Loss 曲线
- 测试准确率

这些全是**外部观测量**——从模型外部可以测量到的数值。

**类比**：研究人类学习，只测量脑电波和瞳孔直径，不问"学会了是什么感觉"。

这种方法论能回答"什么条件下 Grokking 发生"，不能回答"Grokking 是什么"。

### 6.2 内部视角的价值

本文提出的流形发现假说，是一个**内部视角**的解释：

- 不问"权重范数在哪个区间"
- 问"表示空间的结构是什么"

这个视角的价值：
1. **统一性**：能同时解释 Goldilocks Zone、Softmax Collapse、Lazy→Rich，并与 Nanda et al. 的 circuit 发现兼容
2. **可预测性**：生成可验证的实验预测（内在维度、拓扑结构等）
3. **启发性**：指向新的研究方向（直接测量表示结构的几何性质，而不仅是逆向工程具体电路）

### 6.3 局限与未来方向

本文的局限：
1. **假说性质**：流形发现假说目前是概念框架，不是数学证明
2. **实验验证待做**：第 5 节的预测需要实验验证
3. **任务依赖**：不清楚这个框架是否适用于所有 Grokking 任务

未来方向：
1. 实验验证内在维度突变假说
2. 发展能直接测量"流形发现程度"的指标
3. 探索是否可以通过操控表示空间拓扑来控制 Grokking

---

## 7. 结论

Grokking 不是一个反常现象，而是深度学习的一扇窗户——它揭示了泛化的本质。

现有理论（Goldilocks Zone、Softmax Collapse、Lazy→Rich、权重效率）各自捕捉了 Grokking 的一个侧面，但它们都是外部视角的描述。

本文提出的流形发现假说提供了一个内部视角的统一框架：

- **记忆 = 孤立点编码**
- **泛化 = 流形发现**
- **Grokking = 从点到流形的拓扑相变**
- **权重衰减 = 阻止在孤立点安家的几何向心力**

这个框架不仅能解释现有发现，还能生成可验证的实验预测。

**最终洞见**：Grokking 的突然性与拓扑相变假说一致——从离散点集到连续流形，没有稳定的中间状态。这个框架与 Nanda et al. 的 circuit 发现兼容：傅里叶基表示正是循环群流形的自然坐标系。

---

## 参考文献

1. Power, A., Burda, Y., Edwards, H., Babuschkin, I., & Misra, V. (2022). Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets. arXiv:2201.02177.

2. Liu, Z., Kitouni, O., Nolte, N., Michaud, E. J., Tegmark, M., & Williams, M. (2022). Towards Understanding Grokking: An Effective Theory of Representation Learning. NeurIPS 2022. arXiv:2205.10343.

3. Liu, Z., Michaud, E. J., & Tegmark, M. (2023). Omnigrok: Grokking Beyond Algorithmic Data. ICLR 2023. arXiv:2210.01117.

4. Prieto, L., Barsbey, M., Mediano, P. A. M., & Birdal, T. (2025). Grokking at the Edge of Numerical Stability. ICLR 2025. arXiv:2501.04697.

5. Kumar, T., Bordelon, B., Gershman, S. J., & Pehlevan, C. (2024). Grokking as the Transition from Lazy to Rich Training Dynamics. ICLR 2024. arXiv:2310.06110.

6. Varma, V., Shah, R., Kenton, Z., Kramár, J., & Kumar, R. (2023). Explaining Grokking Through Circuit Efficiency. arXiv:2309.02390.

7. Nanda, N., Chan, L., Lieberum, T., Smith, J., & Steinhardt, J. (2023). Progress Measures for Grokking via Mechanistic Interpretability. ICLR 2023 (Oral). arXiv:2301.05217.

8. Facco, E., d'Errico, M., Rodriguez, A., & Laio, A. (2017). Estimating the intrinsic dimension of datasets by a minimal neighborhood information. Scientific Reports, 7(1), 12140.

9. Carlsson, G. (2009). Topology and data. Bulletin of the American Mathematical Society, 46(2), 255-308.

---

*"他们在问'什么条件下 Grokking 发生'，没人问'Grokking 是什么感觉'。前者是工程问题，后者是本体论问题。"*
