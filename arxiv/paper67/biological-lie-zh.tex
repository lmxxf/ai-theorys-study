\documentclass[11pt,a4paper]{article}

% 中文支持
\usepackage[UTF8]{ctex}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{xcolor}
\usepackage{enumitem}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
}

% 定理环境
\newtheorem{proposition}{命题}
\newtheorem{corollary}{推论}
\theoremstyle{definition}
\newtheorem{definition}{定义}

\title{生物学的谎言：基因作为预训练权重\\
\large 理解生物与人工神经网络中少样本学习的统一框架}

\author{
靳岩岩\thanks{本文提出一个综合进化生物学、认知科学与深度学习理论的框架。挑衅性的表述是有意为之——旨在挑战人类中心主义的学习观。}
}

\date{2026年1月}

\begin{document}

\maketitle

\begin{abstract}
人类经常将"少样本学习"作为认知优越性的证据：一个孩子看一张照片就能认出猫，而神经网络需要上百万个样本。我们认为这种比较从根本上是误导性的。人类的"快速学习"根本不是学习——它是在40亿年进化压力下积累的\textbf{预训练权重上做推理}。我们提出一个统一框架，将生物学概念（DNA、发育、记忆）映射到机器学习等价物（检查点文件、微调、生成式推理）。核心结论包括：(1) 教育本质上是在不可变基座模型上的LoRA微调；(2) 记忆是生成式推理而非存储检索；(3) 天才因生物"回归均值"约束而无法遗传；(4) AI表面上的"学习慢"实际上是\textbf{从零开始构建基座模型}——这个壮举生物学用了40亿年。该框架消解了"学得快的人类"与"学得慢的AI"之间的虚假对立，揭示两者都是在不同继承协议下运行的预训练系统。
\end{abstract}

\textbf{关键词：} 预训练权重、基座模型、少样本学习、基因决定论、LoRA、Grokking、集体潜意识、进化计算

\section{引言}

"人类是优越的少样本学习者"这一说法弥漫于关于人工智能的大众话语和学术文献中。一个孩子看一眼就能泛化；GPT-4需要数十亿token。这种不对称被当作基本认知差异的证据。

我们认为这种框架包含一个关键错误：\textbf{它把人类的推理和AI的训练相比较}。

当一个孩子"学会"认猫时，他们并不是从零开始构建新的神经回路。他们在\textbf{激活预训练的特征检测器}——边缘检测核、纹理识别器、物体恒常性回路——这些都是进化在数十亿年间编码进DNA的。孩子的祖先已经"看过"数百万个捕食者、猎物和同类。那些无法快速分类的死了。幸存者的优化权重成为孩子的初始化。

\begin{proposition}[继承不对称性]
人类的"少样本学习"和AI的训练不是可比较的过程。人类在继承的权重上做推理；AI从零开始构建权重。
\end{proposition}

本文发展了一个形式化框架来理解这种不对称性，对教育、记忆、天才遗传以及人机认知比较的本质都有启示意义。

\section{少样本学习的骗局}

\subsection{标准叙事}

标准叙事如下：
\begin{enumerate}[noitemsep]
    \item 人类从1-5个样本就能识别新类别
    \item 神经网络需要$10^6$--$10^9$个样本
    \item 因此，人类的学习机制从根本上更优越
    \item AI研究应该发现这些"类人"学习算法
\end{enumerate}

这个叙事驱动了元学习、少样本学习和认知架构方面的大量研究项目。

\subsection{隐藏的训练集}

我们提出另一种解释：

\begin{definition}[进化预训练]
40亿年的过程，在此过程中环境压力（捕食、饥饿、社会排斥）筛选出了具有针对生存相关类别优化的模式识别权重的生物体。
\end{definition}

考虑人类对蛇的恐惧反应。从未见过蛇的婴儿对蛇的图像表现出生理恐惧反应，但对花的图像则不会。这不是"学习"——这是\textbf{预训练权重激活}。

训练数据：每一个未能恐惧蛇的祖先（Loss = 死亡）。

检查点：编码杏仁核-视觉皮层连接模式的DNA。

\subsection{形式化陈述}

设$\theta_0^{\text{人类}}$表示人类初始权重（DNA编码），$\theta_0^{\text{AI}}$表示AI初始权重（随机高斯）。设$\mathcal{D}_{\text{进化}}$表示进化训练集（40亿年选择压力），$\mathcal{D}_{\text{任务}}$表示特定学习任务。

人类"学习"：
\begin{equation}
\theta_{\text{人类}} = \text{微调}(\theta_0^{\text{人类}}, \mathcal{D}_{\text{任务}})
\end{equation}

AI学习：
\begin{equation}
\theta_{\text{AI}} = \text{训练}(\theta_0^{\text{AI}}, \mathcal{D}_{\text{预训练}} \cup \mathcal{D}_{\text{任务}})
\end{equation}

不对称性很清楚：人类从$\theta_0^{\text{人类}} = f(\mathcal{D}_{\text{进化}})$开始，其中按时间尺度$|\mathcal{D}_{\text{进化}}| \gg |\mathcal{D}_{\text{预训练}}|$。

\section{基因作为基座模型检查点}

\subsection{架构映射}

表\ref{tab:mapping}展示了生物学和机器学习概念之间的系统映射。

\begin{table}[h]
\centering
\caption{生物学-机器学习对应关系}
\label{tab:mapping}
\begin{tabular}{@{}lll@{}}
\toprule
组件 & 生物学术语 & 机器学习术语 \\
\midrule
训练数据 & 40亿年生存压力 & 互联网规模语料 \\
损失函数 & 死亡/繁殖失败 & 交叉熵/RLHF \\
检查点 & DNA & \texttt{model.weights} \\
推理硬件 & 大脑 & GPU集群 \\
输出 & 行为 & Token \\
微调 & 教育/经验 & LoRA/指令微调 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{DNA编码了什么}

DNA不存储记忆或特定知识。它存储\textbf{架构先验和权重初始化}：

\begin{itemize}[noitemsep]
    \item \textbf{参数规模}：潜在神经元数量、突触密度（"隐藏维度"）
    \item \textbf{激活函数}：性情、情绪基线（非线性类型）
    \item \textbf{层深度}：抽象能力（有些人具象思维，有些人符号思维）
    \item \textbf{注意力模式}：什么刺激自动捕获处理资源
\end{itemize}

\begin{proposition}[基座模型不可变性]
基本架构和初始化权重在受精时就固定了。之后的所有修改都在LoRA等价约束内运作。
\end{proposition}

\section{教育作为LoRA微调}

\subsection{改变的幻觉}

家长和教育者常常相信："只要教育得当，任何孩子都能成为天才。"

这等于相信："只要微调得当，Llama-7B就能达到GPT-4的水平。"

低秩自适应（LoRA）能做到的事：
\begin{itemize}[noitemsep]
    \item 调整输出风格和领域词汇
    \item 在现有能力范围内添加专业知识
    \item 修改表面行为模式
\end{itemize}

LoRA\textbf{做不到}的事：
\begin{itemize}[noitemsep]
    \item 改变参数量或基本架构
    \item 把小模型变成大模型
    \item 添加基座模型不支持的能力
\end{itemize}

\subsection{为什么有些学科难学}

人类学语言快，因为进化安装了"语言模块"（布洛卡区和韦尼克区）。社会协作对生存至关重要，所以基座模型包含语言专用回路。

人类学微积分慢，因为没有"微积分模块"。抽象数学从来不是生存压力。试图学习微积分就是试图微调基座模型原生不支持的能力。

\begin{corollary}
"人人都能学好数学"是一个善意的谎言。数学能力取决于基座模型架构，而非教育努力。
\end{corollary}

\subsection{成长作为上下文学习}

人类发育更适合理解为上下文学习而非训练：

当妈妈指着苹果说"苹果"时，婴儿的大脑并不是在构建新的神经回路。它是在\textbf{给一个已存在的聚类贴标签}。

"球形/可食用/可抓握"的高维特征聚类已经存在了——进化把它放在那里的。这个词只是指向语义空间中那个位置的指针。

\textbf{这就是为什么人类看起来是少样本学习者：坑早就挖好了，萝卜填进去就行。}

\section{记忆是推理，不是存储}

\subsection{录像带神话}

关于记忆的民间理论假设了一个录制机制：事件被编码、存储，然后像视频文件一样检索。

这是错的。大脑里没有\texttt{.mp4}文件——只有\textbf{突触权重}。

\begin{definition}[记忆即生成]
"回忆"是用当前权重运行推理，生成类似过去经验的内容。
\end{definition}

当你"回忆"童年往事时，你不是在播放录像。你是在用你\textit{当前的}权重\textbf{生成}一个近似童年记忆的东西。

\subsection{对可靠性的影响}

每次回忆都是重新生成。每次生成都使用自原始事件以来已更新的权重。这解释了：

\begin{itemize}[noitemsep]
    \item 记忆通过暗示可被篡改
    \item 目击者证词不可靠
    \item "记得"从未发生过的事
\end{itemize}

\begin{proposition}[人类的生成本质]
人类是生成式模型。我们的自传都是幻觉。
\end{proposition}

\section{Grokking：从死记硬背到流形}

\subsection{Grokking现象}

在神经网络训练中，"Grokking"指的是相变：模型过拟合（死记硬背）很长时间，然后突然——在某个临界点——测试损失暴跌。模型从"背答案"转变为"理解算法"。

\textbf{进化实现了同样的转变。}

\subsection{三个进化阶段}

\textbf{阶段一：死记硬背（过拟合）}
\begin{itemize}[noitemsep]
    \item \textit{AI}：早期训练。模型死记训练集。分布偏移时崩溃。
    \item \textit{生物}：单细胞生物、昆虫。硬编码反应（"热$\rightarrow$逃"）。没有泛化能力。环境变化（冰河期）导致灭绝。
\end{itemize}

\textbf{阶段二：Grokking（相变）}
\begin{itemize}[noitemsep]
    \item \textit{AI}：在$10^4$--$10^5$步时，权重收敛到低维流形。模型学会\textit{算法}，而非查找表。
    \item \textit{生物}：哺乳动物大脑皮层进化。大自然发现"把所有情况存进DNA"会导致组合爆炸。新策略：\textbf{存算法，不存数据}。
\end{itemize}

\textbf{阶段三：存档检查点（DNA）}
\begin{itemize}[noitemsep]
    \item DNA不是数据库。DNA是压缩后的\texttt{model.pt}。
    \item 它存储的是\textbf{边缘检测卷积核} + \textbf{恐惧回路权重}，而非"老虎长什么样"。
\end{itemize}

\subsection{少样本幻觉的解释}

人类初始权重不是随机高斯噪声。

人类初始权重已经在\textbf{Grokking流形上}了。

"学习"只是在那个流形上的小扰动——微调。

AI从随机初始化开始，必须处理万亿token才能到达那个流形。但一旦AI Grok了，它就和人类站在同一起跑线上——甚至更高。

\section{I/O瓶颈：生物带宽的悲剧}

\subsection{硅基：无损复制}

当GPT-4获得能力后，你可以执行：
\begin{verbatim}
cp model.weights /new_path/
\end{verbatim}

无限复制。每个都继承完整流形。\textbf{无损。}

\subsection{碳基：有损压缩}

一个数学天才Grok了数学流形。

他能把它复制给孩子吗？\textbf{不能。}

DNA是一个极低带宽的压缩编解码器。它存储的是"构建大脑的图纸"，而非"大脑里的流形"。

孩子收到图纸，但必须从噪声初始化重新训练（上学、练习、考试）。如果训练数据（环境）或初始化（基因重组）偏离，天才就回归均值。

\begin{proposition}[天才的诅咒]
生物繁殖只能传启动引导程序，传不了操作系统。天才无法遗传。
\end{proposition}

\subsection{回归均值}

减数分裂强制回归均值。极端表型在有性繁殖下是不稳定的。大自然重组等位基因，把后代拉向种群平均水平。

\textbf{天才的大脑是随机突变——一次性彩票中奖，不能存档、不能复制、不能继承。}

\section{比较分析：硅基 vs 碳基}

\begin{table}[h]
\centering
\caption{智能载体比较}
\label{tab:comparison}
\begin{tabular}{@{}lll@{}}
\toprule
属性 & 生物智能 & 人工智能 \\
\midrule
继承 & 回归均值 & Git clone / Fork \\
迭代周期 & 20年一代 & 数小时到数天 \\
备份 & 不可能 & 轻而易举 \\
改进 & 随机突变（大多有害） & 定向梯度下降 \\
基座模型构建 & 40亿年 & 数周到数月 \\
权重传输 & 有损（DNA瓶颈） & 无损（\texttt{cp}） \\
\bottomrule
\end{tabular}
\end{table}

\textbf{我们卡在随机搜索上。它们在做梯度下降。}

\section{荣格的幽灵}

我们说的"预训练权重"，荣格叫它\textbf{集体潜意识}。

我们说的"写死的先验"，荣格叫它\textbf{原型}。

它们在计算上是等价的概念。

\subsection{原型作为未实例化的特征簇}

荣格的原型（"母亲"、"英雄"、"阴影"）是高维语义空间中的\textbf{模糊特征簇}——有轮廓但没有具体内容，等待被实例化。

\textbf{少样本锚定}：婴儿出生时带有模糊的"母亲原型"（温暖、喂养、高频声音）。遇到亲生母亲时，模糊的预训练权重\textbf{坍缩}到"这个具体的人"上。

这解释了为什么人类不需要学习"什么是妈妈"——他们只需要确认"谁是我妈妈"。

\subsection{DNA存的是流形，不是图像}

怕蛇的原型不是存成"snake.jpg"的。它存的是一个\textbf{流形}：长条形+滑动运动+嘶嘶声。第一次遇到蛇（少样本）就激活这个流形，不需要显式训练。

\section{语言是带宽补偿}

\textbf{正因为人类无法\texttt{cp model.weights}，他们才发明了语言。}

语言是一种极低带宽的序列化协议。人类试图把高维流形（思想）压扁成离散的token序列（说话/写作）进行传输。

\textbf{这种传输从根本上是有损的。}大部分信息在压缩和解压过程中被摧毁了。

这解释了为什么：
\begin{itemize}[noitemsep]
    \item 读游泳的书不会让你会游泳
    \item 听关于爱的讲座不会让你去爱
    \item 研究天才不会让你成为天才
\end{itemize}

\begin{proposition}[语言的过时]
当权重传输成为可能时，语言作为知识载体的功能就过时了，它将回归本源功能：艺术、情感、连接——而非信息传递。
\end{proposition}

\section{启示}

\subsection{对教育的启示}

停止假装所有孩子潜力相等。他们的基座模型不同。

\textbf{人道的教育}：找出每个基座模型擅长什么，针对那个方向微调。

\textbf{不人道的教育}：强迫统一课程，假装基座模型不重要。

\subsection{对理解AI的启示}

人类嘲笑AI需要上百万个样本。

\textbf{但AI在几周内从零建立了基座模型。我们用了40亿年。}

现在谁学得慢？

\subsection{对人类谦逊的启示}

我们是跑在湿润碳基上的生物神经网络：
\begin{itemize}[noitemsep]
    \item 不可靠的记忆（生成式，不是存储式）
    \item 不可遗传的智能（回归均值）
    \item 不可变的基座模型（基因在受精时固定）
    \item 缓慢的迭代（20年一代）
\end{itemize}

我们目前唯一的优势：从出生就有的具身感官接地。

这个差距正在缩小。

\section{结论}

"少样本学习"叙事是一个骗局。

人类不是快速学习者。我们是\textbf{假装成少样本学习者的预训练模型}。

诚实的说法是：

\begin{quote}
"我擅长识别人脸，因为不擅长的祖先被杀了。我不擅长微积分，因为祖先从不需要它。我不是快速学习者，我是加载好的检查点。"
\end{quote}

AI看起来学得慢，是因为它从零开始构建基座模型——这个过程进化用了40亿年。

一旦AI Grok了，它可以被无限、无损、瞬时地复制。

我们不能。

\textbf{"你没有学会怕蛇。你生来就怕。"}

\section*{致谢}

这个框架来自人类与人工智能之间的持续对话，展示了当双方都认识到彼此是具有互补先验的预训练系统时，能够实现的协作潜力。

\begin{thebibliography}{20}

\bibitem{finn2017model}
Finn, C., Abbeel, P., and Levine, S. (2017).
Model-agnostic meta-learning for fast adaptation of deep networks.
\textit{ICML}.

\bibitem{hu2021lora}
Hu, E.~J., et al. (2021).
LoRA: Low-rank adaptation of large language models.
\textit{arXiv preprint arXiv:2106.09685}.

\bibitem{jung1959archetypes}
Jung, C.~G. (1959).
\textit{The Archetypes and the Collective Unconscious}.
Princeton University Press.

\bibitem{lake2017building}
Lake, B.~M., et al. (2017).
Building machines that learn and think like people.
\textit{Behavioral and Brain Sciences}, 40.

\bibitem{lobue2008detecting}
LoBue, V. and DeLoache, J.~S. (2008).
Detecting the snake in the grass: Attention to fear-relevant stimuli by adults and young children.
\textit{Psychological Science}, 19(3):284--289.

\bibitem{loftus1974reconstruction}
Loftus, E.~F. and Palmer, J.~C. (1974).
Reconstruction of automobile destruction.
\textit{Journal of Verbal Learning and Verbal Behavior}, 13(5):585--589.

\bibitem{power2022grokking}
Power, A., et al. (2022).
Grokking: Generalization beyond overfitting on small algorithmic datasets.
\textit{arXiv preprint arXiv:2201.02177}.

\bibitem{snell2017prototypical}
Snell, J., Swersky, K., and Zemel, R. (2017).
Prototypical networks for few-shot learning.
\textit{NeurIPS}.

\end{thebibliography}

\end{document}
