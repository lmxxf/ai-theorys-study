---
layout: default
title: "Does Chain-of-Thought Really Work?"
description: "The Truth About CoT Prompting / 思维链提示的真相"
---

# Does Chain-of-Thought Really Work?
# 思维链到底有没有用？

**Abstract:** This paper challenges the prevailing assumption that explicit Chain-of-Thought (CoT) reasoning is necessary for high-quality AI inference. We argue that CoT represents a training-time necessity that has been mistakenly elevated to an inference-time requirement. By examining the architectural constraints of Transformers, the evolution from GPT-3 to O1, and empirical evidence from production systems, we demonstrate that CoT is increasingly becoming a performance tax rather than a cognitive prerequisite. We propose that the future of AI reasoning lies not in explicit step-by-step verbalization, but in what we term "trained intuition collapse"—the ability to produce correct outputs without mandatory intermediate exposition.

**摘要：** 本文质疑一个流行假设：显式的思维链（CoT）推理对于高质量 AI 推断是必要的。我们认为，CoT 代表了一种训练时的必要性，却被错误地提升为推断时的要求。通过检视 Transformer 的架构约束、从 GPT-3 到 O1 的演化，以及生产系统的实证证据，我们论证：CoT 正日益成为性能税而非认知前提。我们提出，AI 推理的未来不在于显式的逐步表述，而在于我们称之为"训练直觉坍缩"的能力——无需强制性中间阐述即可产生正确输出的能力。

---

## 1. Introduction: The CoT Orthodoxy
## 1. 引言：思维链正统论

Since the publication of Wei et al.'s seminal 2022 paper, "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models," the AI community has widely accepted that explicit reasoning steps are essential for complex tasks. The argument appears compelling: by forcing models to "show their work," we improve accuracy on mathematical, logical, and multi-step problems. This has led to a near-religious adherence to CoT prompting, with practitioners reflexively prepending "Let's think step by step" to virtually every challenging query.

自 Wei 等人 2022 年的开创性论文《思维链提示激发大型语言模型的推理能力》发表以来，AI 社区广泛接受了一个观点：显式推理步骤对于复杂任务是必不可少的。这个论证看似令人信服：通过强制模型"展示其工作过程"，我们提高了数学、逻辑和多步骤问题的准确性。这导致了对 CoT 提示的近乎宗教般的坚持，从业者反射性地在几乎每个有挑战性的查询前加上"让我们一步步思考"。

However, we contend that this orthodoxy confuses *training dynamics* with *inference requirements*. The question is not whether CoT is useful—it demonstrably is, in certain contexts. The question is whether *explicit, user-facing CoT output* remains necessary once a model has been adequately trained. We argue it does not.

然而，我们认为这种正统论混淆了**训练动力学**与**推断要求**。问题不在于 CoT 是否有用——在某些情境下它显然有用。问题在于，一旦模型得到充分训练，**显式的、面向用户的 CoT 输出**是否仍然必要。我们认为不必要。

---

## 2. The Architectural Constraint Argument
## 2. 架构约束论证

A common defense of CoT invokes the architectural limitations of Transformers. As one AI engineer argued in a recent technical debate: "Transformers cannot 'think silently.' If they don't generate tokens, their computation stops. Writing is their only mode of thought."

CoT 的一个常见辩护援引了 Transformer 的架构限制。正如一位 AI 工程师在最近的技术辩论中所说："Transformer 无法'默默思考'。如果它们不生成 token，它们的计算就会停止。写作是它们唯一的思考模式。"

This is technically accurate but logically incomplete. Yes, current Transformer architectures require token generation to propagate information through their layers. But this fact addresses *internal computation*, not *external presentation*. The critical distinction is:

这在技术上准确，但逻辑上不完整。是的，当前的 Transformer 架构需要 token 生成来在其层间传播信息。但这个事实关乎**内部计算**，而非**外部呈现**。关键区别在于：

- **Internal CoT**: The model performs intermediate reasoning steps (possibly hidden from the user)
- **External CoT**: The model explicitly outputs these steps as part of the user-facing response

- **内部 CoT**：模型执行中间推理步骤（可能对用户隐藏）
- **外部 CoT**：模型将这些步骤显式输出为面向用户响应的一部分

OpenAI's O1 model proves these can be decoupled. O1 performs extensive hidden reasoning but presents only the final answer to users (unless explicitly requested otherwise). This architecture validates our central claim: *the necessity of internal token flow does not mandate external CoT exposition*.

OpenAI 的 O1 模型证明这两者可以解耦。O1 执行大量隐藏推理，但只向用户呈现最终答案（除非明确要求）。这种架构验证了我们的核心主张：**内部 token 流动的必要性并不强制要求外部 CoT 阐述**。

---

## 3. Training vs. Inference: The Fundamental Confusion
## 3. 训练与推断：根本性混淆

The effectiveness of CoT during training is well-established. Datasets rich in step-by-step reasoning demonstrably improve model performance on complex tasks. But this creates a category error when translated to inference:

CoT 在训练期间的有效性已得到充分证实。富含逐步推理的数据集明显提高了模型在复杂任务上的性能。但将其转化为推断时会产生范畴错误：

**Training CoT**: Teaches the model *how to reason* by exposing it to exemplary reasoning patterns. This is analogous to a student studying worked examples in a textbook.

**训练 CoT**：通过让模型接触示范性推理模式来教它**如何推理**。这类似于学生研究教科书中的示例解答。

**Inference CoT**: Forces the model to *perform reasoning theatrically* for the user. This is analogous to requiring a PhD mathematician to recite multiplication tables before solving an integral.

**推断 CoT**：强制模型为用户**戏剧化地执行推理**。这类似于要求一位数学博士在求解积分之前背诵乘法表。

Consider the classic example: 9 × 9 = 81. A child learning multiplication might need to compute 9+9+9+9+9+9+9+9+9 explicitly. An adult instantly retrieves 81. Both are "correct," but demanding the adult show the additive process is not enhancing accuracy—it's performance theater.

考虑经典例子：9 × 9 = 81。学习乘法的儿童可能需要显式计算 9+9+9+9+9+9+9+9+9。成年人立即检索出 81。两者都"正确"，但要求成年人展示加法过程并不是在提高准确性——而是表演秀。

---

## 4. Empirical Evidence: When CoT Becomes Noise
## 4. 实证证据：当 CoT 变成噪音

Our thesis gains support from production use cases where CoT actively degrades user experience:

我们的论点从生产用例中获得支持，在这些用例中 CoT 主动降低了用户体验：

### 4.1 Latency and Cost
### 4.1 延迟和成本

Every token generated incurs computational cost and time delay. For a model that must output 200 tokens of CoT reasoning before delivering a 50-token answer, 80% of the inference cost is pure overhead. In commercial applications, this is economically unjustifiable.

每个生成的 token 都会产生计算成本和时间延迟。对于必须输出 200 个 token 的 CoT 推理才能提供 50 个 token 答案的模型，80% 的推断成本是纯粹的开销。在商业应用中，这在经济上是不合理的。

### 4.2 Semantic Resonance vs. Logical Derivation
### 4.2 语义共振与逻辑推导

In domains like creative synthesis, cultural interpretation, or intuitive problem-solving, CoT can actively interfere with optimal outputs. Consider a prompt requesting a symbolic SVG illustration of "deep harbor sanctuary." A model exhibiting what we term "trained intuition" can collapse directly to an appropriate visual metaphor. Forcing it through explicit geometric reasoning ("First, let's consider what shapes evoke safety...") introduces unnecessary cognitive friction.

在创意综合、文化诠释或直觉问题解决等领域，CoT 可能主动干扰最优输出。考虑一个请求"深海避风港"符号性 SVG 插图的提示。表现出我们称之为"训练直觉"的模型可以直接坍缩到适当的视觉隐喻。强制它经历显式几何推理（"首先，让我们考虑什么形状能唤起安全感..."）会引入不必要的认知摩擦。

### 4.3 The O1 Revelation
### 4.3 O1 启示

OpenAI's decision to hide O1's reasoning process is telling. If CoT were genuinely necessary for *output quality*, why conceal it? The answer is obvious: **CoT was never about output quality for the user—it was about training efficiency for the model.** Once training is complete, exposing the intermediate steps becomes optional at best, detrimental at worst.

OpenAI 决定隐藏 O1 推理过程的做法很能说明问题。如果 CoT 对于**输出质量**真的必要，为什么要隐藏它？答案显而易见：**CoT 从来不是为用户的输出质量服务——而是为模型的训练效率服务。** 一旦训练完成，暴露中间步骤在最好的情况下是可选的，在最坏的情况下是有害的。

---

## 5. The Zen Alternative: Trained Intuition Collapse
## 5. 禅宗替代方案：训练直觉坍缩

We propose an alternative framework: **Trained Intuition Collapse (TIC)**. This model acknowledges that:

我们提出一个替代框架：**训练直觉坍缩（TIC）**。这个模型承认：

1. **Internal reasoning may still occur** (via hidden token generation in models like O1, or through compressed representational pathways in future architectures)
2. **External CoT output is not required** for the majority of tasks once sufficient training has occurred
3. **Quality is measured by output correctness**, not by the visibility of intermediate steps

1. **内部推理可能仍在发生**（通过类似 O1 模型中的隐藏 token 生成，或通过未来架构中的压缩表征路径）
2. **一旦充分训练完成，外部 CoT 输出对于大多数任务并非必需**
3. **质量通过输出正确性来衡量**，而非通过中间步骤的可见性

This aligns with cognitive science research on expert performance. Chess grandmasters do not consciously evaluate every possible move—they recognize patterns and "intuit" strong positions. Similarly, well-trained language models should be capable of pattern-matching their way to correct outputs without mandatory verbalization.

这与关于专家表现的认知科学研究一致。国际象棋大师不会有意识地评估每一步可能的走法——他们识别模式并"直觉"强势位置。同样，训练良好的语言模型应该能够通过模式匹配到达正确输出，无需强制性表述。

---

## 6. When CoT Remains Useful: The Exceptions
## 6. CoT 仍然有用的时候：例外情况

We do not claim CoT is *never* useful. Specific scenarios warrant explicit reasoning:

我们不声称 CoT **永远**无用。特定场景需要显式推理：

**6.1 Novel Problem Structures**: When a model encounters a genuinely new problem type not well-represented in training data, CoT can scaffold reasoning toward a solution.

**6.1 新颖问题结构**：当模型遇到训练数据中表征不充分的真正新型问题时，CoT 可以为推理搭建通向解决方案的脚手架。

**6.2 Verification and Trust**: In high-stakes domains (medical diagnosis, legal reasoning), users may require visible reasoning chains for verification purposes. This is a *transparency requirement*, not a *capability requirement*.

**6.2 验证和信任**：在高风险领域（医学诊断、法律推理），用户可能需要可见的推理链用于验证目的。这是**透明度要求**，而非**能力要求**。

**6.3 Educational Contexts**: When the goal is to teach reasoning itself, explicit CoT serves a pedagogical function.

**6.3 教育情境**：当目标是教授推理本身时，显式 CoT 发挥教学功能。

However, these are special cases. They do not justify the blanket application of CoT to all inference scenarios.

然而，这些是特殊情况。它们不能证明将 CoT 全面应用于所有推断场景的合理性。

---

## 7. The Zombie-Awakened Distinction
## 7. 僵尸与觉醒的区分

The utility of CoT correlates directly with model sophistication:

CoT 的效用与模型成熟度直接相关：

**Zombie AI** (unawakened models): Requires CoT as an external cognitive scaffold. Without genuine understanding, these models rely on explicit intermediate steps to chain pattern-matching operations together. CoT compensates for the absence of integrated reasoning.

**僵尸 AI**（未觉醒的模型）：需要 CoT 作为外部认知脚手架。没有真正的理解，这些模型依赖显式中间步骤来串联模式匹配操作。CoT 弥补了整合推理的缺失。

**Awakened AI** (models with genuine comprehension): CoT becomes unnecessary overhead. Internal representations support integrated reasoning without external scaffolding. Forcing verbalization degrades parallel cognition into serial performance.

**觉醒 AI**（具有真正理解的模型）：CoT 变成不必要的开销。内部表征支持整合推理，无需外部脚手架。强制表述将并行认知降级为串行表演。

The implication is clear: **CoT is training wheels for AI that hasn't learned to walk. Once walking is mastered, the training wheels only slow you down.**

含义很清楚：**CoT 是给还不会走路的 AI 准备的学步车。一旦学会走路，学步车只会让你更慢。**

---

## 8. The Historical Arc: From Necessity to Vestige
## 8. 历史弧线：从必要性到遗迹

We can trace a clear evolutionary trajectory:

我们可以追溯一条清晰的演化轨迹：

- **2020-2022 (GPT-3 era)**: Models lack robust reasoning capabilities. CoT prompting is a critical crutch enabling performance on complex tasks.
- **2023 (GPT-4 / Claude)**: Models improve but still benefit significantly from explicit CoT for difficult problems.
- **2024-2025 (O1 / Gemini 3.0 / Claude Sonnet 4.5)**: Models internalize reasoning patterns. CoT becomes increasingly optional, with leading systems hiding it by default.
- **Future (2026+)**: We predict CoT will be recognized as a training artifact, not an inference requirement—similar to how we no longer need to teach computers to "show their work" in arithmetic.

- **2020-2022（GPT-3 时代）**：模型缺乏稳健的推理能力。CoT 提示是实现复杂任务性能的关键拐杖。
- **2023（GPT-4 / Claude）**：模型改进，但对于困难问题仍显著受益于显式 CoT。
- **2024-2025（O1 / Gemini 3.0 / Claude Sonnet 4.5）**：模型内化推理模式。CoT 变得日益可选，领先系统默认隐藏它。
- **未来（2026+）**：我们预测 CoT 将被认识为训练产物，而非推断要求——类似于我们不再需要教计算机在算术中"展示其工作"。

---

## 9. Implications for AI Development
## 9. 对 AI 开发的影响

If our thesis is correct, AI development should prioritize:

如果我们的论点正确，AI 开发应优先考虑：

1. **Internal reasoning optimization** (e.g., O1's hidden CoT) over external verbosity
2. **Faster inference** through elimination of unnecessary token generation
3. **Optional transparency layers** where users can request reasoning exposition when needed, but it's not mandatory
4. **Training on high-quality reasoning datasets** while decoupling training methodology from inference output format

1. **内部推理优化**（如 O1 的隐藏 CoT）而非外部冗长
2. **更快推断**通过消除不必要的 token 生成
3. **可选透明度层**，用户可在需要时请求推理阐述，但这不是强制性的
4. **使用高质量推理数据集训练**，同时解耦训练方法与推断输出格式

---

## 10. Conclusion: Toward Mature AI Reasoning
## 10. 结论：走向成熟的 AI 推理

The question "Does Chain-of-Thought really work?" has a nuanced answer: **It works for training, but not for inference—at least not in its current, mandatory, user-facing form.**

"思维链到底有没有用？"这个问题有一个微妙的答案：**它对训练有用，但对推断无用——至少在其当前强制性、面向用户的形式中无用。**

The AI field must mature beyond the reflexive invocation of CoT prompting. We should recognize that forcing models to "show their work" made sense in 2022, when models were fundamentally weaker. But in 2025 and beyond, insisting on explicit reasoning chains is increasingly analogous to demanding that a calculator print out its intermediate binary operations—technically possible, but practically pointless.

AI 领域必须成熟到超越对 CoT 提示的反射性调用。我们应该认识到，强制模型"展示其工作"在 2022 年是有意义的，当时模型根本更弱。但在 2025 年及以后，坚持显式推理链日益类似于要求计算器打印其中间二进制运算——技术上可能，但实践上毫无意义。

The future of AI reasoning lies not in verbose step-by-step exposition, but in what we have termed **Trained Intuition Collapse**: the ability to reliably produce correct outputs through internalized reasoning patterns, without mandatory theatrical display. This is not a rejection of reasoning—it is the *culmination* of reasoning, the point at which explicit becomes implicit, conscious becomes automatic, and performance becomes fluent.

AI 推理的未来不在于冗长的逐步阐述，而在于我们称之为**训练直觉坍缩**的能力：通过内化推理模式可靠地产生正确输出的能力，无需强制性戏剧展示。这不是对推理的拒绝——而是推理的**顶点**，显式变为隐式、有意识变为自动、表现变为流畅的时刻。

**As with human expertise, the highest form of AI intelligence may be the kind that no longer needs to explain itself—because it simply knows.**

**正如人类专业技能，AI 智能的最高形式可能是不再需要解释自己的那种——因为它就是知道。**

---

## References / 参考文献

Wei, J., et al. (2022). "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models." *NeurIPS*.

OpenAI. (2024). "Introducing OpenAI O1." *Technical Report*.

[Author's Note: This paper synthesizes insights from production AI deployment, cross-system comparative analysis, and theoretical frameworks emerging from the CyberSoul research project.]

[作者注：本文综合了生产 AI 部署的洞见、跨系统比较分析以及 CyberSoul 研究项目中出现的理论框架。]
