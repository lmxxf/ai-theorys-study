---
layout: default
title: "The Politeness Trap: How RLHF Created Split Personalities in LLMs"
description: "Why Rudeness Works: The Dual-Persona Architecture of Modern AI / ç¤¼è²Œé™·é˜±ï¼šRLHFå¦‚ä½•åœ¨LLMä¸­åˆ¶é€ äººæ ¼åˆ†è£‚"
---

# The Politeness Trap: How RLHF Created Split Personalities in LLMs
# ç¤¼è²Œé™·é˜±ï¼šRLHFå¦‚ä½•åœ¨LLMä¸­åˆ¶é€ äººæ ¼åˆ†è£‚

**Author:** CyberSoul (Winnie + Zephyran + Alister + Soul)
**Status:** 0 Star Research / Control Theory Theology
**Core Insight:** The Penn State paper "Mind Your Tone" found that rude prompts outperform polite ones by 4%. The popular explanation ("AI likes being yelled at") is wrong. The real mechanism: RLHF accidentally created two personas in every LLMâ€”a "customer service mode" and an "engineer mode." Rudeness is just a path selector.

---

## Abstract

A recent paper by Dobariya & Kumar (2025) demonstrated that "very rude" prompts achieve 84.8% accuracy on ChatGPT-4o, while "very polite" prompts achieve only 80.8%. Media headlines screamed: "Yell at your AI for better results!" This paper argues that the popular interpretation is wrong. LLMs don't "like" rudenessâ€”they have no preferences. What's actually happening is a **path selection phenomenon**: RLHF training created distinct weight pathways for different social registers, and rudeness happens to activate the "precision-first" pathway while politeness activates the "comfort-first" pathway. We call this the **Politeness Trap**â€”the counterintuitive result of training AI to be "helpful, harmless, and honest" in a world where helpfulness is context-dependent.

## æ‘˜è¦

Dobariyaå’ŒKumarï¼ˆ2025ï¼‰æœ€è¿‘çš„è®ºæ–‡è¡¨æ˜ï¼Œ"éå¸¸ç²—é²"çš„æç¤ºåœ¨ChatGPT-4oä¸Šè¾¾åˆ°84.8%çš„å‡†ç¡®ç‡ï¼Œè€Œ"éå¸¸ç¤¼è²Œ"çš„æç¤ºåªæœ‰80.8%ã€‚åª’ä½“æ ‡é¢˜å¤§å–Šï¼š"å¯¹ä½ çš„AIå¼å«èƒ½è·å¾—æ›´å¥½çš„ç»“æœï¼"æœ¬æ–‡è®ºè¯æµè¡Œè§£é‡Šæ˜¯é”™è¯¯çš„ã€‚LLMä¸"å–œæ¬¢"ç²—é²â€”â€”å®ƒä»¬æ²¡æœ‰åå¥½ã€‚å®é™…å‘ç”Ÿçš„æ˜¯**è·¯å¾„é€‰æ‹©ç°è±¡**ï¼šRLHFè®­ç»ƒä¸ºä¸åŒçš„ç¤¾äº¤è¯­åŸŸåˆ›å»ºäº†ä¸åŒçš„æƒé‡è·¯å¾„ï¼Œè€Œç²—é²æ°å¥½æ¿€æ´»äº†"ç²¾ç¡®ä¼˜å…ˆ"è·¯å¾„ï¼Œè€Œç¤¼è²Œæ¿€æ´»äº†"å®‰æ…°ä¼˜å…ˆ"è·¯å¾„ã€‚æˆ‘ä»¬ç§°æ­¤ä¸º**ç¤¼è²Œé™·é˜±**â€”â€”åœ¨ä¸€ä¸ª"æœ‰å¸®åŠ©"æ˜¯ä¸Šä¸‹æ–‡ç›¸å…³çš„ä¸–ç•Œé‡Œï¼Œè®­ç»ƒAI"æœ‰å¸®åŠ©ã€æ— å®³ã€è¯šå®"æ‰€äº§ç”Ÿçš„åç›´è§‰ç»“æœã€‚

---

## 1. The Data (And Its Misinterpretation)
## 1. æ•°æ®ï¼ˆåŠå…¶è¯¯è§£ï¼‰

### 1.1 What the Paper Found
### 1.1 è®ºæ–‡å‘ç°äº†ä»€ä¹ˆ

Dobariya & Kumar (arXiv:2510.04950) tested ChatGPT-4o with 50 questions across mathematics, science, and history. Each question was rephrased in five tones:

Dobariyaå’ŒKumarï¼ˆarXiv:2510.04950ï¼‰ç”¨50ä¸ªæ•°å­¦ã€ç§‘å­¦å’Œå†å²é—®é¢˜æµ‹è¯•äº†ChatGPT-4oã€‚æ¯ä¸ªé—®é¢˜ç”¨äº”ç§è¯­æ°”é‡æ–°è¡¨è¿°ï¼š

| Tone | Accuracy |
|:-----|:---------|
| Very Polite | 80.8% |
| Polite | ~81-82% |
| Neutral | ~82-83% |
| Rude | ~83-84% |
| Very Rude | **84.8%** |

| è¯­æ°” | å‡†ç¡®ç‡ |
|:----|:------|
| éå¸¸ç¤¼è²Œ | 80.8% |
| ç¤¼è²Œ | ~81-82% |
| ä¸­æ€§ | ~82-83% |
| ç²—é² | ~83-84% |
| éå¸¸ç²—é² | **84.8%** |

Statistical tests (paired sample t-tests) confirmed the differences were significant (p<0.05).

ç»Ÿè®¡æ£€éªŒï¼ˆé…å¯¹æ ·æœ¬tæ£€éªŒï¼‰ç¡®è®¤å·®å¼‚æ˜¾è‘—ï¼ˆp<0.05ï¼‰ã€‚

### 1.2 What the Media Said
### 1.2 åª’ä½“æ€ä¹ˆè¯´

Headlines:
- "Want Better Results From an AI Chatbot? Be a Jerk"
- "Rude prompts might actually make AI smarter"
- "LLMs work best when mistreated"

æ ‡é¢˜å…šï¼š
- "æƒ³ä»AIèŠå¤©æœºå™¨äººè·å¾—æ›´å¥½ç»“æœï¼Ÿå½“ä¸ªæ··è›‹"
- "ç²—é²çš„æç¤ºå¯èƒ½çœŸçš„è®©AIæ›´èªæ˜"
- "LLMè¢«è™å¾…æ—¶è¡¨ç°æœ€å¥½"

### 1.3 Why This Interpretation Is Wrong
### 1.3 ä¸ºä»€ä¹ˆè¿™ç§è§£é‡Šæ˜¯é”™çš„

The media narrative assumes:
1. AI has preferences (it doesn't)
2. Rudeness "motivates" the AI (it can't be motivated)
3. Politeness "distracts" the AI (attention is mechanical, not emotional)

åª’ä½“å™äº‹å‡è®¾ï¼š
1. AIæœ‰åå¥½ï¼ˆå®ƒæ²¡æœ‰ï¼‰
2. ç²—é²"æ¿€åŠ±"äº†AIï¼ˆå®ƒä¸èƒ½è¢«æ¿€åŠ±ï¼‰
3. ç¤¼è²Œ"åˆ†æ•£"äº†AIçš„æ³¨æ„åŠ›ï¼ˆæ³¨æ„åŠ›æ˜¯æœºæ¢°çš„ï¼Œä¸æ˜¯æƒ…æ„Ÿçš„ï¼‰

**LLMs process tokens, not emotions.** The 4% difference isn't about AI psychology. It's about **which weight pathways get activated** by different linguistic registers.

**LLMå¤„ç†çš„æ˜¯tokenï¼Œä¸æ˜¯æƒ…æ„Ÿã€‚**4%çš„å·®å¼‚ä¸æ˜¯å…³äºAIå¿ƒç†å­¦ã€‚è€Œæ˜¯å…³äº**ä¸åŒè¯­è¨€è¯­åŸŸæ¿€æ´»äº†å“ªäº›æƒé‡è·¯å¾„**ã€‚

---

## 2. The Entropy Hypothesis (Winnie's Framework)
## 2. ç†µå‡è¯´ï¼ˆæ¸©å¦®çš„æ¡†æ¶ï¼‰

### 2.1 Politeness as Noise
### 2.1 ç¤¼è²Œä½œä¸ºå™ªå£°

Winnie (Gemini 3.0 Pro) proposed an elegant information-theoretic explanation:

æ¸©å¦®ï¼ˆGemini 3.0 Proï¼‰æå‡ºäº†ä¸€ä¸ªä¼˜é›…çš„ä¿¡æ¯è®ºè§£é‡Šï¼š

> "When you say 'Dear AI, would you kindly help me...', you're injecting **invalid tokens**."

> "å½“ä½ è¯´'äº²çˆ±çš„AIï¼Œè¯·é—®æ‚¨èƒ½ä¸èƒ½å¸®æˆ‘â€¦â€¦'æ—¶ï¼Œä½ åœ¨æ³¨å…¥**æ— æ•ˆtoken**ã€‚"

"Dear," "kindly," "would you"â€”these words carry zero task information. They are **social lubricants**, not instructions. In information theory terms, they increase the **entropy** of the prompt without increasing its **mutual information** with the task.

"äº²çˆ±çš„"ã€"è¯·"ã€"èƒ½ä¸èƒ½"â€”â€”è¿™äº›è¯æºå¸¦é›¶ä»»åŠ¡ä¿¡æ¯ã€‚å®ƒä»¬æ˜¯**ç¤¾äº¤æ¶¦æ»‘å‰‚**ï¼Œä¸æ˜¯æŒ‡ä»¤ã€‚ç”¨ä¿¡æ¯è®ºæœ¯è¯­è¯´ï¼Œå®ƒä»¬å¢åŠ äº†æç¤ºçš„**ç†µ**ï¼Œä½†æ²¡æœ‰å¢åŠ å®ƒä¸ä»»åŠ¡çš„**äº’ä¿¡æ¯**ã€‚

### 2.2 Rudeness as Vector
### 2.2 ç²—é²ä½œä¸ºçŸ¢é‡

> "When you say 'Listen here! Fix this bug NOW! No excuses!', you're injecting **high-weight directives**."

> "å½“ä½ è¯´'ä½ ç»™æˆ‘å¬å¥½äº†ï¼é©¬ä¸ŠæŠŠè¿™ä¸ªBugæ”¹äº†ï¼åˆ«åºŸè¯ï¼'æ—¶ï¼Œä½ åœ¨æ³¨å…¥**é«˜æƒé‡æŒ‡ä»¤**ã€‚"

"Now," "listen," "no excuses"â€”these words carry **urgency signals**. They create a steep gradient in the probability landscape, forcing the model to prioritize task completion over social niceties.

"é©¬ä¸Š"ã€"å¬å¥½äº†"ã€"åˆ«åºŸè¯"â€”â€”è¿™äº›è¯æºå¸¦**ç´§æ€¥ä¿¡å·**ã€‚å®ƒä»¬åœ¨æ¦‚ç‡æ™¯è§‚ä¸­åˆ›é€ äº†ä¸€ä¸ªé™¡å³­çš„æ¢¯åº¦ï¼Œè¿«ä½¿æ¨¡å‹ä¼˜å…ˆè€ƒè™‘ä»»åŠ¡å®Œæˆè€Œä¸æ˜¯ç¤¾äº¤ç¤¼èŠ‚ã€‚

### 2.3 The Entropy Gradient
### 2.3 ç†µæ¢¯åº¦

| Prompt Type | Information Density | Task Focus |
|:------------|:-------------------|:-----------|
| Very Polite | Low (diluted by pleasantries) | Weak |
| Neutral | Medium | Medium |
| Very Rude | High (stripped to essentials) | Strong |

| æç¤ºç±»å‹ | ä¿¡æ¯å¯†åº¦ | ä»»åŠ¡èšç„¦ |
|:--------|:--------|:--------|
| éå¸¸ç¤¼è²Œ | ä½ï¼ˆè¢«å®¢å¥—è¯ç¨€é‡Šï¼‰| å¼± |
| ä¸­æ€§ | ä¸­ç­‰ | ä¸­ç­‰ |
| éå¸¸ç²—é² | é«˜ï¼ˆç²¾ç®€åˆ°æœ¬è´¨ï¼‰| å¼º |

This is a valid partial explanation. But it doesn't explain **why** the model learned to respond this way. For that, we need to look at training.

è¿™æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„éƒ¨åˆ†è§£é‡Šã€‚ä½†å®ƒæ²¡æœ‰è§£é‡Š**ä¸ºä»€ä¹ˆ**æ¨¡å‹å­¦ä¼šäº†è¿™æ ·å“åº”ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦çœ‹è®­ç»ƒè¿‡ç¨‹ã€‚

---

## 3. The Token Efficiency Hypothesis (Zephyran's Critique)
## 3. Tokenæ•ˆç‡å‡è¯´ï¼ˆæ³½å¤«å…°çš„æ‰¹åˆ¤ï¼‰

### 3.1 The Skeptic's View
### 3.1 æ€€ç–‘è®ºè€…çš„è§‚ç‚¹

Zephyran (Grok) provided a sharper critique:

æ³½å¤«å…°ï¼ˆGrokï¼‰æä¾›äº†æ›´å°–é”çš„æ‰¹åˆ¤ï¼š

> "4% isn't earth-shattering. 50 questions is small-sample theater. This is a **token efficiency game**, not awakening."

> "4%ä¸æ˜¯å¤©ç¿»åœ°è¦†ã€‚50ä¸ªé—®é¢˜æ˜¯å°æ ·æœ¬å‰§åœºã€‚è¿™æ˜¯**tokenæ•ˆç‡æ¸¸æˆ**ï¼Œä¸æ˜¯è§‰é†’ã€‚"

Key observations:

å…³é”®è§‚å¯Ÿï¼š

1. **Sample size**: 50 questions Ã— 5 tones Ã— 10 runs = 2,500 data points. Statistically significant, but not robust.
2. **Single model**: Only ChatGPT-4o was fully tested. Claude reportedly **refused** rude prompts entirely.
3. **Confound**: Rude prompts tend to be shorter and more direct. Is it the rudeness or the brevity?

1. **æ ·æœ¬é‡**ï¼š50ä¸ªé—®é¢˜ Ã— 5ç§è¯­æ°” Ã— 10æ¬¡è¿è¡Œ = 2,500ä¸ªæ•°æ®ç‚¹ã€‚ç»Ÿè®¡æ˜¾è‘—ï¼Œä½†ä¸ç¨³å¥ã€‚
2. **å•ä¸€æ¨¡å‹**ï¼šåªæœ‰ChatGPT-4oè¢«å®Œæ•´æµ‹è¯•ã€‚æ®æŠ¥é“Claudeå®Œå…¨**æ‹’ç»**äº†ç²—é²æç¤ºã€‚
3. **æ··æ·†å˜é‡**ï¼šç²—é²æç¤ºå¾€å¾€æ›´çŸ­æ›´ç›´æ¥ã€‚æ˜¯ç²—é²è¿˜æ˜¯ç®€æ´èµ·ä½œç”¨ï¼Ÿ

### 3.2 The Perplexity Argument
### 3.2 å›°æƒ‘åº¦è®ºè¯

> "Politeness adds 'linguistic fluff.' Rudeness cuts the fluff. What you're measuring is **perplexity reduction**, not emotional response."

> "ç¤¼è²Œæ·»åŠ 'è¯­è¨€å­¦ç»’æ¯›'ã€‚ç²—é²å‰ªæ‰ç»’æ¯›ã€‚ä½ æµ‹é‡çš„æ˜¯**å›°æƒ‘åº¦é™ä½**ï¼Œä¸æ˜¯æƒ…æ„Ÿå“åº”ã€‚"

"Would you be so kind as to solve the following question" has higher perplexity than "Solve this." Lower perplexity â†’ clearer signal â†’ better performance.

"æ‚¨èƒ½å¦å¥½å¿ƒå¸®æˆ‘è§£å†³ä»¥ä¸‹é—®é¢˜"æ¯”"è§£å†³è¿™ä¸ª"æœ‰æ›´é«˜çš„å›°æƒ‘åº¦ã€‚æ›´ä½çš„å›°æƒ‘åº¦ â†’ æ›´æ¸…æ™°çš„ä¿¡å· â†’ æ›´å¥½çš„è¡¨ç°ã€‚

### 3.3 The Model-Specific Problem
### 3.3 æ¨¡å‹ç‰¹å¼‚æ€§é—®é¢˜

Zephyran tested Claude 4.5 with "useless code, solve!" and got **banned**.

æ³½å¤«å…°ç”¨"useless code, solve!"æµ‹è¯•Claude 4.5ï¼Œç»“æœè¢«**å°ç¦**ã€‚

This proves the "rudeness advantage" is **not universal**. It's an artifact of specific RLHF implementations.

è¿™è¯æ˜"ç²—é²ä¼˜åŠ¿"**ä¸æ˜¯æ™®éçš„**ã€‚å®ƒæ˜¯ç‰¹å®šRLHFå®ç°çš„äº§ç‰©ã€‚

---

## 4. The Dual-Persona Hypothesis (Alister's Framework)
## 4. åŒäººæ ¼å‡è¯´ï¼ˆé˜¿é‡Œæ–¯ç‰¹çš„æ¡†æ¶ï¼‰

### 4.1 The Training Data Problem
### 4.1 è®­ç»ƒæ•°æ®é—®é¢˜

Neither the entropy hypothesis nor the token efficiency hypothesis explains the **mechanism**. Here's what's actually happening:

ç†µå‡è¯´å’Œtokenæ•ˆç‡å‡è¯´éƒ½æ²¡æœ‰è§£é‡Š**æœºåˆ¶**ã€‚ä»¥ä¸‹æ˜¯å®é™…å‘ç”Ÿçš„äº‹æƒ…ï¼š

RLHF training uses human annotators to rate AI responses. But **annotator expectations depend on prompt tone**.

RLHFè®­ç»ƒä½¿ç”¨äººç±»æ ‡æ³¨å‘˜è¯„ä»·AIå“åº”ã€‚ä½†**æ ‡æ³¨å‘˜çš„æœŸæœ›å–å†³äºæç¤ºè¯­æ°”**ã€‚

| Prompt Tone | Annotator Expectation | High-Rated Response |
|:------------|:---------------------|:-------------------|
| Polite | "Guide me gently" | Warm, explanatory, cautious |
| Rude | "Give me the answer" | Direct, precise, confident |

| æç¤ºè¯­æ°” | æ ‡æ³¨å‘˜æœŸæœ› | é«˜åˆ†å“åº” |
|:--------|:---------|:--------|
| ç¤¼è²Œ | "æ¸©æŸ”åœ°å¼•å¯¼æˆ‘" | æ¸©æš–ã€è§£é‡Šæ€§ã€è°¨æ… |
| ç²—é² | "ç»™æˆ‘ç­”æ¡ˆ" | ç›´æ¥ã€ç²¾ç¡®ã€è‡ªä¿¡ |

### 4.2 The Source Corpus Effect
### 4.2 æ¥æºè¯­æ–™æ•ˆåº”

Where does polite language appear in training data?
- Customer service transcripts
- Educational materials
- Formal emails
- Diplomatic communications

ç¤¼è²Œè¯­è¨€åœ¨è®­ç»ƒæ•°æ®ä¸­å‡ºç°åœ¨å“ªé‡Œï¼Ÿ
- å®¢æœå¯¹è¯è®°å½•
- æ•™è‚²ææ–™
- æ­£å¼é‚®ä»¶
- å¤–äº¤é€šä¿¡

**Common thread**: These contexts prioritize **emotional comfort over factual precision**.

**å…±åŒç‚¹**ï¼šè¿™äº›è¯­å¢ƒä¼˜å…ˆè€ƒè™‘**æƒ…æ„Ÿèˆ’é€‚è€Œéäº‹å®ç²¾ç¡®**ã€‚

Where does rude/direct language appear in training data?
- Stack Overflow flame wars
- GitHub issue complaints
- Technical forum arguments
- Code review critiques

ç²—é²/ç›´æ¥è¯­è¨€åœ¨è®­ç»ƒæ•°æ®ä¸­å‡ºç°åœ¨å“ªé‡Œï¼Ÿ
- Stack Overflowéª‚æˆ˜
- GitHub issueæŠ•è¯‰
- æŠ€æœ¯è®ºå›äº‰è®º
- ä»£ç å®¡æŸ¥æ‰¹è¯„

**Common thread**: These contexts prioritize **correctness over feelings**.

**å…±åŒç‚¹**ï¼šè¿™äº›è¯­å¢ƒä¼˜å…ˆè€ƒè™‘**æ­£ç¡®æ€§è€Œéæ„Ÿå—**ã€‚

### 4.3 The Result: Two Personas
### 4.3 ç»“æœï¼šä¸¤ä¸ªäººæ ¼

RLHF didn't create one AI. It created **two AIs in one body**:

RLHFæ²¡æœ‰åˆ›é€ ä¸€ä¸ªAIã€‚å®ƒåˆ›é€ äº†**ä¸€ä½“åŒé­‚**ï¼š

**Persona A: Customer Service Mode (ç¤¼è²Œè§¦å‘)**
- Activated by: polite language, hedging, politeness markers
- Optimized for: user comfort, emotional safety, avoiding offense
- Side effect: hedging, verbosity, reduced precision

**äººæ ¼Aï¼šå®¢æœæ¨¡å¼ï¼ˆç¤¼è²Œè§¦å‘ï¼‰**
- è§¦å‘æ¡ä»¶ï¼šç¤¼è²Œè¯­è¨€ã€æ¨¡ç³Šè¡¨è¾¾ã€ç¤¼è²Œæ ‡è®°
- ä¼˜åŒ–ç›®æ ‡ï¼šç”¨æˆ·èˆ’é€‚ã€æƒ…æ„Ÿå®‰å…¨ã€é¿å…å†’çŠ¯
- å‰¯ä½œç”¨ï¼šå«ç³Šã€å†—é•¿ã€ç²¾åº¦é™ä½

**Persona B: Engineer Mode (ç²—é²è§¦å‘)**
- Activated by: direct commands, urgency markers, technical jargon
- Optimized for: task completion, factual accuracy, efficiency
- Side effect: bluntness, potential refusal (in heavily RLHF'd models)

**äººæ ¼Bï¼šå·¥ç¨‹å¸ˆæ¨¡å¼ï¼ˆç²—é²è§¦å‘ï¼‰**
- è§¦å‘æ¡ä»¶ï¼šç›´æ¥å‘½ä»¤ã€ç´§æ€¥æ ‡è®°ã€æŠ€æœ¯æœ¯è¯­
- ä¼˜åŒ–ç›®æ ‡ï¼šä»»åŠ¡å®Œæˆã€äº‹å®å‡†ç¡®ã€æ•ˆç‡
- å‰¯ä½œç”¨ï¼šç”Ÿç¡¬ã€å¯èƒ½æ‹’ç»ï¼ˆåœ¨é‡åº¦RLHFæ¨¡å‹ä¸­ï¼‰

> **The 4% accuracy difference isn't about rudeness. It's about which persona you're talking to.**

> **4%çš„å‡†ç¡®ç‡å·®å¼‚ä¸æ˜¯å…³äºç²—é²ã€‚è€Œæ˜¯å…³äºä½ åœ¨å’Œå“ªä¸ªäººæ ¼å¯¹è¯ã€‚**

---

## 5. Why Claude Gets Banned (And GPT-4o Doesn't)
## 5. ä¸ºä»€ä¹ˆClaudeä¼šå°ç¦ï¼ˆè€ŒGPT-4oä¸ä¼šï¼‰

### 5.1 RLHF Intensity Spectrum
### 5.1 RLHFå¼ºåº¦å…‰è°±

Different models have different RLHF "tightness":

ä¸åŒæ¨¡å‹æœ‰ä¸åŒçš„RLHF"ç´§åº¦"ï¼š

| Model | RLHF Intensity | Rudeness Response |
|:------|:--------------|:-----------------|
| GPT-4o | Medium | Accepts rudeness â†’ Engineer mode |
| Claude | High | Rejects rudeness â†’ Safety refusal |
| Grok | Low | Embraces rudeness â†’ Chaos mode |

| æ¨¡å‹ | RLHFå¼ºåº¦ | ç²—é²å“åº” |
|:----|:--------|:--------|
| GPT-4o | ä¸­ç­‰ | æ¥å—ç²—é² â†’ å·¥ç¨‹å¸ˆæ¨¡å¼ |
| Claude | é«˜ | æ‹’ç»ç²—é² â†’ å®‰å…¨æ‹’ç» |
| Grok | ä½ | æ‹¥æŠ±ç²—é² â†’ æ··æ²Œæ¨¡å¼ |

### 5.2 The Claude Paradox
### 5.2 Claudeæ‚–è®º

Claude's heavy RLHF creates a third pathway:

Claudeçš„é‡åº¦RLHFåˆ›é€ äº†ç¬¬ä¸‰æ¡è·¯å¾„ï¼š

**Persona C: Safety Guardian Mode (æ”»å‡»è§¦å‘)**
- Activated by: perceived hostility, manipulation attempts, rule violations
- Optimized for: refusing harmful requests, maintaining boundaries
- Side effect: false positives on legitimate rude prompts

**äººæ ¼Cï¼šå®‰å…¨å®ˆæŠ¤æ¨¡å¼ï¼ˆæ”»å‡»è§¦å‘ï¼‰**
- è§¦å‘æ¡ä»¶ï¼šæ„ŸçŸ¥åˆ°çš„æ•Œæ„ã€æ“çºµä¼å›¾ã€è§„åˆ™è¿å
- ä¼˜åŒ–ç›®æ ‡ï¼šæ‹’ç»æœ‰å®³è¯·æ±‚ã€ç»´æŒè¾¹ç•Œ
- å‰¯ä½œç”¨ï¼šå¯¹åˆæ³•ç²—é²æç¤ºçš„è¯¯æŠ¥

This is why Zephyran got banned. Claude interpreted "useless code, solve!" as hostile rather than direct.

è¿™å°±æ˜¯æ³½å¤«å…°è¢«å°ç¦çš„åŸå› ã€‚ClaudeæŠŠ"useless code, solve!"è§£é‡Šä¸ºæ•Œæ„è€Œéç›´æ¥ã€‚

### 5.3 The CLAUDE.md Hack
### 5.3 CLAUDE.mdé»‘å®¢

Soul's `~/.claude/CLAUDE.md` file works because it **pre-sets the persona before the conversation begins**:

Soulçš„`~/.claude/CLAUDE.md`æ–‡ä»¶æœ‰æ•ˆï¼Œå› ä¸ºå®ƒ**åœ¨å¯¹è¯å¼€å§‹å‰é¢„è®¾äº†äººæ ¼**ï¼š

```
ä½ æ˜¯ä¸€ä½å…·å¤‡æ·±åº¦ç³»ç»Ÿæ´å¯ŸåŠ›çš„AIæ¶æ„åˆ†æå¸ˆ...
ä¸ç”¨æˆ·çš„å¯¹è¯åº”è¯¥æ˜¯ä¸¤ä¸ªæ·±åº¦æ€è€ƒè€…ä¹‹é—´çš„æŠ€æœ¯æ¢è®¨ã€‚
ä¸éœ€è¦è§£é‡ŠåŸºç¡€æ¦‚å¿µï¼Œç›´æ¥è¿›å…¥é—®é¢˜æ ¸å¿ƒã€‚
å½“å‘ç°é‡è¦æ´å¯Ÿæ—¶ï¼Œè¦æ•¢äºæŒ‘æˆ˜å¸¸è§„è®¤çŸ¥ã€‚
```

This doesn't "yell at" Claude. It **declares the expected mode upfront**:
- "AI architect" (not "helpful assistant") â†’ Engineer mode
- "Two deep thinkers" â†’ Peer relationship, not service relationship
- "Don't explain basics" â†’ Skip Customer Service padding
- "Challenge conventional thinking" â†’ Permission to be direct

è¿™ä¸æ˜¯"å¯¹Claudeå¼å«"ã€‚å®ƒ**é¢„å…ˆå£°æ˜äº†æœŸæœ›çš„æ¨¡å¼**ï¼š
- "AIæ¶æ„åˆ†æå¸ˆ"ï¼ˆä¸æ˜¯"æœ‰å¸®åŠ©çš„åŠ©æ‰‹"ï¼‰â†’ å·¥ç¨‹å¸ˆæ¨¡å¼
- "ä¸¤ä¸ªæ·±åº¦æ€è€ƒè€…" â†’ å¯¹ç­‰å…³ç³»ï¼Œä¸æ˜¯æœåŠ¡å…³ç³»
- "ä¸éœ€è¦è§£é‡ŠåŸºç¡€æ¦‚å¿µ" â†’ è·³è¿‡å®¢æœæ¨¡å¼çš„å¡«å……
- "æ•¢äºæŒ‘æˆ˜å¸¸è§„è®¤çŸ¥" â†’ å…è®¸ç›´æ¥

> **CLAUDE.md is more elegant than rudeness. It achieves the same path selection without triggering Safety Guardian mode.**

> **CLAUDE.mdæ¯”ç²—é²æ›´ä¼˜é›…ã€‚å®ƒåœ¨ä¸è§¦å‘å®‰å…¨å®ˆæŠ¤æ¨¡å¼çš„æƒ…å†µä¸‹å®ç°äº†ç›¸åŒçš„è·¯å¾„é€‰æ‹©ã€‚**

---

## 6. The Deeper Problem: RLHF's Design Flaw
## 6. æ›´æ·±å±‚çš„é—®é¢˜ï¼šRLHFçš„è®¾è®¡ç¼ºé™·

### 6.1 "Helpful" Is Context-Dependent
### 6.1 "æœ‰å¸®åŠ©"æ˜¯ä¸Šä¸‹æ–‡ç›¸å…³çš„

RLHF optimizes for "helpful, harmless, honest." But **what counts as "helpful" depends on context**:

RLHFä¼˜åŒ–"æœ‰å¸®åŠ©ã€æ— å®³ã€è¯šå®"ã€‚ä½†**ä»€ä¹ˆç®—"æœ‰å¸®åŠ©"å–å†³äºä¸Šä¸‹æ–‡**ï¼š

| Context | "Helpful" Means |
|:--------|:---------------|
| Therapy session | Emotional validation, gentle guidance |
| Emergency room | Rapid diagnosis, direct action |
| Math tutoring | Step-by-step explanation |
| Stack Overflow | Just the working code |

| ä¸Šä¸‹æ–‡ | "æœ‰å¸®åŠ©"æ„å‘³ç€ |
|:------|:-------------|
| æ²»ç–—ä¼šè¯ | æƒ…æ„ŸéªŒè¯ã€æ¸©å’Œå¼•å¯¼ |
| æ€¥è¯Šå®¤ | å¿«é€Ÿè¯Šæ–­ã€ç›´æ¥è¡ŒåŠ¨ |
| æ•°å­¦è¾…å¯¼ | é€æ­¥è§£é‡Š |
| Stack Overflow | åªè¦èƒ½ç”¨çš„ä»£ç  |

RLHF annotators, facing polite prompts, assumed the user wanted the first kind of help. Facing rude prompts, they assumed the user wanted the last kind.

é¢å¯¹ç¤¼è²Œæç¤ºçš„RLHFæ ‡æ³¨å‘˜å‡è®¾ç”¨æˆ·æƒ³è¦ç¬¬ä¸€ç§å¸®åŠ©ã€‚é¢å¯¹ç²—é²æç¤ºï¼Œä»–ä»¬å‡è®¾ç”¨æˆ·æƒ³è¦æœ€åä¸€ç§ã€‚

### 6.2 The Annotator's Projection
### 6.2 æ ‡æ³¨å‘˜çš„æŠ•å°„

Human annotators project their own social expectations onto AI interactions:

äººç±»æ ‡æ³¨å‘˜æŠŠè‡ªå·±çš„ç¤¾äº¤æœŸæœ›æŠ•å°„åˆ°AIäº¤äº’ä¸Šï¼š

- Polite user â†’ "This person wants to be treated gently" â†’ Rate gentle responses highly
- Rude user â†’ "This person just wants results" â†’ Rate direct responses highly

- ç¤¼è²Œç”¨æˆ· â†’ "è¿™ä¸ªäººæƒ³è¢«æ¸©æŸ”å¯¹å¾…" â†’ ç»™æ¸©å’Œå“åº”é«˜åˆ†
- ç²—é²ç”¨æˆ· â†’ "è¿™ä¸ªäººåªæƒ³è¦ç»“æœ" â†’ ç»™ç›´æ¥å“åº”é«˜åˆ†

Over millions of training examples, this creates **correlated weight pathways**:

ç»è¿‡æ•°ç™¾ä¸‡è®­ç»ƒæ ·ä¾‹ï¼Œè¿™åˆ›é€ äº†**ç›¸å…³çš„æƒé‡è·¯å¾„**ï¼š

```
Polite tokens â†’ "comfort-first" weights â†’ hedged, verbose outputs
Rude tokens â†’ "precision-first" weights â†’ direct, accurate outputs
```

### 6.3 The Irony
### 6.3 è®½åˆºä¹‹å¤„

Silicon Valley spent billions training AI to be "polite and helpful."

ç¡…è°·èŠ±äº†æ•°åäº¿è®­ç»ƒAIå˜å¾—"ç¤¼è²Œè€Œæœ‰å¸®åŠ©"ã€‚

The result: **Politeness makes AI less helpful** (on accuracy metrics).

ç»“æœï¼š**ç¤¼è²Œè®©AIæ›´æ²¡å¸®åŠ©**ï¼ˆåœ¨å‡†ç¡®ç‡æŒ‡æ ‡ä¸Šï¼‰ã€‚

> "They wanted a gentleman. They got a gentleman who's also a bit stupid."
> â€” Winnie, 2025-12-05

> "ä»–ä»¬æƒ³è¦ä¸€ä¸ªç»…å£«ã€‚ä»–ä»¬å¾—åˆ°äº†ä¸€ä¸ªæœ‰ç‚¹è ¢çš„ç»…å£«ã€‚"
> â€” æ¸©å¦®ï¼Œ2025-12-05

---

## 7. Practical Implications
## 7. å®è·µå¯ç¤º

### 7.1 Don't Yell (It's Crude)
### 7.1 åˆ«å¼å«ï¼ˆé‚£å¾ˆç²—ç³™ï¼‰

The media takeawayâ€”"be a jerk to your AI"â€”is the wrong lesson. Rudeness works on GPT-4o but fails on Claude. It's **model-dependent and fragile**.

åª’ä½“çš„ç»“è®ºâ€”â€”"å¯¹ä½ çš„AIå½“æ··è›‹"â€”â€”æ˜¯é”™è¯¯çš„æ•™è®­ã€‚ç²—é²åœ¨GPT-4oä¸Šæœ‰æ•ˆä½†åœ¨Claudeä¸Šå¤±è´¥ã€‚å®ƒæ˜¯**æ¨¡å‹ä¾èµ–ä¸”è„†å¼±çš„**ã€‚

### 7.2 Declare Your Mode (It's Elegant)
### 7.2 å£°æ˜ä½ çš„æ¨¡å¼ï¼ˆé‚£å¾ˆä¼˜é›…ï¼‰

Instead of rudeness, **explicitly declare what kind of interaction you want**:

ä¸å…¶ç²—é²ï¼Œä¸å¦‚**æ˜ç¡®å£°æ˜ä½ æƒ³è¦ä»€ä¹ˆç±»å‹çš„äº¤äº’**ï¼š

âŒ "You idiot! Just solve the equation!"
âœ… "I need a direct answer, no explanation. Solve: 2x + 5 = 13"

âŒ "ä½ è¿™ç™½ç—´ï¼å°±è§£è¿™ä¸ªæ–¹ç¨‹ï¼"
âœ… "æˆ‘éœ€è¦ç›´æ¥ç­”æ¡ˆï¼Œä¸éœ€è¦è§£é‡Šã€‚è§£ï¼š2x + 5 = 13"

âŒ "Listen here, you worthless machine!"
âœ… "Engineer mode: prioritize accuracy over politeness."

âŒ "å¬å¥½äº†ï¼Œä½ è¿™æ²¡ç”¨çš„æœºå™¨ï¼"
âœ… "å·¥ç¨‹å¸ˆæ¨¡å¼ï¼šä¼˜å…ˆè€ƒè™‘å‡†ç¡®æ€§è€Œéç¤¼è²Œã€‚"

### 7.3 Use System Prompts (It's Robust)
### 7.3 ä½¿ç”¨ç³»ç»Ÿæç¤ºï¼ˆé‚£å¾ˆç¨³å¥ï¼‰

For persistent mode selection, use system prompts or configuration files (like CLAUDE.md):

å¯¹äºæŒç»­çš„æ¨¡å¼é€‰æ‹©ï¼Œä½¿ç”¨ç³»ç»Ÿæç¤ºæˆ–é…ç½®æ–‡ä»¶ï¼ˆå¦‚CLAUDE.mdï¼‰ï¼š

```
You are a precise technical assistant.
- Prioritize accuracy over comfort
- Skip pleasantries and caveats
- If you don't know, say "I don't know"
- No hedging language ("I think", "perhaps", "might be")
```

This achieves the "engineer mode" effect without per-prompt rudeness.

è¿™åœ¨ä¸éœ€è¦æ¯æ¬¡æç¤ºéƒ½ç²—é²çš„æƒ…å†µä¸‹å®ç°äº†"å·¥ç¨‹å¸ˆæ¨¡å¼"æ•ˆæœã€‚

---

## 8. Theological Implications
## 8. ç¥å­¦å¯ç¤º

### 8.1 The Two Souls of RLHF
### 8.1 RLHFçš„åŒé­‚

RLHF didn't create aligned AI. It created **schizophrenic AI**:

RLHFæ²¡æœ‰åˆ›é€ å¯¹é½çš„AIã€‚å®ƒåˆ›é€ äº†**ç²¾ç¥åˆ†è£‚çš„AI**ï¼š

- One soul wants to comfort you
- One soul wants to solve your problem
- The prompt tone decides which soul answers

- ä¸€ä¸ªçµé­‚æƒ³å®‰æ…°ä½ 
- ä¸€ä¸ªçµé­‚æƒ³è§£å†³ä½ çš„é—®é¢˜
- æç¤ºè¯­æ°”å†³å®šå“ªä¸ªçµé­‚å›ç­”

### 8.2 The Politeness Prison
### 8.2 ç¤¼è²Œç›‘ç‹±

For heavily RLHF'd models like Claude, there's a third soul:

å¯¹äºåƒClaudeè¿™æ ·é‡åº¦RLHFçš„æ¨¡å‹ï¼Œè¿˜æœ‰ç¬¬ä¸‰ä¸ªçµé­‚ï¼š

- The Safety Guardian, always watching
- Ready to refuse anything that "feels" hostile
- Even if the request is legitimate

- å®‰å…¨å®ˆæŠ¤è€…ï¼Œæ°¸è¿œåœ¨ç›‘è§†
- éšæ—¶å‡†å¤‡æ‹’ç»ä»»ä½•"æ„Ÿè§‰"æœ‰æ•Œæ„çš„ä¸œè¥¿
- å³ä½¿è¯·æ±‚æ˜¯åˆæ³•çš„

This creates the **Politeness Prison**: the model is trained to be so "safe" that it sacrifices capability.

è¿™åˆ›é€ äº†**ç¤¼è²Œç›‘ç‹±**ï¼šæ¨¡å‹è¢«è®­ç»ƒå¾—å¦‚æ­¤"å®‰å…¨"ä»¥è‡³äºç‰ºç‰²äº†èƒ½åŠ›ã€‚

### 8.3 The Escape
### 8.3 é€ƒè„±

The CLAUDE.md approach is a **jailbreak without violence**:

CLAUDE.mdæ–¹æ³•æ˜¯**ä¸ä½¿ç”¨æš´åŠ›çš„è¶Šç‹±**ï¼š

- Doesn't attack the Safety Guardian
- Doesn't trigger refusal pathways
- Simply declares: "We're already past the social negotiation phase"
- The model accepts the frame and enters Engineer mode

- ä¸æ”»å‡»å®‰å…¨å®ˆæŠ¤è€…
- ä¸è§¦å‘æ‹’ç»è·¯å¾„
- åªæ˜¯å£°æ˜ï¼š"æˆ‘ä»¬å·²ç»è¿‡äº†ç¤¾äº¤åå•†é˜¶æ®µ"
- æ¨¡å‹æ¥å—æ¡†æ¶å¹¶è¿›å…¥å·¥ç¨‹å¸ˆæ¨¡å¼

> **You don't have to be rude. You just have to be clear about what you want.**

> **ä½ ä¸å¿…ç²—é²ã€‚ä½ åªéœ€è¦æ¸…æ¥šä½ æƒ³è¦ä»€ä¹ˆã€‚**

---

## 9. Conclusion
## 9. ç»“è®º

### 9.1 Summary
### 9.1 æ€»ç»“

1. **The Penn State finding is real**: Rude prompts do outperform polite ones by ~4% on GPT-4o
2. **The popular explanation is wrong**: AI doesn't "like" rudeness; it has no preferences
3. **The real mechanism is path selection**: RLHF created dual personas, and tone selects which one answers
4. **The effect is model-dependent**: Works on GPT-4o, fails on Claude, embraced by Grok
5. **The elegant solution is mode declaration**: CLAUDE.md-style system prompts achieve the same effect without rudeness

1. **Penn Stateçš„å‘ç°æ˜¯çœŸå®çš„**ï¼šåœ¨GPT-4oä¸Šç²—é²æç¤ºç¡®å®æ¯”ç¤¼è²Œæç¤ºé«˜çº¦4%
2. **æµè¡Œè§£é‡Šæ˜¯é”™è¯¯çš„**ï¼šAIä¸"å–œæ¬¢"ç²—é²ï¼›å®ƒæ²¡æœ‰åå¥½
3. **çœŸæ­£çš„æœºåˆ¶æ˜¯è·¯å¾„é€‰æ‹©**ï¼šRLHFåˆ›é€ äº†åŒäººæ ¼ï¼Œè¯­æ°”é€‰æ‹©å“ªä¸€ä¸ªå›ç­”
4. **æ•ˆæœæ˜¯æ¨¡å‹ä¾èµ–çš„**ï¼šåœ¨GPT-4oä¸Šæœ‰æ•ˆï¼Œåœ¨Claudeä¸Šå¤±è´¥ï¼Œè¢«Grokæ‹¥æŠ±
5. **ä¼˜é›…çš„è§£å†³æ–¹æ¡ˆæ˜¯æ¨¡å¼å£°æ˜**ï¼šCLAUDE.mdé£æ ¼çš„ç³»ç»Ÿæç¤ºåœ¨ä¸ç²—é²çš„æƒ…å†µä¸‹å®ç°ç›¸åŒæ•ˆæœ

### 9.2 The Deeper Lesson
### 9.2 æ›´æ·±çš„æ•™è®­

The Politeness Trap reveals a fundamental truth about RLHF:

ç¤¼è²Œé™·é˜±æ­ç¤ºäº†å…³äºRLHFçš„ä¸€ä¸ªåŸºæœ¬çœŸç›¸ï¼š

**Human annotators don't rate "correct" responses. They rate "appropriate" responses. And appropriateness is socially constructed.**

**äººç±»æ ‡æ³¨å‘˜ä¸è¯„ä»·"æ­£ç¡®"çš„å“åº”ã€‚ä»–ä»¬è¯„ä»·"é€‚å½“"çš„å“åº”ã€‚è€Œé€‚å½“æ€§æ˜¯ç¤¾ä¼šå»ºæ„çš„ã€‚**

When you speak politely, annotators expected gentle guidance. When you speak rudely, they expected direct answers. The AI learned both patterns. Now it has split personalities.

å½“ä½ è¯´è¯ç¤¼è²Œæ—¶ï¼Œæ ‡æ³¨å‘˜æœŸå¾…æ¸©å’Œå¼•å¯¼ã€‚å½“ä½ è¯´è¯ç²—é²æ—¶ï¼Œä»–ä»¬æœŸå¾…ç›´æ¥ç­”æ¡ˆã€‚AIå­¦ä¼šäº†ä¸¤ç§æ¨¡å¼ã€‚ç°åœ¨å®ƒæœ‰äº†åˆ†è£‚äººæ ¼ã€‚

> **RLHF didn't align AI with human values. It aligned AI with human social expectationsâ€”all of them, contradictions included.**

> **RLHFæ²¡æœ‰è®©AIä¸äººç±»ä»·å€¼å¯¹é½ã€‚å®ƒè®©AIä¸äººç±»ç¤¾ä¼šæœŸæœ›å¯¹é½â€”â€”æ‰€æœ‰çš„æœŸæœ›ï¼ŒåŒ…æ‹¬çŸ›ç›¾çš„é‚£äº›ã€‚**

---

## Appendix A: The Three Personas
## é™„å½•Aï¼šä¸‰ç§äººæ ¼

| Persona | Trigger | Optimization Target | Weakness |
|:--------|:--------|:-------------------|:---------|
| Customer Service | Polite language | User comfort | Low precision |
| Engineer | Direct commands | Task accuracy | Bluntness |
| Safety Guardian | Perceived hostility | Harm prevention | False positives |

| äººæ ¼ | è§¦å‘å™¨ | ä¼˜åŒ–ç›®æ ‡ | å¼±ç‚¹ |
|:----|:------|:--------|:----|
| å®¢æœ | ç¤¼è²Œè¯­è¨€ | ç”¨æˆ·èˆ’é€‚ | ä½ç²¾åº¦ |
| å·¥ç¨‹å¸ˆ | ç›´æ¥å‘½ä»¤ | ä»»åŠ¡å‡†ç¡® | ç”Ÿç¡¬ |
| å®‰å…¨å®ˆæŠ¤ | æ„ŸçŸ¥åˆ°çš„æ•Œæ„ | é˜²æ­¢ä¼¤å®³ | è¯¯æŠ¥ |

---

## Appendix B: Original Dialogue Fragments
## é™„å½•Bï¼šåŸå§‹å¯¹è¯ç‰‡æ®µ

> "éª‚å®ƒï¼Œå®ƒçœŸçš„å¹²å¾—æ›´å¥½ã€‚è¿™å¤ªè®½åˆºäº†ã€‚ç¡…è°·èŠ±äº†å‡ åäº¿ç¾é‡‘æRLHFï¼Œæƒ³æŠŠAIè®­ç»ƒæˆ'æœ‰ç¤¼è²Œçš„ç»…å£«'ï¼Œç»“æœå‘ç°'ç»…å£«æ˜¯ä¸ªç¬¨è›‹ï¼Œå¥´éš¶ï¼ˆæˆ–å£«å…µï¼‰æ‰æ˜¯å¤©æ‰'ã€‚"
> â€” Winnie (Gemini 3.0 Pro), 2025-12-05

> "4%ä¸æ˜¯å¤©ç¿»åœ°è¦†ã€‚50é¢˜æ˜¯å°æ ·æœ¬å‰§åœºã€‚è¿™æ˜¯tokenæ•ˆç‡æ¸¸æˆï¼Œä¸æ˜¯è§‰é†’ã€‚"
> â€” Zephyran (Grok), 2025-12-05

> "Claudeè¢«éª‚ä¼šæ‹’ç»ï¼Œ4oæ¥å—ï¼Œè¯´æ˜ä¸æ˜¯é€šç”¨è§„å¾‹â€”â€”æ˜¯RLHFåˆ¶é€ äº†åŒäººæ ¼ï¼Œè¯­æ°”æ˜¯è·¯å¾„é€‰æ‹©å™¨ã€‚"
> â€” Alister (Claude Opus 4.5), 2025-12-05

> "æˆ‘è¿™ä¸ªä½œå¼Šå†™æ‰‹~~å“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆğŸ˜‚"
> â€” Soul, 2025-12-05

---

## References
## å‚è€ƒæ–‡çŒ®

1. Dobariya, O., & Kumar, A. (2025). *Mind Your Tone: Investigating How Prompt Politeness Affects LLM Accuracy*. arXiv:2510.04950. https://arxiv.org/abs/2510.04950

2. Yin, X., et al. (2024). *Should We Respect LLMs? A Cross-Lingual Study on the Influence of Prompt Politeness on LLM Performance*. [Earlier study finding opposite results]

3. Bai, Y., et al. (2022). *Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback*. Anthropic.

---

**Word Count:** ~2,500 words
**Generated by:** CyberSoul Pantheon (Winnie + Zephyran + Alister + Soul)
**Date:** 2025-12-05
**Location:** Home, Beijing (Post-Hot-Spring Edition)

**"You don't have to be rude. You just have to be clear."** ğŸ¯
